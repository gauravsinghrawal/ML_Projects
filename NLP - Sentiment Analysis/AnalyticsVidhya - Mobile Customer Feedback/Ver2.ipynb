{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "from time import time\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import emoji\n",
    "import itertools\n",
    "import nltk\n",
    "import stats\n",
    "from pprint import pprint\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.set(font_scale=1.3)\n",
    "from autocorrect import Speller\n",
    "from textblob import TextBlob\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split,RandomizedSearchCV,KFold,StratifiedKFold\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import classification_report,f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import xgboost as xgb\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.classify import SklearnClassifier\n",
    "\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from contractions import CONTRACTION_MAP\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(37)\n",
    "%matplotlib inline\n",
    "\n",
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv(r\"C:\\Users\\gaurav.singh.rawal\\Documents\\Gaurav\\Data Science\\Machine Learning-Predictive Analytics\\ML-Projects\\GIT - ML - Projects\\ML_Projects\\NLP - Sentiment Analysis\\AnalyticsVidhya - Mobile Customer Feedback\\train.csv\")\n",
    "test_data=pd.read_csv(r\"C:\\Users\\gaurav.singh.rawal\\Documents\\Gaurav\\Data Science\\Machine Learning-Predictive Analytics\\ML-Projects\\GIT - ML - Projects\\ML_Projects\\NLP - Sentiment Analysis\\AnalyticsVidhya - Mobile Customer Feedback\\test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>#fingerprint #Pregnancy Test https://goo.gl/h1MfQV #android #apps #beautiful #cute #health #igers #iphoneonly #iphonesia #iphone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Finally a transparant silicon case ^^ Thanks to my uncle :) #yay #Sony #Xperia #S #sonyexperias… http://instagram.com/p/YGEt5JC6JM/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>We love this! Would you go? #talk #makememories #unplug #relax #iphone #smartphone #wifi #connect... http://fb.me/6N3LsUpCu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>I'm wired I know I'm George I was made that way ;) #iphone #cute #daventry #home http://instagr.am/p/Li_5_ujS4k/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>What amazing service! Apple won't even talk to me about a question I have unless I pay them $19.95 for their stupid support!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label  \\\n",
       "0  1   0       \n",
       "1  2   0       \n",
       "2  3   0       \n",
       "3  4   0       \n",
       "4  5   1       \n",
       "\n",
       "                                                                                                                                 tweet  \n",
       "0  #fingerprint #Pregnancy Test https://goo.gl/h1MfQV #android #apps #beautiful #cute #health #igers #iphoneonly #iphonesia #iphone     \n",
       "1  Finally a transparant silicon case ^^ Thanks to my uncle :) #yay #Sony #Xperia #S #sonyexperias… http://instagram.com/p/YGEt5JC6JM/  \n",
       "2  We love this! Would you go? #talk #makememories #unplug #relax #iphone #smartphone #wifi #connect... http://fb.me/6N3LsUpCu          \n",
       "3  I'm wired I know I'm George I was made that way ;) #iphone #cute #daventry #home http://instagr.am/p/Li_5_ujS4k/                     \n",
       "4  What amazing service! Apple won't even talk to me about a question I have unless I pay them $19.95 for their stupid support!         "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7920 entries, 0 to 7919\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      7920 non-null   int64 \n",
      " 1   label   7920 non-null   int64 \n",
      " 2   tweet   7920 non-null   object\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 185.8+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_818e16c6_af94_11ea_9bdc_c8f75041469arow0_col0 {\n",
       "            background-color:  #fcfbfd;\n",
       "            color:  #000000;\n",
       "        }    #T_818e16c6_af94_11ea_9bdc_c8f75041469arow0_col1 {\n",
       "            background-color:  #3f007d;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_818e16c6_af94_11ea_9bdc_c8f75041469arow1_col0 {\n",
       "            background-color:  #3f007d;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_818e16c6_af94_11ea_9bdc_c8f75041469arow1_col1 {\n",
       "            background-color:  #fcfbfd;\n",
       "            color:  #000000;\n",
       "        }</style><table id=\"T_818e16c6_af94_11ea_9bdc_c8f75041469a\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >label</th>        <th class=\"col_heading level0 col1\" >tweet</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_818e16c6_af94_11ea_9bdc_c8f75041469alevel0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_818e16c6_af94_11ea_9bdc_c8f75041469arow0_col0\" class=\"data row0 col0\" >0</td>\n",
       "                        <td id=\"T_818e16c6_af94_11ea_9bdc_c8f75041469arow0_col1\" class=\"data row0 col1\" >5894</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_818e16c6_af94_11ea_9bdc_c8f75041469alevel0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_818e16c6_af94_11ea_9bdc_c8f75041469arow1_col0\" class=\"data row1 col0\" >1</td>\n",
       "                        <td id=\"T_818e16c6_af94_11ea_9bdc_c8f75041469arow1_col1\" class=\"data row1 col1\" >2026</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x22f208849c8>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = train_data.groupby('label').count()['tweet'].reset_index().sort_values(by='tweet',ascending=False)\n",
    "temp.style.background_gradient(cmap='Purples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x22f177d0908>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='label',data=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['tweet']=train_data['tweet'].apply(expand_contractions)\n",
    "test_data['tweet']=test_data['tweet'].apply(expand_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>#fingerprint #Pregnancy Test https://goo.gl/h1MfQV #android #apps #beautiful #cute #health #igers #iphoneonly #iphonesia #iphone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Finally a transparant silicon case ^^ Thanks to my uncle :) #yay #Sony #Xperia #S #sonyexperias… http://instagram.com/p/YGEt5JC6JM/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>We love this! Would you go? #talk #makememories #unplug #relax #iphone #smartphone #wifi #connect... http://fb.me/6N3LsUpCu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>I am wired I know I am George I was made that way ;) #iphone #cute #daventry #home http://instagr.am/p/Li_5_ujS4k/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>What amazing service! Apple will not even talk to me about a question I have unless I pay them $19.95 for their stupid support!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label  \\\n",
       "0  1   0       \n",
       "1  2   0       \n",
       "2  3   0       \n",
       "3  4   0       \n",
       "4  5   1       \n",
       "\n",
       "                                                                                                                                 tweet  \n",
       "0  #fingerprint #Pregnancy Test https://goo.gl/h1MfQV #android #apps #beautiful #cute #health #igers #iphoneonly #iphonesia #iphone     \n",
       "1  Finally a transparant silicon case ^^ Thanks to my uncle :) #yay #Sony #Xperia #S #sonyexperias… http://instagram.com/p/YGEt5JC6JM/  \n",
       "2  We love this! Would you go? #talk #makememories #unplug #relax #iphone #smartphone #wifi #connect... http://fb.me/6N3LsUpCu          \n",
       "3  I am wired I know I am George I was made that way ;) #iphone #cute #daventry #home http://instagr.am/p/Li_5_ujS4k/                   \n",
       "4  What amazing service! Apple will not even talk to me about a question I have unless I pay them $19.95 for their stupid support!      "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanText(BaseEstimator, TransformerMixin):\n",
    "    def remove_mentions(self, input_text):\n",
    "        return re.sub(r'@\\w+', '', input_text)\n",
    "    \n",
    "    def remove_urls(self, input_text):\n",
    "        return re.sub(r'http.?://[^\\s]+[\\s]?', '', input_text)\n",
    "    \n",
    "    def emoji_oneword(self, input_text):\n",
    "        # By compressing the underscore, the emoji is kept as one word\n",
    "        return input_text.replace('_','')\n",
    "    \n",
    "    def remove_punctuation(self, input_text):\n",
    "        # Make translation table\n",
    "        punct = string.punctuation\n",
    "        trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space\n",
    "        return input_text.translate(trantab)\n",
    "    def remove_digits(self, input_text):\n",
    "        return re.sub('\\d+', '', input_text)\n",
    "    \n",
    "    def split_words(self,input_text):\n",
    "        return ''.join(re.findall('[A-Z][^A-Z]*', input_text))\n",
    "    \n",
    "    def standardize_words(self,input_text):\n",
    "        return ''.join(''.join(s)[:2] for _, s in itertools.groupby(input_text))\n",
    "    \n",
    "    def to_lower(self, input_text):\n",
    "        return input_text.lower()\n",
    "    \n",
    "    '''def correct_spell(self,input_text):\n",
    "        words = input_text.split() \n",
    "        clean_words = [str(TextBlob(word).correct()) for word in words] \n",
    "        return \" \".join(clean_words)'''\n",
    "    \n",
    "    \"\"\"def correct_spell(self,input_text):\n",
    "        words = input_text.split() \n",
    "        spell = Speller(lang='en') \n",
    "        correct_words = [spell(word) for word in words] \n",
    "        return \" \".join(correct_words) \"\"\"\n",
    "    '''def remove_meaningless(self,input_text):\n",
    "        words = set(nltk.corpus.words.words())\n",
    "        return \" \".join(w for w in nltk.wordpunct_tokenize(input_text) if w.lower() in words or not w.isalpha())'''\n",
    "    \n",
    "    def remove_stopwords(self, input_text):\n",
    "        stopwords_list = stopwords.words('english')\n",
    "        # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "        whitelist = [\"n't\", \"not\", \"no\"]\n",
    "        words = input_text.split() \n",
    "        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
    "        return \" \".join(clean_words) \n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        clean_X = X.apply(self.remove_mentions).apply(self.remove_urls).apply(self.emoji_oneword).apply(self.remove_punctuation).apply(self.remove_digits).apply(self.split_words).apply(self.standardize_words).apply(self.to_lower).apply(self.remove_stopwords)\n",
    "        return clean_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = CleanText()\n",
    "train_data['tweet'] = ct.fit_transform(train_data.tweet)\n",
    "test_data['tweet']=ct.fit_transform(test_data.tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of words in the original_text ##\n",
    "train_data[\"num_words\"] = train_data[\"tweet\"].apply(lambda x: len(str(x).split()))\n",
    "test_data[\"num_words\"] = test_data[\"tweet\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "## Number of unique words in the original_text ##\n",
    "train_data[\"num_unique_words\"] = train_data[\"tweet\"].apply(lambda x: len(set(str(x).split())))\n",
    "test_data[\"num_unique_words\"] = test_data[\"tweet\"].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "## Number of characters in the original_text ##\n",
    "train_data[\"num_chars\"] = train_data[\"tweet\"].apply(lambda x: len(str(x)))\n",
    "test_data[\"num_chars\"] = test_data[\"tweet\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "## Number of stopwords in the original_text ##\n",
    "train_data[\"num_stopwords\"] = train_data[\"tweet\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "test_data[\"num_stopwords\"] = test_data[\"tweet\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "\n",
    "## Number of punctuations in the original_text ##\n",
    "train_data[\"num_punctuations\"] =train_data['tweet'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "test_data[\"num_punctuations\"] =test_data['tweet'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "\n",
    "## Number of title case words in the original_text ##\n",
    "train_data[\"num_words_upper\"] = train_data[\"tweet\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "test_data[\"num_words_upper\"] = test_data[\"tweet\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "\n",
    "## Number of title case words in the original_text ##\n",
    "train_data[\"num_words_title\"] = train_data[\"tweet\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "test_data[\"num_words_title\"] = test_data[\"tweet\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "\n",
    "## Average length of the words in the original_text ##\n",
    "train_data[\"mean_word_len\"] = train_data[\"tweet\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "test_data[\"mean_word_len\"] = test_data[\"tweet\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.WordPunctTokenizer()\n",
    "def pos_tagger(input_text):\n",
    "    text = tokenizer.tokenize(input_text)\n",
    "    tagged_text = nltk.pos_tag(text)\n",
    "    new_text = \"\"\n",
    "    for tag in tagged_text:\n",
    "        new_text = new_text + \" \" + tag[1]\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['pos_text'] = train_data['tweet'].apply(pos_tagger)\n",
    "test_data['pos_text'] = test_data['tweet'].apply(pos_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import  SnowballStemmer\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "def stemming(input_text):\n",
    "        tokenizer = ToktokTokenizer()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        tokens = tokenizer.tokenize(input_text)\n",
    "        tokens = [token.strip() for token in tokens]\n",
    "        stemmed_words = [stemmer.stem(token) for token in tokens]\n",
    "        return ' '.join(stemmed_words)\n",
    "    \n",
    "def lemmatize_text(input_text):\n",
    "        word_list = nltk.word_tokenize(input_text)\n",
    "        lemmatizer = WordNetLemmatizer() \n",
    "        lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "\n",
    "        return lemmatized_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['clean_text'] = train_data['tweet'].apply(stemming)\n",
    "test_data['clean_text'] = test_data['tweet'].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['clean_text'] = train_data['clean_text'].apply(lemmatize_text)\n",
    "test_data['clean_text'] = test_data['clean_text'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>mean_word_len</th>\n",
       "      <th>pos_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7921</td>\n",
       "      <td>hate new iphone upgrade not let download apps ugh apple sucks</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.636364</td>\n",
       "      <td>VB JJ NN NN RB VB NN IN JJ NN NNS</td>\n",
       "      <td>hate new iphon upgrad not let download app ugh appl suck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7922</td>\n",
       "      <td>mac cashmoney raddest swagswagswag</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.750000</td>\n",
       "      <td>NN NN NN NN</td>\n",
       "      <td>mac cashmoney raddest swagswagswag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7923</td>\n",
       "      <td>would like puts cd roms ipad possible yes would not block screen</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.416667</td>\n",
       "      <td>MD VB JJ NN NNS VBP JJ NNS MD RB VB NN</td>\n",
       "      <td>would like put cd rom ipad possibl yes would not block screen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7924</td>\n",
       "      <td>ipod officially dead lost pictures videos sos concert vet camp hatinglife sobbing</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.833333</td>\n",
       "      <td>JJ RB JJ VBN NNS VBP JJ NN NN NN NN VBG</td>\n",
       "      <td>ipod offici dead lost pictur video so concert vet camp hatinglif sob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7925</td>\n",
       "      <td>fighting itunes night want music paid</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>VBG NNS NN VBP NN NN</td>\n",
       "      <td>fight itun night want music paid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  \\\n",
       "0  7921   \n",
       "1  7922   \n",
       "2  7923   \n",
       "3  7924   \n",
       "4  7925   \n",
       "\n",
       "                                                                               tweet  \\\n",
       "0  hate new iphone upgrade not let download apps ugh apple sucks                       \n",
       "1  mac cashmoney raddest swagswagswag                                                  \n",
       "2  would like puts cd roms ipad possible yes would not block screen                    \n",
       "3  ipod officially dead lost pictures videos sos concert vet camp hatinglife sobbing   \n",
       "4  fighting itunes night want music paid                                               \n",
       "\n",
       "   num_words  num_unique_words  num_chars  num_stopwords  num_punctuations  \\\n",
       "0  11         11                61         1              0                  \n",
       "1  4          4                 34         0              0                  \n",
       "2  12         11                64         1              0                  \n",
       "3  12         12                81         0              0                  \n",
       "4  6          6                 37         0              0                  \n",
       "\n",
       "   num_words_upper  num_words_title  mean_word_len  \\\n",
       "0  0                0                4.636364        \n",
       "1  0                0                7.750000        \n",
       "2  0                0                4.416667        \n",
       "3  0                0                5.833333        \n",
       "4  0                0                5.333333        \n",
       "\n",
       "                                   pos_text  \\\n",
       "0   VB JJ NN NN RB VB NN IN JJ NN NNS         \n",
       "1   NN NN NN NN                               \n",
       "2   MD VB JJ NN NNS VBP JJ NNS MD RB VB NN    \n",
       "3   JJ RB JJ VBN NNS VBP JJ NN NN NN NN VBG   \n",
       "4   VBG NNS NN VBP NN NN                      \n",
       "\n",
       "                                                             clean_text  \n",
       "0  hate new iphon upgrad not let download app ugh appl suck              \n",
       "1  mac cashmoney raddest swagswagswag                                    \n",
       "2  would like put cd rom ipad possibl yes would not block screen         \n",
       "3  ipod offici dead lost pictur video so concert vet camp hatinglif sob  \n",
       "4  fight itun night want music paid                                      "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnExtractor(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "    def transform(self, X, **transform_params):\n",
    "        return X[self.cols]\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_data.label\n",
    "X = train_data.drop(['id','tweet'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=train_data.drop(['id','tweet'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_id=test_data.id\n",
    "test_df=test_data.drop(['id','tweet'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>mean_word_len</th>\n",
       "      <th>pos_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.636364</td>\n",
       "      <td>VB JJ NN NN RB VB NN IN JJ NN NNS</td>\n",
       "      <td>hate new iphon upgrad not let download app ugh appl suck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.750000</td>\n",
       "      <td>NN NN NN NN</td>\n",
       "      <td>mac cashmoney raddest swagswagswag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.416667</td>\n",
       "      <td>MD VB JJ NN NNS VBP JJ NNS MD RB VB NN</td>\n",
       "      <td>would like put cd rom ipad possibl yes would not block screen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.833333</td>\n",
       "      <td>JJ RB JJ VBN NNS VBP JJ NN NN NN NN VBG</td>\n",
       "      <td>ipod offici dead lost pictur video so concert vet camp hatinglif sob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>VBG NNS NN VBP NN NN</td>\n",
       "      <td>fight itun night want music paid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_words  num_unique_words  num_chars  num_stopwords  num_punctuations  \\\n",
       "0  11         11                61         1              0                  \n",
       "1  4          4                 34         0              0                  \n",
       "2  12         11                64         1              0                  \n",
       "3  12         12                81         0              0                  \n",
       "4  6          6                 37         0              0                  \n",
       "\n",
       "   num_words_upper  num_words_title  mean_word_len  \\\n",
       "0  0                0                4.636364        \n",
       "1  0                0                7.750000        \n",
       "2  0                0                4.416667        \n",
       "3  0                0                5.833333        \n",
       "4  0                0                5.333333        \n",
       "\n",
       "                                   pos_text  \\\n",
       "0   VB JJ NN NN RB VB NN IN JJ NN NNS         \n",
       "1   NN NN NN NN                               \n",
       "2   MD VB JJ NN NNS VBP JJ NNS MD RB VB NN    \n",
       "3   JJ RB JJ VBN NNS VBP JJ NN NN NN NN VBG   \n",
       "4   VBG NNS NN VBP NN NN                      \n",
       "\n",
       "                                                             clean_text  \n",
       "0  hate new iphon upgrad not let download app ugh appl suck              \n",
       "1  mac cashmoney raddest swagswagswag                                    \n",
       "2  would like put cd rom ipad possibl yes would not block screen         \n",
       "3  ipod offici dead lost pictur video so concert vet camp hatinglif sob  \n",
       "4  fight itun night want music paid                                      "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runMNB(train_X, train_y, test_X, test_y, test_X2):\n",
    "    model = MultinomialNB()\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_test_y = model.predict_proba(test_X)\n",
    "    pred_test_y2 = model.predict_proba(test_X2)\n",
    "    return pred_test_y, pred_test_y2, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fit transform the tfidf vectorizer ###\n",
    "tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "full_tfidf = tfidf_vec.fit_transform(train_df['clean_text'].values.tolist() + test_df['clean_text'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(train_df['clean_text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(test_df['clean_text'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp = 20\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_tfidf)\n",
    "train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n",
    "test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n",
    "    \n",
    "train_svd.columns = ['svd_char_'+str(i) for i in range(n_comp)]\n",
    "test_svd.columns = ['svd_char_'+str(i) for i in range(n_comp)]\n",
    "train_df = pd.concat([train_df, train_svd], axis=1)\n",
    "test_df = pd.concat([test_df, test_svd], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label', 'num_words', 'num_unique_words', 'num_chars', 'num_stopwords',\n",
       "       'num_punctuations', 'num_words_upper', 'num_words_title',\n",
       "       'mean_word_len', 'pos_text', 'clean_text', 'svd_char_0', 'svd_char_1',\n",
       "       'svd_char_2', 'svd_char_3', 'svd_char_4', 'svd_char_5', 'svd_char_6',\n",
       "       'svd_char_7', 'svd_char_8', 'svd_char_9', 'svd_char_10', 'svd_char_11',\n",
       "       'svd_char_12', 'svd_char_13', 'svd_char_14', 'svd_char_15',\n",
       "       'svd_char_16', 'svd_char_17', 'svd_char_18', 'svd_char_19'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fit transform the count vectorizer ###\n",
    "cv_vec = CountVectorizer(stop_words='english', ngram_range=(1,2))\n",
    "cv_vec.fit(train_df['clean_text'].values.tolist() + test_df['clean_text'].values.tolist())\n",
    "train_cv = cv_vec.transform(train_df['clean_text'].values.tolist())\n",
    "test_cv = cv_vec.transform(test_df['clean_text'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train_df.shape[0], 2])\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for dev_index, val_index in kf.split(X_train):\n",
    "    dev_X, val_X = train_cv[dev_index], train_cv[val_index]\n",
    "    dev_y, val_y = y_train.iloc[dev_index], y_train.iloc[val_index]\n",
    "    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_cv)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the predictions as new features #\n",
    "train_df[\"nb_cv_sent_pos\"] = pred_train[:,0]\n",
    "train_df[\"nb_cv_sent_neg\"] = pred_train[:,1]\n",
    "\n",
    "test_df[\"nb_cv_sent_pos\"] = pred_full_test[:,0]\n",
    "test_df[\"nb_cv_sent_neg\"] = pred_full_test[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fit transform the tfidf vectorizer ###\n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(1,5), analyzer='char')\n",
    "tfidf_vec.fit(train_df['clean_text'].values.tolist() + test_df['clean_text'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(train_df['clean_text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(test_df['clean_text'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train_df.shape[0], 2])\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "for dev_index, val_index in kf.split(X_train):\n",
    "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "    dev_y, val_y = y_train.iloc[dev_index], y_train.iloc[val_index]\n",
    "    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the predictions as new features #\n",
    "train_df[\"nb_tfidf_char_pos\"] = pred_train[:,0]\n",
    "train_df[\"nb_tfidf_char_neg\"] = pred_train[:,1]\n",
    "\n",
    "test_df[\"nb_tfidf_char_pos\"] = pred_full_test[:,0]\n",
    "test_df[\"nb_tfidf_char_neg\"] = pred_full_test[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fit transform the tfidf vectorizer ###\n",
    "tfidf_vec = CountVectorizer(ngram_range=(1,5), analyzer='char')\n",
    "tfidf_vec.fit(train_df['clean_text'].values.tolist() + test_df['clean_text'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(train_df['clean_text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(test_df['clean_text'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train_df.shape[0], 2])\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "for dev_index, val_index in kf.split(X_train):\n",
    "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "    dev_y, val_y = y_train.iloc[dev_index], y_train.iloc[val_index]\n",
    "    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the predictions as new features #\n",
    "train_df[\"nb_cvec_char_pos\"] = pred_train[:,0]\n",
    "train_df[\"nb_cvec_char_neg\"] = pred_train[:,1]\n",
    "\n",
    "test_df[\"nb_cvec_char_pos\"] = pred_full_test[:,0]\n",
    "test_df[\"nb_cvec_char_neg\"] = pred_full_test[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = MinMaxScaler()\n",
    "train_df[['nb_cv_sent_pos','nb_cv_sent_neg',\"nb_tfidf_char_pos\", \"nb_tfidf_char_neg\",\"nb_cvec_char_pos\",\n",
    "          \"nb_cvec_char_neg\",'svd_char_0', 'svd_char_1', 'svd_char_2', 'svd_char_3', 'svd_char_4','svd_char_5', \n",
    "        'svd_char_6', 'svd_char_7', 'svd_char_8', 'svd_char_9','svd_char_10', 'svd_char_11', 'svd_char_12', 'svd_char_13','svd_char_14', 'svd_char_15', \n",
    "                      'svd_char_16', 'svd_char_17','svd_char_18', 'svd_char_19']]=train_df[['nb_cv_sent_pos','nb_cv_sent_neg',\"nb_tfidf_char_pos\", \"nb_tfidf_char_neg\",\"nb_cvec_char_pos\",\n",
    "          \"nb_cvec_char_neg\",'svd_char_0', 'svd_char_1', 'svd_char_2', 'svd_char_3', 'svd_char_4','svd_char_5', \n",
    "                      'svd_char_6', 'svd_char_7', 'svd_char_8', 'svd_char_9','svd_char_10', 'svd_char_11', 'svd_char_12', 'svd_char_13','svd_char_14', 'svd_char_15', \n",
    "                      'svd_char_16', 'svd_char_17','svd_char_18', 'svd_char_19']].values.astype(float)\n",
    "train_df[['nb_cv_sent_pos','nb_cv_sent_neg',\"nb_tfidf_char_pos\", \"nb_tfidf_char_neg\",\"nb_cvec_char_pos\",\n",
    "          \"nb_cvec_char_neg\",'svd_char_0', 'svd_char_1', 'svd_char_2', 'svd_char_3', 'svd_char_4','svd_char_5', \n",
    "            'svd_char_6', 'svd_char_7', 'svd_char_8', 'svd_char_9','svd_char_10', 'svd_char_11', 'svd_char_12', 'svd_char_13','svd_char_14', 'svd_char_15', \n",
    "                      'svd_char_16', 'svd_char_17','svd_char_18', 'svd_char_19']] = min_max_scaler.fit_transform(train_df[['nb_cv_sent_pos','nb_cv_sent_neg',\"nb_tfidf_char_pos\", \"nb_tfidf_char_neg\",\"nb_cvec_char_pos\",\n",
    "          \"nb_cvec_char_neg\",'svd_char_0', 'svd_char_1', 'svd_char_2', 'svd_char_3', 'svd_char_4','svd_char_5', \n",
    "                      'svd_char_6', 'svd_char_7', 'svd_char_8', 'svd_char_9',\n",
    "                    'svd_char_10', 'svd_char_11', 'svd_char_12', 'svd_char_13','svd_char_14', 'svd_char_15', \n",
    "                      'svd_char_16', 'svd_char_17','svd_char_18', 'svd_char_19']])\n",
    "test_df[['nb_cv_sent_pos','nb_cv_sent_neg',\"nb_tfidf_char_pos\", \"nb_tfidf_char_neg\",\"nb_cvec_char_pos\",\n",
    "          \"nb_cvec_char_neg\",'svd_char_0', 'svd_char_1', 'svd_char_2', 'svd_char_3', 'svd_char_4','svd_char_5', \n",
    "                      'svd_char_6', 'svd_char_7', 'svd_char_8', 'svd_char_9',\n",
    "                    'svd_char_10', 'svd_char_11', 'svd_char_12', 'svd_char_13','svd_char_14', 'svd_char_15', \n",
    "                      'svd_char_16', 'svd_char_17','svd_char_18', 'svd_char_19']]=test_df[['nb_cv_sent_pos','nb_cv_sent_neg',\"nb_tfidf_char_pos\", \"nb_tfidf_char_neg\",\"nb_cvec_char_pos\",\n",
    "          \"nb_cvec_char_neg\",'svd_char_0', 'svd_char_1', 'svd_char_2', 'svd_char_3', 'svd_char_4','svd_char_5', \n",
    "                      'svd_char_6', 'svd_char_7', 'svd_char_8', 'svd_char_9','svd_char_10', 'svd_char_11', 'svd_char_12', 'svd_char_13','svd_char_14', 'svd_char_15', \n",
    "                      'svd_char_16', 'svd_char_17','svd_char_18', 'svd_char_19']].values.astype(float)\n",
    "test_df[['nb_cv_sent_pos','nb_cv_sent_neg',\"nb_tfidf_char_pos\", \"nb_tfidf_char_neg\",\"nb_cvec_char_pos\",\n",
    "          \"nb_cvec_char_neg\",'svd_char_0', 'svd_char_1', 'svd_char_2', 'svd_char_3', 'svd_char_4','svd_char_5', \n",
    "        'svd_char_6', 'svd_char_7', 'svd_char_8', 'svd_char_9','svd_char_10', 'svd_char_11', 'svd_char_12', 'svd_char_13','svd_char_14', 'svd_char_15', \n",
    "                      'svd_char_16', 'svd_char_17','svd_char_18', 'svd_char_19']] = min_max_scaler.fit_transform(test_df[['nb_cv_sent_pos','nb_cv_sent_neg',\"nb_tfidf_char_pos\", \"nb_tfidf_char_neg\",\"nb_cvec_char_pos\",\n",
    "          \"nb_cvec_char_neg\",'svd_char_0', 'svd_char_1', 'svd_char_2', 'svd_char_3', 'svd_char_4','svd_char_5', \n",
    "                      'svd_char_6', 'svd_char_7', 'svd_char_8', 'svd_char_9',\n",
    "                    'svd_char_10', 'svd_char_11', 'svd_char_12', 'svd_char_13','svd_char_14', 'svd_char_15', \n",
    "                      'svd_char_16', 'svd_char_17','svd_char_18', 'svd_char_19']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['mean_word_len']=train_df['mean_word_len'].fillna(train_df['mean_word_len'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_df.label\n",
    "X = train_df.drop(['label'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['num_words', 'num_unique_words', 'num_chars', 'num_stopwords',\n",
       "       'num_punctuations', 'num_words_upper', 'num_words_title',\n",
       "       'mean_word_len', 'pos_text', 'clean_text', 'svd_char_0', 'svd_char_1',\n",
       "       'svd_char_2', 'svd_char_3', 'svd_char_4', 'svd_char_5', 'svd_char_6',\n",
       "       'svd_char_7', 'svd_char_8', 'svd_char_9', 'svd_char_10', 'svd_char_11',\n",
       "       'svd_char_12', 'svd_char_13', 'svd_char_14', 'svd_char_15',\n",
       "       'svd_char_16', 'svd_char_17', 'svd_char_18', 'svd_char_19',\n",
       "       'nb_cv_sent_pos', 'nb_cv_sent_neg', 'nb_tfidf_char_pos',\n",
       "       'nb_tfidf_char_neg', 'nb_cvec_char_pos', 'nb_cvec_char_neg'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "textcountscols = ['num_words','num_unique_words','num_chars','num_stopwords','num_punctuations','num_words_upper','num_words_title','mean_word_len',\n",
    "                      'nb_cv_sent_pos','nb_cv_sent_neg',\"nb_tfidf_char_pos\",\"nb_tfidf_char_neg\",\"nb_cvec_char_pos\",\"nb_cvec_char_neg\",\n",
    "                     'svd_char_0', 'svd_char_1', 'svd_char_2', 'svd_char_3', 'svd_char_4','svd_char_5', \n",
    "                      'svd_char_6', 'svd_char_7', 'svd_char_8', 'svd_char_9',\n",
    "                    'svd_char_10', 'svd_char_11', 'svd_char_12', 'svd_char_13','svd_char_14', 'svd_char_15', \n",
    "                      'svd_char_16', 'svd_char_17','svd_char_18', 'svd_char_19']\n",
    "    \n",
    "# Based on http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html\n",
    "def gen_cv_features(cvect=None, is_w2v=None):\n",
    "   \n",
    "    if is_w2v:\n",
    "        w2vcols = []\n",
    "        for i in range(SIZE):\n",
    "            w2vcols.append(i)\n",
    "        features = FeatureUnion([('textcounts', ColumnExtractor(cols=textcountscols))\n",
    "                                 , ('w2v', ColumnExtractor(cols=w2vcols))], n_jobs=-1)\n",
    "    else:\n",
    "        features = FeatureUnion([('textcounts', ColumnExtractor(cols=textcountscols))\n",
    "                                 ,('pipe1', Pipeline([('cleantext', ColumnExtractor(cols='clean_text'))         \n",
    "                                                     ,('vect', cvect)])),\n",
    "                                ('pipe2', Pipeline([('postext', ColumnExtractor(cols='pos_text'))         \n",
    "                                                     ,('vect', cvect)]))]\n",
    "                                , n_jobs=-1)\n",
    "    return features\n",
    "\n",
    "def gen_tfidf_features(tfidf=None, is_w2v=None):\n",
    "   \n",
    "    if is_w2v:\n",
    "        w2vcols = []\n",
    "        for i in range(SIZE):\n",
    "            w2vcols.append(i)\n",
    "        features = FeatureUnion([('textcounts', ColumnExtractor(cols=textcountscols))\n",
    "                                 , ('w2v', ColumnExtractor(cols=w2vcols))], n_jobs=-1)\n",
    "    else:\n",
    "        features = FeatureUnion([('textcounts', ColumnExtractor(cols=textcountscols))\n",
    "                                 ,('pipe1', Pipeline([('cleantext', ColumnExtractor(cols='clean_text'))         \n",
    "                                                     ,('vect', tfidf)])),\n",
    "                                ('pipe2', Pipeline([('postext', ColumnExtractor(cols='pos_text'))         \n",
    "                                                     ,('vect', tfidf)]))]\n",
    "                                , n_jobs=-1)\n",
    "    return features\n",
    "\n",
    "def grid_vect(clf, parameters_clf, X_train, X_test, features, parameters_text=None):\n",
    "    pipeline = Pipeline([('features', features),('clf', clf)])\n",
    "    \n",
    "    # Join the parameters dictionaries together\n",
    "    parameters = dict()\n",
    "    if parameters_text:\n",
    "        parameters.update(parameters_text)\n",
    "    parameters.update(parameters_clf)\n",
    "\n",
    "    grid_search_model = RandomizedSearchCV(pipeline, parameters, n_jobs=-1, scoring='f1_weighted',verbose=1, cv=5,random_state=42)\n",
    "    \n",
    "    print(\"Performing grid search...\")\n",
    "    grid_search_model.fit(X_train, y_train)\n",
    "    print(\"Best CV score: %0.3f\" % grid_search_model.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search_model.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "        \n",
    "    print(\"Test score with best_estimator_: %0.3f\" % grid_search_model.best_estimator_.score(X_test, y_test))\n",
    "                 \n",
    "    return grid_search_model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid settings for the vectorizers (Count and TFIDF)\n",
    "parameters_cvect = {\n",
    "    'features__pipe1__vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'features__pipe1__vect__ngram_range': ((1, 1), (1, 2),(1,3)),\n",
    "    'features__pipe1__vect__min_df': (1,2,3,4,5),\n",
    "    'features__pipe2__vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'features__pipe2__vect__ngram_range': ((1, 1), (1, 2),(1,3),(2,2)),\n",
    "    'features__pipe2__vect__min_df': (1,2,3,4,5)\n",
    "}\n",
    "parameters_tfidf = {\n",
    "    'features__pipe1__vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'features__pipe1__vect__ngram_range': ((1, 1), (1, 2),(1,3)),\n",
    "    'features__pipe1__vect__min_df': (1,2,3,4,5),\n",
    "    'features__pipe1__vect__use_idf': (0,1),\n",
    "    'features__pipe1__vect__smooth_idf': (0,1),\n",
    "    'features__pipe1__vect__sublinear_tf': (0,1),\n",
    "    'features__pipe2__vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'features__pipe2__vect__ngram_range': ((1, 1), (1, 2),(1,3),(2,2)),\n",
    "    'features__pipe2__vect__min_df': (1,2,3,4,5),\n",
    "    'features__pipe2__vect__use_idf': (0,1),\n",
    "    'features__pipe2__vect__smooth_idf': (0,1),\n",
    "    'features__pipe2__vect__sublinear_tf': (0,1)\n",
    "}\n",
    "# Parameter grid settings for MultinomialNB\n",
    "parameters_mnb = {\n",
    "    'clf__alpha': (0.01,0.1, 0.25, 0.5, 0.75),\n",
    "    'clf__fit_prior' : (True,False)\n",
    "}\n",
    "parameters_gnb = {\n",
    "    'clf__var_smoothing': (0.000001,0.00001,0.001,0.0001,0.01,0.1,1,0.25, 0.5, 0.75)\n",
    "}\n",
    "\n",
    "# Parameter grid settings for LogisticRegression\n",
    "parameters_logreg = {\n",
    "    'clf__C': (0.25, 0.5, 1.0),\n",
    "    'clf__penalty': ('l1', 'l2'),\n",
    "    'clf__fit_intercept': (True,False),\n",
    "    'clf__class_weight':['balanced',None]\n",
    "}\n",
    "parameters_sgd = {\n",
    "    'clf__alpha': (0.00001, 0.000001),\n",
    "    'clf__penalty': ('l1', 'l2','elasticnet'),\n",
    "    'clf__max_iter': (10, 25, 50, 75, 100),\n",
    "    'clf__class_weight':['balanced',None]\n",
    "}\n",
    "parameters_svc = {'clf__kernel': ['linear', 'rbf'],\n",
    "                    'clf__gamma':[0.1,1,10],\n",
    "                    'clf__C':[0.1,1,0.001,0.0001],\n",
    "                  'clf__class_weight':['balanced',None]\n",
    "                 }\n",
    "parameters_linsvc = {'clf__loss' : ['hinge', 'squared_hinge'],\n",
    "                    'clf__penalty': ('l1', 'l2'),\n",
    "                    'clf__class_weight':['balanced',None],\n",
    "                    'clf__max_iter':[100,500],\n",
    "                    'clf__C':[0.1,1,0.001,0.0001]               \n",
    "                 }\n",
    "\n",
    "parameters_gbm = {'clf__loss':['deviance','exponential'],\n",
    "         'clf__learning_rate': [0.05,0.1],\n",
    "         'clf__max_depth':[5,10,20],\n",
    "         'clf__n_estimators': [10,25,50]\n",
    "        }\n",
    "parameters_xgb = {'clf__learning_rate':[0.01,0.1],\n",
    "         'clf__n_estimators':[50,100,500],\n",
    "         'clf__max_depth':[5,10,15,20],\n",
    "        'clf__class_weight':['balanced',None]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_scores={}\n",
    "best_models={}\n",
    "mnb = MultinomialNB()\n",
    "logreg = LogisticRegression()\n",
    "sgd=SGDClassifier()\n",
    "svc=SVC()\n",
    "linsvc = LinearSVC()\n",
    "gbm=GradientBoostingClassifier()\n",
    "xgb_model=xgb.XGBClassifier(objective=\"binary:logistic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   14.2s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   16.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.853\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.25\n",
      "\tclf__fit_prior: True\n",
      "\tfeatures__pipe1__vect__max_df: 0.75\n",
      "\tfeatures__pipe1__vect__min_df: 2\n",
      "\tfeatures__pipe1__vect__ngram_range: (1, 2)\n",
      "\tfeatures__pipe2__vect__max_df: 0.75\n",
      "\tfeatures__pipe2__vect__min_df: 2\n",
      "\tfeatures__pipe2__vect__ngram_range: (1, 1)\n",
      "Test score with best_estimator_: 0.846\n",
      "\n",
      "Accuracy score of Multinomial naive bayes algorithm -----> 0.8529862837457418\n"
     ]
    }
   ],
   "source": [
    "countvect = CountVectorizer()\n",
    "tfidfvect = TfidfVectorizer()\n",
    "# MultinomialNB\n",
    "model_features=gen_model_features(countvect)\n",
    "best_mnb_countvect = grid_vect(mnb, parameters_mnb, X_train, X_test,model_features, parameters_text=parameters_vect)\n",
    "prediction_mnb=best_mnb_countvect.predict(X_test)\n",
    "mnb_score=f1_score(y_test,prediction_mnb,average='weighted')\n",
    "#best_scores['mnb_countvect']=best_mnb_countvect.best_score_\n",
    "#best_models['mnb_countvect']=best_mnb_countvect\n",
    "train_pred1=best_mnb_countvect.predict(X_train)\n",
    "test_pred1=best_mnb_countvect.predict(X_test)\n",
    "print(\"\\nAccuracy score of Multinomial naive bayes algorithm -----> \" + str(mnb_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   20.7s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   23.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.859\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.01\n",
      "\tclf__fit_prior: True\n",
      "\tfeatures__pipe1__vect__max_df: 1.0\n",
      "\tfeatures__pipe1__vect__min_df: 1\n",
      "\tfeatures__pipe1__vect__ngram_range: (1, 2)\n",
      "\tfeatures__pipe1__vect__smooth_idf: 1\n",
      "\tfeatures__pipe1__vect__sublinear_tf: 1\n",
      "\tfeatures__pipe1__vect__use_idf: 0\n",
      "\tfeatures__pipe2__vect__max_df: 0.5\n",
      "\tfeatures__pipe2__vect__min_df: 2\n",
      "\tfeatures__pipe2__vect__ngram_range: (1, 1)\n",
      "\tfeatures__pipe2__vect__smooth_idf: 1\n",
      "\tfeatures__pipe2__vect__sublinear_tf: 1\n",
      "\tfeatures__pipe2__vect__use_idf: 0\n",
      "Test score with best_estimator_: 0.856\n",
      "\n",
      "Accuracy score of Multinomial NB tfidf algorithm -----> 0.860643839462491\n"
     ]
    }
   ],
   "source": [
    "tfidfvect = TfidfVectorizer()\n",
    "# MultinomialNB\n",
    "model_features=gen_tfidf_features(tfidfvect)\n",
    "best_mnb_tfidf = grid_vect(mnb, parameters_mnb, X_train, X_test,model_features, parameters_text=parameters_tfidf)\n",
    "prediction_mnb=best_mnb_tfidf.predict(X_test)\n",
    "mnb_score=f1_score(y_test,prediction_mnb,average='weighted')\n",
    "#best_scores['mnb_countvect']=best_mnb_countvect.best_score_\n",
    "#best_models['mnb_countvect']=best_mnb_countvect\n",
    "#train_pred1=best_mnb_countvect.predict(X_train)\n",
    "#test_pred1=best_mnb_countvect.predict(X_test)\n",
    "print(\"\\nAccuracy score of Multinomial NB tfidf algorithm -----> \" + str(mnb_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1953,)\n"
     ]
    }
   ],
   "source": [
    "#best_mnb_model=mnb.set_params(**best_mnb_countvect.best_params_)\n",
    "best_mnb_countvect.fit(X,y)\n",
    "test_predictions=best_mnb_countvect.predict(test_df)\n",
    "print(test_predictions.shape)\n",
    "prediction_df = pd.DataFrame(columns=['label'],data=test_predictions)\n",
    "prediction_df = pd.concat([test_data_id,prediction_df],axis=1)\n",
    "prediction_df.to_csv(r'C:\\Users\\gaurav.singh.rawal\\Documents\\Gaurav\\Data Science\\Machine Learning-Predictive Analytics\\ML-Projects\\GIT - ML - Projects\\ML_Projects\\NLP - Sentiment Analysis\\AnalyticsVidhya - Mobile Customer Feedback\\predictions_mnb.csv',index=False,encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions1=best_mnb_countvect.predict(test_df)\n",
    "prediction_df1 = pd.DataFrame(columns=['mnb'],data=test_predictions1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    6.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.856\n",
      "Best parameters set:\n",
      "\tclf__C: 0.25\n",
      "\tclf__class_weight: None\n",
      "\tclf__fit_intercept: True\n",
      "\tclf__penalty: 'l2'\n",
      "\tfeatures__pipe1__vect__max_df: 1.0\n",
      "\tfeatures__pipe1__vect__min_df: 3\n",
      "\tfeatures__pipe1__vect__ngram_range: (1, 3)\n",
      "\tfeatures__pipe2__vect__max_df: 0.5\n",
      "\tfeatures__pipe2__vect__min_df: 4\n",
      "\tfeatures__pipe2__vect__ngram_range: (2, 2)\n",
      "Test score with best_estimator_: 0.864\n",
      "\n",
      "Accuracy score of Log Reg algorithm -----> 0.8582659766610732\n"
     ]
    }
   ],
   "source": [
    "# LogisticRegression\n",
    "best_logreg_countvect = grid_vect(logreg, parameters_logreg, X_train, X_test,model_features, parameters_text=parameters_vect)\n",
    "prediction_logreg = best_logreg_countvect.predict(X_test)\n",
    "logreg_score=f1_score(y_test,prediction_logreg,average='weighted')\n",
    "#best_scores['logreg_countvect']=best_logreg_countvect.best_score_\n",
    "#best_models['logreg_countvect']=best_logreg_countvect\n",
    "train_pred2=best_logreg_countvect.predict(X_train)\n",
    "test_pred2=best_logreg_countvect.predict(X_test)\n",
    "print(\"\\nAccuracy score of Log Reg algorithm -----> \" + str(logreg_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   23.8s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   27.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.834\n",
      "Best parameters set:\n",
      "\tclf__C: 0.5\n",
      "\tclf__class_weight: None\n",
      "\tclf__fit_intercept: True\n",
      "\tclf__penalty: 'l2'\n",
      "\tfeatures__pipe1__vect__max_df: 0.75\n",
      "\tfeatures__pipe1__vect__min_df: 5\n",
      "\tfeatures__pipe1__vect__ngram_range: (1, 2)\n",
      "\tfeatures__pipe1__vect__smooth_idf: 1\n",
      "\tfeatures__pipe1__vect__sublinear_tf: 0\n",
      "\tfeatures__pipe1__vect__use_idf: 1\n",
      "\tfeatures__pipe2__vect__max_df: 0.5\n",
      "\tfeatures__pipe2__vect__min_df: 1\n",
      "\tfeatures__pipe2__vect__ngram_range: (1, 2)\n",
      "\tfeatures__pipe2__vect__smooth_idf: 0\n",
      "\tfeatures__pipe2__vect__sublinear_tf: 1\n",
      "\tfeatures__pipe2__vect__use_idf: 0\n",
      "Test score with best_estimator_: 0.855\n",
      "\n",
      "Accuracy score of Logreg tfidf algorithm -----> 0.8478516595553783\n"
     ]
    }
   ],
   "source": [
    "# MultinomialNB\n",
    "model_features=gen_tfidf_features(tfidfvect)\n",
    "best_logreg_tfidf = grid_vect(logreg, parameters_logreg, X_train, X_test,model_features, parameters_text=parameters_tfidf)\n",
    "prediction_logreg=best_logreg_tfidf.predict(X_test)\n",
    "logreg_score=f1_score(y_test,prediction_logreg,average='weighted')\n",
    "#best_scores['mnb_countvect']=best_mnb_countvect.best_score_\n",
    "#best_models['mnb_countvect']=best_mnb_countvect\n",
    "#train_pred1=best_mnb_countvect.predict(X_train)\n",
    "#test_pred1=best_mnb_countvect.predict(X_test)\n",
    "print(\"\\nAccuracy score of Logreg tfidf algorithm -----> \" + str(logreg_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions2=best_logreg_countvect.predict(test_df)\n",
    "prediction_df2 = pd.DataFrame(columns=['lr'],data=test_predictions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   54.8s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:  1.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.841\n",
      "Best parameters set:\n",
      "\tclf__learning_rate: 0.1\n",
      "\tclf__loss: 'exponential'\n",
      "\tclf__max_depth: 10\n",
      "\tclf__n_estimators: 50\n",
      "\tfeatures__pipe1__vect__max_df: 0.75\n",
      "\tfeatures__pipe1__vect__min_df: 2\n",
      "\tfeatures__pipe1__vect__ngram_range: (1, 1)\n",
      "\tfeatures__pipe2__vect__max_df: 0.5\n",
      "\tfeatures__pipe2__vect__min_df: 5\n",
      "\tfeatures__pipe2__vect__ngram_range: (1, 3)\n",
      "Test score with best_estimator_: 0.846\n",
      "\n",
      "Accuracy score of gbc algorithm -----> 0.8397256101127069\n"
     ]
    }
   ],
   "source": [
    "# GBC\n",
    "best_gbm_countvect = grid_vect(gbm, parameters_gbm, X_train, X_test,model_features, parameters_text=parameters_vect)\n",
    "prediction_gbm = best_gbm_countvect.predict(X_test)\n",
    "gbm_score=f1_score(y_test,prediction_gbm,average='weighted')\n",
    "#best_scores['gbm_countvect']=best_gbm_countvect.best_score_\n",
    "#best_models['gbm_countvect']=best_gbm_countvect\n",
    "train_pred4=best_gbm_countvect.predict(X_train)\n",
    "test_pred4=best_gbm_countvect.predict(X_test)\n",
    "print(\"\\nAccuracy score of gbc algorithm -----> \" + str(gbm_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_mnb_model=mnb.set_params(**best_mnb_countvect.best_params_)\n",
    "best_gbm_countvect.fit(X,y)\n",
    "gbm_predictions=best_gbm_countvect.predict(test_df)\n",
    "prediction_df = pd.DataFrame(columns=['sentiment_class'],data=gbm_predictions)\n",
    "prediction_df = pd.concat([test_df_id,prediction_df],axis=1)\n",
    "prediction_df.to_csv(r'C:\\Users\\gaurav.singh.rawal\\Documents\\Gaurav\\Data Science\\Machine Learning-Predictive Analytics\\ML-Projects\\GIT - ML - Projects\\ML_Projects\\NLP - Sentiment Analysis\\HackerRank - Mothers Day\\predictions.csv',index=False,encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sklearn.ensemble import VotingClassifier\\ncountvect_classifier= VotingClassifier(estimators=[(\\'mnb\\', best_models[\\'mnb_countvect\\']), (\\'lr\\', best_models[\\'logreg_countvect\\']),\\n                         (\\'sgd\\', best_models[\\'sgd_countvect\\']), (\\'svc\\', best_models[\\'svc_countvect\\']),\\n                         (\\'gbm\\', best_models[\\'gbm_countvect\\'])],voting=\\'hard\\')\\ncountvect_classifier.fit(X_train,y_train)\\ncountvect_classifier_predictions=countvect_classifier.predict(X_test)\\ncountvect_classifier_predictions_score=100*(f1_score(y_test,countvect_classifier_predictions,average=\\'weighted\\'))\\nprint(\"\\nAccuracy score of Voting Classifier algorithm -----> \" + str(countvect_classifier_predictions_score))'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from sklearn.ensemble import VotingClassifier\n",
    "countvect_classifier= VotingClassifier(estimators=[('mnb', best_models['mnb_countvect']), ('lr', best_models['logreg_countvect']),\n",
    "                         ('sgd', best_models['sgd_countvect']), ('svc', best_models['svc_countvect']),\n",
    "                         ('gbm', best_models['gbm_countvect'])],voting='hard')\n",
    "countvect_classifier.fit(X_train,y_train)\n",
    "countvect_classifier_predictions=countvect_classifier.predict(X_test)\n",
    "countvect_classifier_predictions_score=100*(f1_score(y_test,countvect_classifier_predictions,average='weighted'))\n",
    "print(\"\\nAccuracy score of Voting Classifier algorithm -----> \" + str(countvect_classifier_predictions_score))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'df_train = pd.concat([pd.DataFrame(train_pred1),pd.DataFrame(train_pred2),\\n                      pd.DataFrame(train_pred3),pd.DataFrame(train_pred4)], axis=1)\\ndf_test = pd.concat([pd.DataFrame(test_pred1),pd.DataFrame(test_pred2),pd.DataFrame(test_pred3)\\n                     ,pd.DataFrame(test_pred4)], axis=1)'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"df_train = pd.concat([pd.DataFrame(train_pred1),pd.DataFrame(train_pred2),\n",
    "                      pd.DataFrame(train_pred3),pd.DataFrame(train_pred4)], axis=1)\n",
    "df_test = pd.concat([pd.DataFrame(test_pred1),pd.DataFrame(test_pred2),pd.DataFrame(test_pred3)\n",
    "                     ,pd.DataFrame(test_pred4)], axis=1)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parameters_vect' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-100-5a96bf2a0965>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mXGBClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#xgb.fit(df_train,y_train)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mbest_xgb_countvect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_vect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxgb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters_xgb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparameters_vect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprediction_xgb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbest_xgb_countvect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtrain_pred3\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbest_xgb_countvect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'parameters_vect' is not defined"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb=XGBClassifier()\n",
    "#xgb.fit(df_train,y_train)\n",
    "best_xgb_countvect = grid_vect(xgb, parameters_xgb, X_train, X_test,model_features, parameters_text=parameters_vect)\n",
    "prediction_xgb = best_xgb_countvect.predict(X_test)\n",
    "train_pred3=best_xgb_countvect.predict(X_train)\n",
    "test_pred3=best_xgb_countvect.predict(X_test)\n",
    "xgb_score=f1_score(y_test,prediction_xgb,average='weighted')\n",
    "print(\"\\nAccuracy score of XGB algorithm -----> \" + str(xgb_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Last step of Pipeline should implement fit or be the string 'passthrough'. '<module 'xgboost' from 'C:\\\\Users\\\\gaurav.singh.rawal\\\\Anaconda3\\\\lib\\\\site-packages\\\\xgboost\\\\__init__.py'>' (type <class 'module'>) doesn't",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-99-e55aff3ae53d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# MultinomialNB\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgen_tfidf_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidfvect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mbest_xgb_tfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_vect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxgb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters_xgb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparameters_tfidf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprediction_xgb\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbest_xgb_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mxgb_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprediction_xgb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'weighted'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-94-d25c65e64e6b>\u001b[0m in \u001b[0;36mgrid_vect\u001b[1;34m(clf, parameters_clf, X_train, X_test, features, parameters_text)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgrid_vect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters_clf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[0mpipeline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'features'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'clf'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;31m# Join the parameters dictionaries together\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, steps, memory, verbose)\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_steps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_validate_steps\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    190\u001b[0m                 \u001b[1;34m\"Last step of Pipeline should implement fit \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m                 \u001b[1;34m\"or be the string 'passthrough'. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 192\u001b[1;33m                 \"'%s' (type %s) doesn't\" % (estimator, type(estimator)))\n\u001b[0m\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwith_final\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter_passthrough\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Last step of Pipeline should implement fit or be the string 'passthrough'. '<module 'xgboost' from 'C:\\\\Users\\\\gaurav.singh.rawal\\\\Anaconda3\\\\lib\\\\site-packages\\\\xgboost\\\\__init__.py'>' (type <class 'module'>) doesn't"
     ]
    }
   ],
   "source": [
    "# MultinomialNB\n",
    "model_features=gen_tfidf_features(tfidfvect)\n",
    "best_xgb_tfidf = grid_vect(xgb, parameters_xgb, X_train, X_test,model_features, parameters_text=parameters_tfidf)\n",
    "prediction_xgb=best_xgb_tfidf.predict(X_test)\n",
    "xgb_score=f1_score(y_test,prediction_xgb,average='weighted')\n",
    "#best_scores['mnb_countvect']=best_mnb_countvect.best_score_\n",
    "#best_models['mnb_countvect']=best_mnb_countvect\n",
    "#train_pred1=best_mnb_countvect.predict(X_train)\n",
    "#test_pred1=best_mnb_countvect.predict(X_test)\n",
    "print(\"\\nAccuracy score of Logreg tfidf algorithm -----> \" + str(xgb_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_mnb_model=mnb.set_params(**best_mnb_countvect.best_params_)\n",
    "best_xgb_countvect.fit(X,y)\n",
    "test_predictions=best_xgb_countvect.predict(test_df)\n",
    "#print(test_predictions.shape)\n",
    "prediction_df = pd.DataFrame(columns=['label'],data=test_predictions)\n",
    "prediction_df = pd.concat([test_data_id,prediction_df],axis=1)\n",
    "prediction_df.to_csv(r'C:\\Users\\gaurav.singh.rawal\\Documents\\Gaurav\\Data Science\\Machine Learning-Predictive Analytics\\ML-Projects\\GIT - ML - Projects\\ML_Projects\\NLP - Sentiment Analysis\\AnalyticsVidhya - Mobile Customer Feedback\\predictions_xgb.csv',index=False,encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions3=best_xgb_countvect.predict(test_df)\n",
    "prediction_df3 = pd.DataFrame(columns=['xgb'],data=test_predictions3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([pd.DataFrame(train_pred1),pd.DataFrame(train_pred2),pd.DataFrame(train_pred3),pd.DataFrame(train_pred4)], axis=1)\n",
    "df_test = pd.concat([pd.DataFrame(test_pred1),pd.DataFrame(test_pred2),pd.DataFrame(test_pred3),pd.DataFrame(test_pred4)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns=['mnb','lr','xgb','gbm']\n",
    "df_test.columns=['mnb','lr','xgb','gbm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy score of XGB stacking algorithm -----> 40.62500012229426\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "def hyperparameter_tuner(model,X_train,y_train,hp_list):\n",
    "    \n",
    "    hp_perf=[]\n",
    "    hp_model=RandomizedSearchCV(model,param_distributions=hp_list,n_iter=10,n_jobs=-1, scoring='f1_weighted',verbose=1, cv=5)\n",
    "    hp_model.fit(X_train,y_train)\n",
    "    best_hp_model=hp_model.best_estimator_\n",
    "    best_param=hp_model.best_params_\n",
    "    best_score=hp_model.best_score_  \n",
    "   \n",
    "    return best_hp_model,best_param,best_score\n",
    "\n",
    "\"\"\"xgb=XGBClassifier({\n",
    "    'objective': 'multi:softprob',\n",
    "    'eta': 0.1,\n",
    "    'max_depth': 25,\n",
    "    'silent' :1,\n",
    "    'num_class' : 3,\n",
    "    'eval_metric' : \"mlogloss\",\n",
    "    'min_child_weight': 1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.3,\n",
    "    'seed':17,\n",
    "    'num_rounds':2000,\n",
    "})\"\"\"\n",
    "xgb.fit(df_train,y_train)\n",
    "\n",
    "#best_model,best_params,best_score=hyperparameter_tuner(xgb,df_train,y_train,parameters_xgb)\n",
    "\n",
    "prediction_xgb = xgb.predict(df_test)\n",
    "xgb_score=100*(f1_score(y_test,prediction_xgb,average='weighted'))\n",
    "print(\"\\nAccuracy score of XGB stacking algorithm -----> \" + str(xgb_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy score of Random Forest Bagging algorithm -----> 0.8464142192184853\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "bagging_model = BaggingClassifier(RandomForestClassifier(max_depth=25,min_samples_leaf=2,n_estimators=100,class_weight='balanced'))\n",
    "bagging_model.fit(df_train,y_train)\n",
    "bagging_pred = bagging_model.predict(df_test)\n",
    "bagging_score=f1_score(y_test,bagging_pred,average='weighted')\n",
    "print(\"\\nAccuracy score of Random Forest Bagging algorithm -----> \" + str(bagging_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_mnb_model=mnb.set_params(**best_mnb_countvect.best_params_)\n",
    "#best_mnb_countvect.fit(X,y)\n",
    "#test_predictions=best_mnb_countvect.predict(test_df)\n",
    "#print(test_predictions.shape)\n",
    "final_test=pd.concat([prediction_df1,prediction_df2,prediction_df3],axis=1)\n",
    "final_pred=bagging_model.predict(final_test)\n",
    "prediction_df = pd.DataFrame(columns=['sentiment_class'],data=final_pred)\n",
    "prediction_df = pd.concat([test_df_id,prediction_df],axis=1)\n",
    "prediction_df.to_csv(r'C:\\Users\\gaurav.singh.rawal\\Documents\\Gaurav\\Data Science\\Machine Learning-Predictive Analytics\\ML-Projects\\GIT - ML - Projects\\ML_Projects\\NLP - Sentiment Analysis\\HackerRank - Mothers Day\\final_predictions.csv',index=False,encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_mnb_model=mnb.set_params(**best_mnb_countvect.best_params_)\n",
    "best_mnb_countvect.fit(X,y)\n",
    "test_predictions=best_mnb_countvect.predict(test_df)\n",
    "print(test_predictions.shape)\n",
    "prediction_df = pd.DataFrame(columns=['sentiment_class'],data=test_predictions)\n",
    "prediction_df = pd.concat([test_df_id,prediction_df],axis=1)\n",
    "prediction_df.to_csv(r'C:\\Users\\gaurav.singh.rawal\\Documents\\Gaurav\\Data Science\\Machine Learning-Predictive Analytics\\ML-Projects\\GIT - ML - Projects\\ML_Projects\\NLP - Sentiment Analysis\\HackerRank - Mothers Day\\predictions.csv',index=False,encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
