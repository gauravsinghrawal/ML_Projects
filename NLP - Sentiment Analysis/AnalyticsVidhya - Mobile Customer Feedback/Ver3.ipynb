{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gaurav.singh.rawal\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\gaurav.singh.rawal\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "from time import time\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import emoji\n",
    "import itertools\n",
    "import nltk\n",
    "import stats\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.set(font_scale=1.3)\n",
    "from autocorrect import Speller\n",
    "from textblob import TextBlob\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier,ExtraTreesClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split,RandomizedSearchCV,KFold,StratifiedKFold\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import classification_report,f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import xgboost as xgb\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.classify import SklearnClassifier\n",
    "\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from contractions import CONTRACTION_MAP\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(37)\n",
    "%matplotlib inline\n",
    "\n",
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv(r\"C:\\Users\\gaurav.singh.rawal\\Documents\\Gaurav\\Data Science\\Machine Learning-Predictive Analytics\\ML-Projects\\GIT - ML - Projects\\ML_Projects\\NLP - Sentiment Analysis\\AnalyticsVidhya - Mobile Customer Feedback\\train.csv\")\n",
    "test_data=pd.read_csv(r\"C:\\Users\\gaurav.singh.rawal\\Documents\\Gaurav\\Data Science\\Machine Learning-Predictive Analytics\\ML-Projects\\GIT - ML - Projects\\ML_Projects\\NLP - Sentiment Analysis\\AnalyticsVidhya - Mobile Customer Feedback\\test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>#fingerprint #Pregnancy Test https://goo.gl/h1MfQV #android #apps #beautiful #cute #health #igers #iphoneonly #iphonesia #iphone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Finally a transparant silicon case ^^ Thanks to my uncle :) #yay #Sony #Xperia #S #sonyexperias… http://instagram.com/p/YGEt5JC6JM/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>We love this! Would you go? #talk #makememories #unplug #relax #iphone #smartphone #wifi #connect... http://fb.me/6N3LsUpCu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>I'm wired I know I'm George I was made that way ;) #iphone #cute #daventry #home http://instagr.am/p/Li_5_ujS4k/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>What amazing service! Apple won't even talk to me about a question I have unless I pay them $19.95 for their stupid support!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label  \\\n",
       "0  1   0       \n",
       "1  2   0       \n",
       "2  3   0       \n",
       "3  4   0       \n",
       "4  5   1       \n",
       "\n",
       "                                                                                                                                 tweet  \n",
       "0  #fingerprint #Pregnancy Test https://goo.gl/h1MfQV #android #apps #beautiful #cute #health #igers #iphoneonly #iphonesia #iphone     \n",
       "1  Finally a transparant silicon case ^^ Thanks to my uncle :) #yay #Sony #Xperia #S #sonyexperias… http://instagram.com/p/YGEt5JC6JM/  \n",
       "2  We love this! Would you go? #talk #makememories #unplug #relax #iphone #smartphone #wifi #connect... http://fb.me/6N3LsUpCu          \n",
       "3  I'm wired I know I'm George I was made that way ;) #iphone #cute #daventry #home http://instagr.am/p/Li_5_ujS4k/                     \n",
       "4  What amazing service! Apple won't even talk to me about a question I have unless I pay them $19.95 for their stupid support!         "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1953, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_4290439c_b937_11ea_b320_d43b045a62ecrow0_col0 {\n",
       "            background-color:  #fcfbfd;\n",
       "            color:  #000000;\n",
       "        }    #T_4290439c_b937_11ea_b320_d43b045a62ecrow0_col1 {\n",
       "            background-color:  #3f007d;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_4290439c_b937_11ea_b320_d43b045a62ecrow1_col0 {\n",
       "            background-color:  #3f007d;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_4290439c_b937_11ea_b320_d43b045a62ecrow1_col1 {\n",
       "            background-color:  #fcfbfd;\n",
       "            color:  #000000;\n",
       "        }</style><table id=\"T_4290439c_b937_11ea_b320_d43b045a62ec\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >label</th>        <th class=\"col_heading level0 col1\" >tweet</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_4290439c_b937_11ea_b320_d43b045a62eclevel0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_4290439c_b937_11ea_b320_d43b045a62ecrow0_col0\" class=\"data row0 col0\" >0</td>\n",
       "                        <td id=\"T_4290439c_b937_11ea_b320_d43b045a62ecrow0_col1\" class=\"data row0 col1\" >5894</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_4290439c_b937_11ea_b320_d43b045a62eclevel0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_4290439c_b937_11ea_b320_d43b045a62ecrow1_col0\" class=\"data row1 col0\" >1</td>\n",
       "                        <td id=\"T_4290439c_b937_11ea_b320_d43b045a62ecrow1_col1\" class=\"data row1 col1\" >2026</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2d225cb1088>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = train_data.groupby('label').count()['tweet'].reset_index().sort_values(by='tweet',ascending=False)\n",
    "temp.style.background_gradient(cmap='Purples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2d225b8e048>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='label',data=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['tweet']=train_data['tweet'].apply(expand_contractions)\n",
    "test_data['tweet']=test_data['tweet'].apply(expand_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>#fingerprint #Pregnancy Test https://goo.gl/h1MfQV #android #apps #beautiful #cute #health #igers #iphoneonly #iphonesia #iphone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Finally a transparant silicon case ^^ Thanks to my uncle :) #yay #Sony #Xperia #S #sonyexperias… http://instagram.com/p/YGEt5JC6JM/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>We love this! Would you go? #talk #makememories #unplug #relax #iphone #smartphone #wifi #connect... http://fb.me/6N3LsUpCu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>I am wired I know I am George I was made that way ;) #iphone #cute #daventry #home http://instagr.am/p/Li_5_ujS4k/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>What amazing service! Apple will not even talk to me about a question I have unless I pay them $19.95 for their stupid support!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label  \\\n",
       "0  1   0       \n",
       "1  2   0       \n",
       "2  3   0       \n",
       "3  4   0       \n",
       "4  5   1       \n",
       "\n",
       "                                                                                                                                 tweet  \n",
       "0  #fingerprint #Pregnancy Test https://goo.gl/h1MfQV #android #apps #beautiful #cute #health #igers #iphoneonly #iphonesia #iphone     \n",
       "1  Finally a transparant silicon case ^^ Thanks to my uncle :) #yay #Sony #Xperia #S #sonyexperias… http://instagram.com/p/YGEt5JC6JM/  \n",
       "2  We love this! Would you go? #talk #makememories #unplug #relax #iphone #smartphone #wifi #connect... http://fb.me/6N3LsUpCu          \n",
       "3  I am wired I know I am George I was made that way ;) #iphone #cute #daventry #home http://instagr.am/p/Li_5_ujS4k/                   \n",
       "4  What amazing service! Apple will not even talk to me about a question I have unless I pay them $19.95 for their stupid support!      "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanText(BaseEstimator, TransformerMixin):\n",
    "    def remove_mentions(self, input_text):\n",
    "        return re.sub(r'@\\w+', '', input_text)\n",
    "    \n",
    "    def remove_urls(self, input_text):\n",
    "        return re.sub(r'http.?://[^\\s]+[\\s]?', '', input_text)\n",
    "    \n",
    "    def emoji_oneword(self, input_text):\n",
    "        # By compressing the underscore, the emoji is kept as one word\n",
    "        return input_text.replace('_','')\n",
    "    \n",
    "    def remove_punctuation(self, input_text):\n",
    "        # Make translation table\n",
    "        punct = string.punctuation\n",
    "        trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space\n",
    "        return input_text.translate(trantab)\n",
    "    def remove_digits(self, input_text):\n",
    "        return re.sub('\\d+', '', input_text)\n",
    "    \n",
    "    def split_words(self,input_text):\n",
    "        return ''.join(re.findall('[A-Z][^A-Z]*', input_text))\n",
    "    \n",
    "    def standardize_words(self,input_text):\n",
    "        return ''.join(''.join(s)[:2] for _, s in itertools.groupby(input_text))\n",
    "    \n",
    "    def to_lower(self, input_text):\n",
    "        return input_text.lower()\n",
    "    \n",
    "    '''def correct_spell(self,input_text):\n",
    "        words = input_text.split() \n",
    "        clean_words = [str(TextBlob(word).correct()) for word in words] \n",
    "        return \" \".join(clean_words)'''\n",
    "    \n",
    "    \"\"\"def correct_spell(self,input_text):\n",
    "        words = input_text.split() \n",
    "        spell = Speller(lang='en') \n",
    "        correct_words = [spell(word) for word in words] \n",
    "        return \" \".join(correct_words) \"\"\"\n",
    "    '''def remove_meaningless(self,input_text):\n",
    "        words = set(nltk.corpus.words.words())\n",
    "        return \" \".join(w for w in nltk.wordpunct_tokenize(input_text) if w.lower() in words or not w.isalpha())'''\n",
    "    \n",
    "    def remove_stopwords(self, input_text):\n",
    "        stopwords_list = stopwords.words('english')\n",
    "        # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "        whitelist = [\"n't\", \"not\", \"no\"]\n",
    "        words = input_text.split() \n",
    "        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
    "        return \" \".join(clean_words) \n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        clean_X = X.apply(self.remove_mentions).apply(self.remove_urls).apply(self.emoji_oneword).apply(self.remove_punctuation).apply(self.remove_digits).apply(self.split_words).apply(self.standardize_words).apply(self.to_lower).apply(self.remove_stopwords)\n",
    "        return clean_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = CleanText()\n",
    "train_data['tweet'] = ct.fit_transform(train_data.tweet)\n",
    "test_data['tweet']=ct.fit_transform(test_data.tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of words in the original_text ##\n",
    "train_data[\"num_words\"] = train_data[\"tweet\"].apply(lambda x: len(str(x).split()))\n",
    "test_data[\"num_words\"] = test_data[\"tweet\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "## Number of unique words in the original_text ##\n",
    "train_data[\"num_unique_words\"] = train_data[\"tweet\"].apply(lambda x: len(set(str(x).split())))\n",
    "test_data[\"num_unique_words\"] = test_data[\"tweet\"].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "## Number of characters in the original_text ##\n",
    "train_data[\"num_chars\"] = train_data[\"tweet\"].apply(lambda x: len(str(x)))\n",
    "test_data[\"num_chars\"] = test_data[\"tweet\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "## Number of stopwords in the original_text ##\n",
    "train_data[\"num_stopwords\"] = train_data[\"tweet\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "test_data[\"num_stopwords\"] = test_data[\"tweet\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "\n",
    "## Number of punctuations in the original_text ##\n",
    "train_data[\"num_punctuations\"] =train_data['tweet'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "test_data[\"num_punctuations\"] =test_data['tweet'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "\n",
    "## Number of title case words in the original_text ##\n",
    "train_data[\"num_words_upper\"] = train_data[\"tweet\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "test_data[\"num_words_upper\"] = test_data[\"tweet\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "\n",
    "## Number of title case words in the original_text ##\n",
    "train_data[\"num_words_title\"] = train_data[\"tweet\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "test_data[\"num_words_title\"] = test_data[\"tweet\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "\n",
    "## Average length of the words in the original_text ##\n",
    "train_data[\"mean_word_len\"] = train_data[\"tweet\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "test_data[\"mean_word_len\"] = test_data[\"tweet\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.WordPunctTokenizer()\n",
    "def pos_tagger(input_text):\n",
    "    text = tokenizer.tokenize(input_text)\n",
    "    tagged_text = nltk.pos_tag(text)\n",
    "    new_text = \"\"\n",
    "    for tag in tagged_text:\n",
    "        new_text = new_text + \" \" + tag[1]\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['pos_text'] = train_data['tweet'].apply(pos_tagger)\n",
    "test_data['pos_text'] = test_data['tweet'].apply(pos_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import  SnowballStemmer\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "def stemming(input_text):\n",
    "        tokenizer = ToktokTokenizer()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        tokens = tokenizer.tokenize(input_text)\n",
    "        tokens = [token.strip() for token in tokens]\n",
    "        stemmed_words = [stemmer.stem(token) for token in tokens]\n",
    "        return ' '.join(stemmed_words)\n",
    "    \n",
    "def lemmatize_text(input_text):\n",
    "        word_list = nltk.word_tokenize(input_text)\n",
    "        lemmatizer = WordNetLemmatizer() \n",
    "        lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "\n",
    "        return lemmatized_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['clean_text'] = train_data['tweet'].apply(stemming)\n",
    "test_data['clean_text'] = test_data['tweet'].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['clean_text'] = train_data['clean_text'].apply(lemmatize_text)\n",
    "test_data['clean_text'] = test_data['clean_text'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>mean_word_len</th>\n",
       "      <th>pos_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7921</td>\n",
       "      <td>hate new iphone upgrade not let download apps ugh apple sucks</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.636364</td>\n",
       "      <td>VB JJ NN NN RB VB NN IN JJ NN NNS</td>\n",
       "      <td>hate new iphon upgrad not let download app ugh appl suck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7922</td>\n",
       "      <td>mac cashmoney raddest swagswagswag</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.750000</td>\n",
       "      <td>NN NN NN NN</td>\n",
       "      <td>mac cashmoney raddest swagswagswag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7923</td>\n",
       "      <td>would like puts cd roms ipad possible yes would not block screen</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.416667</td>\n",
       "      <td>MD VB JJ NN NNS VBP JJ NNS MD RB VB NN</td>\n",
       "      <td>would like put cd rom ipad possibl yes would not block screen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7924</td>\n",
       "      <td>ipod officially dead lost pictures videos sos concert vet camp hatinglife sobbing</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.833333</td>\n",
       "      <td>JJ RB JJ VBN NNS VBP JJ NN NN NN NN VBG</td>\n",
       "      <td>ipod offici dead lost pictur video so concert vet camp hatinglif sob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7925</td>\n",
       "      <td>fighting itunes night want music paid</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>VBG NNS NN VBP NN NN</td>\n",
       "      <td>fight itun night want music paid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  \\\n",
       "0  7921   \n",
       "1  7922   \n",
       "2  7923   \n",
       "3  7924   \n",
       "4  7925   \n",
       "\n",
       "                                                                               tweet  \\\n",
       "0  hate new iphone upgrade not let download apps ugh apple sucks                       \n",
       "1  mac cashmoney raddest swagswagswag                                                  \n",
       "2  would like puts cd roms ipad possible yes would not block screen                    \n",
       "3  ipod officially dead lost pictures videos sos concert vet camp hatinglife sobbing   \n",
       "4  fighting itunes night want music paid                                               \n",
       "\n",
       "   num_words  num_unique_words  num_chars  num_stopwords  num_punctuations  \\\n",
       "0  11         11                61         1              0                  \n",
       "1  4          4                 34         0              0                  \n",
       "2  12         11                64         1              0                  \n",
       "3  12         12                81         0              0                  \n",
       "4  6          6                 37         0              0                  \n",
       "\n",
       "   num_words_upper  num_words_title  mean_word_len  \\\n",
       "0  0                0                4.636364        \n",
       "1  0                0                7.750000        \n",
       "2  0                0                4.416667        \n",
       "3  0                0                5.833333        \n",
       "4  0                0                5.333333        \n",
       "\n",
       "                                   pos_text  \\\n",
       "0   VB JJ NN NN RB VB NN IN JJ NN NNS         \n",
       "1   NN NN NN NN                               \n",
       "2   MD VB JJ NN NNS VBP JJ NNS MD RB VB NN    \n",
       "3   JJ RB JJ VBN NNS VBP JJ NN NN NN NN VBG   \n",
       "4   VBG NNS NN VBP NN NN                      \n",
       "\n",
       "                                                             clean_text  \n",
       "0  hate new iphon upgrad not let download app ugh appl suck              \n",
       "1  mac cashmoney raddest swagswagswag                                    \n",
       "2  would like put cd rom ipad possibl yes would not block screen         \n",
       "3  ipod offici dead lost pictur video so concert vet camp hatinglif sob  \n",
       "4  fight itun night want music paid                                      "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnExtractor(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "    def transform(self, X, **transform_params):\n",
    "        return X[self.cols]\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_data.label\n",
    "X = train_data.drop(['id','tweet'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=train_data.drop(['id','tweet'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_id=test_data.id\n",
    "test_df=test_data.drop(['id','tweet'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>mean_word_len</th>\n",
       "      <th>pos_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.636364</td>\n",
       "      <td>VB JJ NN NN RB VB NN IN JJ NN NNS</td>\n",
       "      <td>hate new iphon upgrad not let download app ugh appl suck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.750000</td>\n",
       "      <td>NN NN NN NN</td>\n",
       "      <td>mac cashmoney raddest swagswagswag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.416667</td>\n",
       "      <td>MD VB JJ NN NNS VBP JJ NNS MD RB VB NN</td>\n",
       "      <td>would like put cd rom ipad possibl yes would not block screen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>81</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.833333</td>\n",
       "      <td>JJ RB JJ VBN NNS VBP JJ NN NN NN NN VBG</td>\n",
       "      <td>ipod offici dead lost pictur video so concert vet camp hatinglif sob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>VBG NNS NN VBP NN NN</td>\n",
       "      <td>fight itun night want music paid</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   num_words  num_unique_words  num_chars  num_stopwords  num_punctuations  \\\n",
       "0  11         11                61         1              0                  \n",
       "1  4          4                 34         0              0                  \n",
       "2  12         11                64         1              0                  \n",
       "3  12         12                81         0              0                  \n",
       "4  6          6                 37         0              0                  \n",
       "\n",
       "   num_words_upper  num_words_title  mean_word_len  \\\n",
       "0  0                0                4.636364        \n",
       "1  0                0                7.750000        \n",
       "2  0                0                4.416667        \n",
       "3  0                0                5.833333        \n",
       "4  0                0                5.333333        \n",
       "\n",
       "                                   pos_text  \\\n",
       "0   VB JJ NN NN RB VB NN IN JJ NN NNS         \n",
       "1   NN NN NN NN                               \n",
       "2   MD VB JJ NN NNS VBP JJ NNS MD RB VB NN    \n",
       "3   JJ RB JJ VBN NNS VBP JJ NN NN NN NN VBG   \n",
       "4   VBG NNS NN VBP NN NN                      \n",
       "\n",
       "                                                             clean_text  \n",
       "0  hate new iphon upgrad not let download app ugh appl suck              \n",
       "1  mac cashmoney raddest swagswagswag                                    \n",
       "2  would like put cd rom ipad possibl yes would not block screen         \n",
       "3  ipod offici dead lost pictur video so concert vet camp hatinglif sob  \n",
       "4  fight itun night want music paid                                      "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runMNB(train_X, train_y, test_X, test_y, test_X2):\n",
    "    model = MultinomialNB()\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_test_y = model.predict_proba(test_X)\n",
    "    pred_test_y2 = model.predict_proba(test_X2)\n",
    "    return pred_test_y, pred_test_y2, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fit transform the tfidf vectorizer ###\n",
    "tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\n",
    "full_tfidf = tfidf_vec.fit_transform(train_df['clean_text'].values.tolist() + test_df['clean_text'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(train_df['clean_text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(test_df['clean_text'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comp = 20\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_tfidf)\n",
    "train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n",
    "test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n",
    "    \n",
    "train_svd.columns = ['svd_char_'+str(i) for i in range(n_comp)]\n",
    "test_svd.columns = ['svd_char_'+str(i) for i in range(n_comp)]\n",
    "train_df = pd.concat([train_df, train_svd], axis=1)\n",
    "test_df = pd.concat([test_df, test_svd], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label', 'num_words', 'num_unique_words', 'num_chars', 'num_stopwords',\n",
       "       'num_punctuations', 'num_words_upper', 'num_words_title',\n",
       "       'mean_word_len', 'pos_text', 'clean_text', 'svd_char_0', 'svd_char_1',\n",
       "       'svd_char_2', 'svd_char_3', 'svd_char_4', 'svd_char_5', 'svd_char_6',\n",
       "       'svd_char_7', 'svd_char_8', 'svd_char_9', 'svd_char_10', 'svd_char_11',\n",
       "       'svd_char_12', 'svd_char_13', 'svd_char_14', 'svd_char_15',\n",
       "       'svd_char_16', 'svd_char_17', 'svd_char_18', 'svd_char_19'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fit transform the count vectorizer ###\n",
    "cv_vec = CountVectorizer(stop_words='english', ngram_range=(1,2))\n",
    "cv_vec.fit(train_df['clean_text'].values.tolist() + test_df['clean_text'].values.tolist())\n",
    "train_cv = cv_vec.transform(train_df['clean_text'].values.tolist())\n",
    "test_cv = cv_vec.transform(test_df['clean_text'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train_df.shape[0], 2])\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for dev_index, val_index in kf.split(X_train):\n",
    "    dev_X, val_X = train_cv[dev_index], train_cv[val_index]\n",
    "    dev_y, val_y = y_train.iloc[dev_index], y_train.iloc[val_index]\n",
    "    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_cv)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the predictions as new features #\n",
    "train_df[\"nb_cv_sent_pos\"] = pred_train[:,0]\n",
    "train_df[\"nb_cv_sent_neg\"] = pred_train[:,1]\n",
    "\n",
    "test_df[\"nb_cv_sent_pos\"] = pred_full_test[:,0]\n",
    "test_df[\"nb_cv_sent_neg\"] = pred_full_test[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fit transform the tfidf vectorizer ###\n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(1,5), analyzer='char')\n",
    "tfidf_vec.fit(train_df['clean_text'].values.tolist() + test_df['clean_text'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(train_df['clean_text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(test_df['clean_text'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train_df.shape[0], 2])\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "for dev_index, val_index in kf.split(X_train):\n",
    "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "    dev_y, val_y = y_train.iloc[dev_index], y_train.iloc[val_index]\n",
    "    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the predictions as new features #\n",
    "train_df[\"nb_tfidf_char_pos\"] = pred_train[:,0]\n",
    "train_df[\"nb_tfidf_char_neg\"] = pred_train[:,1]\n",
    "\n",
    "test_df[\"nb_tfidf_char_pos\"] = pred_full_test[:,0]\n",
    "test_df[\"nb_tfidf_char_neg\"] = pred_full_test[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fit transform the tfidf vectorizer ###\n",
    "tfidf_vec = CountVectorizer(ngram_range=(1,5), analyzer='char')\n",
    "tfidf_vec.fit(train_df['clean_text'].values.tolist() + test_df['clean_text'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(train_df['clean_text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(test_df['clean_text'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train_df.shape[0], 2])\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "for dev_index, val_index in kf.split(X_train):\n",
    "    dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "    dev_y, val_y = y_train.iloc[dev_index], y_train.iloc[val_index]\n",
    "    pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the predictions as new features #\n",
    "train_df[\"nb_cvec_char_pos\"] = pred_train[:,0]\n",
    "train_df[\"nb_cvec_char_neg\"] = pred_train[:,1]\n",
    "\n",
    "test_df[\"nb_cvec_char_pos\"] = pred_full_test[:,0]\n",
    "test_df[\"nb_cvec_char_neg\"] = pred_full_test[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = MinMaxScaler()\n",
    "train_df[['nb_cv_sent_pos','nb_cv_sent_neg',\"nb_tfidf_char_pos\", \"nb_tfidf_char_neg\",\"nb_cvec_char_pos\",\n",
    "          \"nb_cvec_char_neg\",'svd_char_0', 'svd_char_1', 'svd_char_2', 'svd_char_3', 'svd_char_4','svd_char_5', \n",
    "        'svd_char_6', 'svd_char_7', 'svd_char_8', 'svd_char_9','svd_char_10', 'svd_char_11', 'svd_char_12', 'svd_char_13','svd_char_14', 'svd_char_15', \n",
    "                      'svd_char_16', 'svd_char_17','svd_char_18', 'svd_char_19']]=train_df[['nb_cv_sent_pos','nb_cv_sent_neg',\"nb_tfidf_char_pos\", \"nb_tfidf_char_neg\",\"nb_cvec_char_pos\",\n",
    "          \"nb_cvec_char_neg\",'svd_char_0', 'svd_char_1', 'svd_char_2', 'svd_char_3', 'svd_char_4','svd_char_5', \n",
    "                      'svd_char_6', 'svd_char_7', 'svd_char_8', 'svd_char_9','svd_char_10', 'svd_char_11', 'svd_char_12', 'svd_char_13','svd_char_14', 'svd_char_15', \n",
    "                      'svd_char_16', 'svd_char_17','svd_char_18', 'svd_char_19']].values.astype(float)\n",
    "train_df[['nb_cv_sent_pos','nb_cv_sent_neg',\"nb_tfidf_char_pos\", \"nb_tfidf_char_neg\",\"nb_cvec_char_pos\",\n",
    "          \"nb_cvec_char_neg\",'svd_char_0', 'svd_char_1', 'svd_char_2', 'svd_char_3', 'svd_char_4','svd_char_5', \n",
    "            'svd_char_6', 'svd_char_7', 'svd_char_8', 'svd_char_9','svd_char_10', 'svd_char_11', 'svd_char_12', 'svd_char_13','svd_char_14', 'svd_char_15', \n",
    "                      'svd_char_16', 'svd_char_17','svd_char_18', 'svd_char_19']] = min_max_scaler.fit_transform(train_df[['nb_cv_sent_pos','nb_cv_sent_neg',\"nb_tfidf_char_pos\", \"nb_tfidf_char_neg\",\"nb_cvec_char_pos\",\n",
    "          \"nb_cvec_char_neg\",'svd_char_0', 'svd_char_1', 'svd_char_2', 'svd_char_3', 'svd_char_4','svd_char_5', \n",
    "                      'svd_char_6', 'svd_char_7', 'svd_char_8', 'svd_char_9',\n",
    "                    'svd_char_10', 'svd_char_11', 'svd_char_12', 'svd_char_13','svd_char_14', 'svd_char_15', \n",
    "                      'svd_char_16', 'svd_char_17','svd_char_18', 'svd_char_19']])\n",
    "test_df[['nb_cv_sent_pos','nb_cv_sent_neg',\"nb_tfidf_char_pos\", \"nb_tfidf_char_neg\",\"nb_cvec_char_pos\",\n",
    "          \"nb_cvec_char_neg\",'svd_char_0', 'svd_char_1', 'svd_char_2', 'svd_char_3', 'svd_char_4','svd_char_5', \n",
    "                      'svd_char_6', 'svd_char_7', 'svd_char_8', 'svd_char_9',\n",
    "                    'svd_char_10', 'svd_char_11', 'svd_char_12', 'svd_char_13','svd_char_14', 'svd_char_15', \n",
    "                      'svd_char_16', 'svd_char_17','svd_char_18', 'svd_char_19']]=test_df[['nb_cv_sent_pos','nb_cv_sent_neg',\"nb_tfidf_char_pos\", \"nb_tfidf_char_neg\",\"nb_cvec_char_pos\",\n",
    "          \"nb_cvec_char_neg\",'svd_char_0', 'svd_char_1', 'svd_char_2', 'svd_char_3', 'svd_char_4','svd_char_5', \n",
    "                      'svd_char_6', 'svd_char_7', 'svd_char_8', 'svd_char_9','svd_char_10', 'svd_char_11', 'svd_char_12', 'svd_char_13','svd_char_14', 'svd_char_15', \n",
    "                      'svd_char_16', 'svd_char_17','svd_char_18', 'svd_char_19']].values.astype(float)\n",
    "test_df[['nb_cv_sent_pos','nb_cv_sent_neg',\"nb_tfidf_char_pos\", \"nb_tfidf_char_neg\",\"nb_cvec_char_pos\",\n",
    "          \"nb_cvec_char_neg\",'svd_char_0', 'svd_char_1', 'svd_char_2', 'svd_char_3', 'svd_char_4','svd_char_5', \n",
    "        'svd_char_6', 'svd_char_7', 'svd_char_8', 'svd_char_9','svd_char_10', 'svd_char_11', 'svd_char_12', 'svd_char_13','svd_char_14', 'svd_char_15', \n",
    "                      'svd_char_16', 'svd_char_17','svd_char_18', 'svd_char_19']] = min_max_scaler.fit_transform(test_df[['nb_cv_sent_pos','nb_cv_sent_neg',\"nb_tfidf_char_pos\", \"nb_tfidf_char_neg\",\"nb_cvec_char_pos\",\n",
    "          \"nb_cvec_char_neg\",'svd_char_0', 'svd_char_1', 'svd_char_2', 'svd_char_3', 'svd_char_4','svd_char_5', \n",
    "                      'svd_char_6', 'svd_char_7', 'svd_char_8', 'svd_char_9',\n",
    "                    'svd_char_10', 'svd_char_11', 'svd_char_12', 'svd_char_13','svd_char_14', 'svd_char_15', \n",
    "                      'svd_char_16', 'svd_char_17','svd_char_18', 'svd_char_19']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['mean_word_len']=train_df['mean_word_len'].fillna(train_df['mean_word_len'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_df.label\n",
    "X = train_df.drop(['label'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train,df_test = train_test_split(train_df, test_size=0.2, random_state=37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df=test_df.fillna(test_df.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7920, 36)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1953, 36)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "textcountscols = ['num_words','num_unique_words','num_chars','num_stopwords','num_punctuations','num_words_upper','num_words_title','mean_word_len',\n",
    "                      'nb_cv_sent_pos','nb_cv_sent_neg',\"nb_tfidf_char_pos\",\"nb_tfidf_char_neg\",\"nb_cvec_char_pos\",\"nb_cvec_char_neg\",\n",
    "                     'svd_char_0', 'svd_char_1', 'svd_char_2', 'svd_char_3', 'svd_char_4','svd_char_5', \n",
    "                      'svd_char_6', 'svd_char_7', 'svd_char_8', 'svd_char_9',\n",
    "                    'svd_char_10', 'svd_char_11', 'svd_char_12', 'svd_char_13','svd_char_14', 'svd_char_15', \n",
    "                      'svd_char_16', 'svd_char_17','svd_char_18', 'svd_char_19']\n",
    "    \n",
    "# Based on http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html\n",
    "def gen_cv_features(cvect=None, is_w2v=None):\n",
    "   \n",
    "    if is_w2v:\n",
    "        w2vcols = []\n",
    "        for i in range(SIZE):\n",
    "            w2vcols.append(i)\n",
    "        features = FeatureUnion([('textcounts', ColumnExtractor(cols=textcountscols))\n",
    "                                 , ('w2v', ColumnExtractor(cols=w2vcols))], n_jobs=-1)\n",
    "    else:\n",
    "        features = FeatureUnion([('textcounts', ColumnExtractor(cols=textcountscols))\n",
    "                                 ,('pipe1', Pipeline([('cleantext', ColumnExtractor(cols='clean_text'))         \n",
    "                                                     ,('vect', cvect)])),\n",
    "                                ('pipe2', Pipeline([('postext', ColumnExtractor(cols='pos_text'))         \n",
    "                                                     ,('vect', cvect)]))]\n",
    "                                , n_jobs=-1)\n",
    "    return features\n",
    "\n",
    "def gen_tfidf_features(tfidf=None, is_w2v=None):\n",
    "   \n",
    "    if is_w2v:\n",
    "        w2vcols = []\n",
    "        for i in range(SIZE):\n",
    "            w2vcols.append(i)\n",
    "        features = FeatureUnion([('textcounts', ColumnExtractor(cols=textcountscols))\n",
    "                                 , ('w2v', ColumnExtractor(cols=w2vcols))], n_jobs=-1)\n",
    "    else:\n",
    "        features = FeatureUnion([('textcounts', ColumnExtractor(cols=textcountscols))\n",
    "                                 ,('pipe1', Pipeline([('cleantext', ColumnExtractor(cols='clean_text'))         \n",
    "                                                     ,('vect', tfidf)])),\n",
    "                                ('pipe2', Pipeline([('postext', ColumnExtractor(cols='pos_text'))         \n",
    "                                                     ,('vect', tfidf)]))]\n",
    "                                , n_jobs=-1)\n",
    "    return features\n",
    "\n",
    "def grid_vect(clf, parameters_clf, X_train, X_test, features, parameters_text=None):\n",
    "    pipeline = Pipeline([('features', features),('clf', clf)])\n",
    "    \n",
    "    # Join the parameters dictionaries together\n",
    "    parameters = dict()\n",
    "    if parameters_text:\n",
    "        parameters.update(parameters_text)\n",
    "    parameters.update(parameters_clf)\n",
    "\n",
    "    grid_search_model = RandomizedSearchCV(pipeline, parameters, n_jobs=-1, scoring='f1_weighted',verbose=1, cv=5,random_state=42)\n",
    "    \n",
    "    print(\"Performing grid search...\")\n",
    "    grid_search_model.fit(X_train, y_train)\n",
    "    print(\"Best CV score: %0.3f\" % grid_search_model.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search_model.best_params_\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "        \n",
    "    print(\"Test score with best_estimator_: %0.3f\" % grid_search_model.best_estimator_.score(X_test, y_test))\n",
    "                 \n",
    "    return best_parameters,grid_search_model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid settings for the vectorizers (Count and TFIDF)\n",
    "parameters_cvect = {\n",
    "    'features__pipe1__vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'features__pipe1__vect__ngram_range': ((1, 1), (1, 2),(1,3)),\n",
    "    'features__pipe1__vect__min_df': (1,2,3,4,5),\n",
    "    'features__pipe2__vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'features__pipe2__vect__ngram_range': ((1, 1), (1, 2),(1,3),(2,2)),\n",
    "    'features__pipe2__vect__min_df': (1,2,3,4,5)\n",
    "}\n",
    "parameters_tfidf = {\n",
    "    'features__pipe1__vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'features__pipe1__vect__ngram_range': ((1, 1), (1, 2),(1,3)),\n",
    "    'features__pipe1__vect__min_df': (1,2,3,4,5),\n",
    "    'features__pipe1__vect__use_idf': (0,1),\n",
    "    'features__pipe1__vect__smooth_idf': (0,1),\n",
    "    'features__pipe1__vect__sublinear_tf': (0,1),\n",
    "    'features__pipe2__vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'features__pipe2__vect__ngram_range': ((1, 1), (1, 2),(1,3),(2,2)),\n",
    "    'features__pipe2__vect__min_df': (1,2,3,4,5),\n",
    "    'features__pipe2__vect__use_idf': (0,1),\n",
    "    'features__pipe2__vect__smooth_idf': (0,1),\n",
    "    'features__pipe2__vect__sublinear_tf': (0,1)\n",
    "}\n",
    "# Parameter grid settings for MultinomialNB\n",
    "parameters_mnb = {\n",
    "    'clf__alpha': (0.01,0.1, 0.25, 0.5, 0.75),\n",
    "    'clf__fit_prior' : (True,False)\n",
    "}\n",
    "parameters_etrees = {'clf__n_estimators': [25,50,100,500],\n",
    "         'clf__criterion':['gini','entropy'],\n",
    "         'clf__class_weight':[None,'balanced'],\n",
    "         'clf__max_depth': [5,10,20],\n",
    "         'clf__max_features': ['auto', 'sqrt'],\n",
    "         'clf__min_samples_leaf': [2,3,4,5],}\n",
    "\n",
    "# Parameter grid settings for LogisticRegression\n",
    "parameters_rforest = {'clf__bootstrap': [True, False],\n",
    "         'clf__criterion':['gini','entropy'],\n",
    "         'clf__max_depth': [5,10,20],\n",
    "         'clf__max_features': ['auto', 'sqrt'],\n",
    "         'clf__min_samples_leaf': [2,3,4,5],\n",
    "         'clf__n_estimators': [50,100,500],\n",
    "         'clf__class_weight':['balanced']}\n",
    "parameters_sgd = {\n",
    "    'clf__alpha': (0.00001, 0.000001),\n",
    "    'clf__penalty': ('l1', 'l2','elasticnet'),\n",
    "    'clf__max_iter': (10, 25, 50, 75, 100),\n",
    "    'clf__class_weight':['balanced',None]\n",
    "}\n",
    "parameters_ada = {'clf__n_estimators':[50,100,250,500], \n",
    "                  'clf__learning_rate':[0.01,0.1,1],\n",
    "                  'clf__algorithm':'SAMME.R',\n",
    "                  'clf__algorithm':'SAMME'\n",
    "                 }\n",
    "parameters_svc = {'clf__kernel': ['linear', 'rbf'],\n",
    "                    'clf__gamma':[0.1,1,10],\n",
    "                    'clf__C':[0.1,1,0.001,0.0001],\n",
    "                  'clf__class_weight':['balanced',None]\n",
    "                 }\n",
    "parameters_linsvc = {'clf__loss' : ['hinge', 'squared_hinge'],\n",
    "                    'clf__penalty': ('l1', 'l2'),\n",
    "                    'clf__class_weight':['balanced',None],\n",
    "                    'clf__max_iter':[100,500],\n",
    "                    'clf__C':[0.1,1,0.001,0.0001]               \n",
    "                 }\n",
    "\n",
    "parameters_gbm = {'clf__loss':['deviance','exponential'],\n",
    "         'clf__learning_rate': [0.05,0.1],\n",
    "         'clf__max_depth':[5,10,20],\n",
    "         'clf__n_estimators': [10,25,50]\n",
    "        }\n",
    "parameters_xgb = {'clf__learning_rate':[0.01,0.1],\n",
    "         'clf__n_estimators':[50,100,500],\n",
    "         'clf__max_depth':[5,10,15,20],\n",
    "        'clf__class_weight':['balanced',None]\n",
    "        }\n",
    "parameters_logreg = {\n",
    "    'clf__C': (0.25, 0.5, 1.0),\n",
    "    'clf__penalty': ('l1', 'l2'),\n",
    "    'clf__fit_intercept': (True,False),\n",
    "    'clf__class_weight':['balanced',None]\n",
    "}\n",
    "parameters_lgb = {\n",
    "    'clf__learning_rate': [0.01,0.05,0.1],\n",
    "    'clf__max_depth':[5,10,15,20],\n",
    "    'clf__n_estimators': [25,50,100,250,500],\n",
    "    'clf__num_leaves':[5,10,15,20,25],\n",
    "    'clf__objective':('multiclass','binary')\n",
    "}\n",
    "\n",
    "parameters_nn = {\n",
    "    'clf__activation': ('logistic','tanh','relu'),\n",
    "    'clf__alpha': (0.0001,0.001,0.01),\n",
    "    'clf__solver': ('lbfgs','sgd'),\n",
    "    'clf__hidden_layer_sizes':(50,100),\n",
    "    'clf__max_iter':(50,100)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()\n",
    "logreg=LogisticRegression()\n",
    "etrees=ExtraTreesClassifier()\n",
    "rforest=RandomForestClassifier()\n",
    "linsvc = LinearSVC()\n",
    "gbm=GradientBoostingClassifier()\n",
    "ada=AdaBoostClassifier()\n",
    "lgb = LGBMClassifier()\n",
    "xgb_model=xgb.XGBClassifier(objective=\"binary:logistic\")\n",
    "countvect = CountVectorizer()\n",
    "tfidfvect = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_param(params):\n",
    "    new_params={}\n",
    "    for param in params:\n",
    "        if re.search('clf', param):\n",
    "            param_value=params[param]\n",
    "            new_param=param.replace('clf__', '')\n",
    "            new_params[new_param]=param_value\n",
    "    return new_params    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_filename = r'C:\\Users\\gaurav.singh.rawal\\Documents\\Gaurav\\Data Science\\Machine Learning-Predictive Analytics\\ML-Projects\\GIT - ML - Projects\\ML_Projects\\NLP - Sentiment Analysis\\AnalyticsVidhya - Mobile Customer Feedback\\mnb_model.sav'\n",
    "rforest_filename = r'C:\\Users\\gaurav.singh.rawal\\Documents\\Gaurav\\Data Science\\Machine Learning-Predictive Analytics\\ML-Projects\\GIT - ML - Projects\\ML_Projects\\NLP - Sentiment Analysis\\AnalyticsVidhya - Mobile Customer Feedback\\rforest_model.sav'\n",
    "gbm_filename = r'C:\\Users\\gaurav.singh.rawal\\Documents\\Gaurav\\Data Science\\Machine Learning-Predictive Analytics\\ML-Projects\\GIT - ML - Projects\\ML_Projects\\NLP - Sentiment Analysis\\AnalyticsVidhya - Mobile Customer Feedback\\gbm_model.sav'\n",
    "etrees_filename = r'C:\\Users\\gaurav.singh.rawal\\Documents\\Gaurav\\Data Science\\Machine Learning-Predictive Analytics\\ML-Projects\\GIT - ML - Projects\\ML_Projects\\NLP - Sentiment Analysis\\AnalyticsVidhya - Mobile Customer Feedback\\etrees_model.sav'\n",
    "linsvc_filename = r'C:\\Users\\gaurav.singh.rawal\\Documents\\Gaurav\\Data Science\\Machine Learning-Predictive Analytics\\ML-Projects\\GIT - ML - Projects\\ML_Projects\\NLP - Sentiment Analysis\\AnalyticsVidhya - Mobile Customer Feedback\\linsvc_model.sav'\n",
    "logreg_filename = r'C:\\Users\\gaurav.singh.rawal\\Documents\\Gaurav\\Data Science\\Machine Learning-Predictive Analytics\\ML-Projects\\GIT - ML - Projects\\ML_Projects\\NLP - Sentiment Analysis\\AnalyticsVidhya - Mobile Customer Feedback\\logreg_model.sav'\n",
    "lgb_filename = r'C:\\Users\\gaurav.singh.rawal\\Documents\\Gaurav\\Data Science\\Machine Learning-Predictive Analytics\\ML-Projects\\GIT - ML - Projects\\ML_Projects\\NLP - Sentiment Analysis\\AnalyticsVidhya - Mobile Customer Feedback\\lgb_model.sav'\n",
    "nn_filename = r'C:\\Users\\gaurav.singh.rawal\\Documents\\Gaurav\\Data Science\\Machine Learning-Predictive Analytics\\ML-Projects\\GIT - ML - Projects\\ML_Projects\\NLP - Sentiment Analysis\\AnalyticsVidhya - Mobile Customer Feedback\\nn_model.sav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_features=gen_cv_features(countvect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  35 tasks      | elapsed:   13.3s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   15.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.853\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.25\n",
      "\tclf__fit_prior: True\n",
      "\tfeatures__pipe1__vect__max_df: 0.75\n",
      "\tfeatures__pipe1__vect__min_df: 2\n",
      "\tfeatures__pipe1__vect__ngram_range: (1, 2)\n",
      "\tfeatures__pipe2__vect__max_df: 0.75\n",
      "\tfeatures__pipe2__vect__min_df: 2\n",
      "\tfeatures__pipe2__vect__ngram_range: (1, 1)\n",
      "Test score with best_estimator_: 0.846\n"
     ]
    }
   ],
   "source": [
    "# MultinomialNB\n",
    "mnb_features={}\n",
    "mnb_cvect_params,mnb_model = grid_vect(mnb, parameters_mnb, X_train, X_test,model_features, parameters_text=parameters_cvect)\n",
    "mnb_params=extract_param(mnb_cvect_params)\n",
    "mnb_model=mnb.set_params(**mnb_params)\n",
    "pickle.dump(mnb_model, open(mnb_filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   14.2s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   27.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.825\n",
      "Best parameters set:\n",
      "\tclf__bootstrap: True\n",
      "\tclf__class_weight: 'balanced'\n",
      "\tclf__criterion: 'gini'\n",
      "\tclf__max_depth: 20\n",
      "\tclf__max_features: 'auto'\n",
      "\tclf__min_samples_leaf: 2\n",
      "\tclf__n_estimators: 500\n",
      "\tfeatures__pipe1__vect__max_df: 1.0\n",
      "\tfeatures__pipe1__vect__min_df: 3\n",
      "\tfeatures__pipe1__vect__ngram_range: (1, 3)\n",
      "\tfeatures__pipe2__vect__max_df: 1.0\n",
      "\tfeatures__pipe2__vect__min_df: 5\n",
      "\tfeatures__pipe2__vect__ngram_range: (1, 2)\n",
      "Test score with best_estimator_: 0.820\n",
      "{'n_estimators': 500, 'min_samples_leaf': 2, 'max_features': 'auto', 'max_depth': 20, 'criterion': 'gini', 'class_weight': 'balanced', 'bootstrap': True}\n"
     ]
    }
   ],
   "source": [
    "# MultinomialNB\n",
    "rforest_cvect_params,rforest_model = grid_vect(rforest, parameters_rforest, X_train, X_test,model_features, parameters_text=parameters_cvect)\n",
    "rforest_params=extract_param(rforest_cvect_params)\n",
    "rforest_model=rforest.set_params(**rforest_params)\n",
    "pickle.dump(rforest_model, open(rforest_filename, 'wb'))\n",
    "print(rforest_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   59.3s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.842\n",
      "Best parameters set:\n",
      "\tclf__learning_rate: 0.1\n",
      "\tclf__loss: 'exponential'\n",
      "\tclf__max_depth: 10\n",
      "\tclf__n_estimators: 50\n",
      "\tfeatures__pipe1__vect__max_df: 0.75\n",
      "\tfeatures__pipe1__vect__min_df: 2\n",
      "\tfeatures__pipe1__vect__ngram_range: (1, 1)\n",
      "\tfeatures__pipe2__vect__max_df: 0.5\n",
      "\tfeatures__pipe2__vect__min_df: 5\n",
      "\tfeatures__pipe2__vect__ngram_range: (1, 3)\n",
      "Test score with best_estimator_: 0.853\n",
      "{'n_estimators': 50, 'max_depth': 10, 'loss': 'exponential', 'learning_rate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# GBM\n",
    "gbm_cvect_params,gbm_model = grid_vect(gbm, parameters_gbm, X_train, X_test,model_features, parameters_text=parameters_cvect)\n",
    "gbm_params=extract_param(gbm_cvect_params)\n",
    "gbm_model=gbm.set_params(**gbm_params)\n",
    "pickle.dump(gbm_model, open(gbm_filename, 'wb'))\n",
    "print(gbm_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   13.3s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   17.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.793\n",
      "Best parameters set:\n",
      "\tclf__class_weight: 'balanced'\n",
      "\tclf__criterion: 'gini'\n",
      "\tclf__max_depth: 20\n",
      "\tclf__max_features: 'auto'\n",
      "\tclf__min_samples_leaf: 5\n",
      "\tclf__n_estimators: 500\n",
      "\tfeatures__pipe1__vect__max_df: 0.5\n",
      "\tfeatures__pipe1__vect__min_df: 3\n",
      "\tfeatures__pipe1__vect__ngram_range: (1, 3)\n",
      "\tfeatures__pipe2__vect__max_df: 0.5\n",
      "\tfeatures__pipe2__vect__min_df: 1\n",
      "\tfeatures__pipe2__vect__ngram_range: (1, 1)\n",
      "Test score with best_estimator_: 0.790\n",
      "{'n_estimators': 500, 'min_samples_leaf': 5, 'max_features': 'auto', 'max_depth': 20, 'criterion': 'gini', 'class_weight': 'balanced'}\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost\n",
    "etrees_cvect_params,etrees_model = grid_vect(etrees, parameters_etrees, X_train, X_test,model_features, parameters_text=parameters_cvect)\n",
    "etrees_params=extract_param(etrees_cvect_params,model)\n",
    "etrees_model=etrees.set_params(**etrees_params)\n",
    "pickle.dump(etrees_model, open(etrees_filename, 'wb'))\n",
    "print(etrees_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    4.9s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    6.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.807\n",
      "Best parameters set:\n",
      "\tclf__C: 0.001\n",
      "\tclf__class_weight: None\n",
      "\tclf__loss: 'squared_hinge'\n",
      "\tclf__max_iter: 100\n",
      "\tclf__penalty: 'l2'\n",
      "\tfeatures__pipe1__vect__max_df: 0.5\n",
      "\tfeatures__pipe1__vect__min_df: 3\n",
      "\tfeatures__pipe1__vect__ngram_range: (1, 2)\n",
      "\tfeatures__pipe2__vect__max_df: 0.75\n",
      "\tfeatures__pipe2__vect__min_df: 5\n",
      "\tfeatures__pipe2__vect__ngram_range: (1, 3)\n",
      "Test score with best_estimator_: 0.824\n",
      "{'penalty': 'l2', 'max_iter': 100, 'loss': 'squared_hinge', 'class_weight': None, 'C': 0.001}\n"
     ]
    }
   ],
   "source": [
    "# SVC\n",
    "linsvc_cvect_params,linsvc_model = grid_vect(linsvc, parameters_linsvc, X_train, X_test,model_features, parameters_text=parameters_cvect)\n",
    "linsvc_params=extract_param(linsvc_cvect_params,model)\n",
    "linsvc_model=linsvc.set_params(**linsvc_params)\n",
    "pickle.dump(linsvc_model, open(linsvc_filename, 'wb'))\n",
    "print(linsvc_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   21.3s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   25.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.849\n",
      "Best parameters set:\n",
      "\tclf__learning_rate: 0.1\n",
      "\tclf__max_depth: 15\n",
      "\tclf__n_estimators: 100\n",
      "\tclf__num_leaves: 10\n",
      "\tclf__objective: 'binary'\n",
      "\tfeatures__pipe1__vect__max_df: 1.0\n",
      "\tfeatures__pipe1__vect__min_df: 3\n",
      "\tfeatures__pipe1__vect__ngram_range: (1, 3)\n",
      "\tfeatures__pipe2__vect__max_df: 0.75\n",
      "\tfeatures__pipe2__vect__min_df: 4\n",
      "\tfeatures__pipe2__vect__ngram_range: (1, 3)\n",
      "Test score with best_estimator_: 0.855\n",
      "{'objective': 'binary', 'num_leaves': 10, 'n_estimators': 100, 'max_depth': 15, 'learning_rate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "lgb_cvect_params,lgb_model = grid_vect(lgb, parameters_lgb, X_train, X_test,model_features, parameters_text=parameters_cvect)\n",
    "lgb_params=extract_param(lgb_cvect_params)\n",
    "lgb_model=lgb.set_params(**lgb_params)\n",
    "pickle.dump(lgb_model, open(lgb_filename, 'wb'))\n",
    "print(lgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    7.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.855\n",
      "Best parameters set:\n",
      "\tclf__C: 0.25\n",
      "\tclf__class_weight: None\n",
      "\tclf__fit_intercept: True\n",
      "\tclf__penalty: 'l2'\n",
      "\tfeatures__pipe1__vect__max_df: 1.0\n",
      "\tfeatures__pipe1__vect__min_df: 3\n",
      "\tfeatures__pipe1__vect__ngram_range: (1, 3)\n",
      "\tfeatures__pipe2__vect__max_df: 0.5\n",
      "\tfeatures__pipe2__vect__min_df: 4\n",
      "\tfeatures__pipe2__vect__ngram_range: (2, 2)\n",
      "Test score with best_estimator_: 0.867\n",
      "{'penalty': 'l2', 'fit_intercept': True, 'class_weight': None, 'C': 0.25}\n"
     ]
    }
   ],
   "source": [
    "# Logreg\n",
    "logreg_cvect_params,logreg_model = grid_vect(logreg, parameters_logreg, X_train, X_test,model_features, parameters_text=parameters_cvect)\n",
    "logreg_params=extract_param(logreg_cvect_params,model)\n",
    "logreg_model=logreg.set_params(**logreg_params)\n",
    "pickle.dump(logreg_model, open(logreg_filename, 'wb'))\n",
    "print(logreg_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed: 14.2min\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed: 17.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.828\n",
      "Best parameters set:\n",
      "\tclf__activation: 'relu'\n",
      "\tclf__alpha: 0.0001\n",
      "\tclf__hidden_layer_sizes: 100\n",
      "\tclf__max_iter: 100\n",
      "\tclf__solver: 'lbfgs'\n",
      "\tfeatures__pipe1__vect__max_df: 0.75\n",
      "\tfeatures__pipe1__vect__min_df: 1\n",
      "\tfeatures__pipe1__vect__ngram_range: (1, 3)\n",
      "\tfeatures__pipe2__vect__max_df: 1.0\n",
      "\tfeatures__pipe2__vect__min_df: 2\n",
      "\tfeatures__pipe2__vect__ngram_range: (2, 2)\n",
      "Test score with best_estimator_: 0.824\n",
      "{'solver': 'lbfgs', 'max_iter': 100, 'hidden_layer_sizes': 100, 'alpha': 0.0001, 'activation': 'relu'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "nn = MLPClassifier()\n",
    "nn_cvect_params,nn_model = grid_vect(nn, parameters_nn, X_train, X_test,model_features, parameters_text=parameters_cvect)\n",
    "nn_params=extract_param(nn_cvect_params,model)\n",
    "nn_model = nn.set_params(**nn_params)\n",
    "pickle.dump(nn_model, open(nn_filename, 'wb'))\n",
    "print(nn_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models():\n",
    "    \"\"\"Generate a library of base learners.\"\"\"\n",
    "    base_learners=[pickle.load(open(mnb_filename, 'rb')),\n",
    "                   #pickle.load(open(gbm_filename, 'rb')),\n",
    "                   pickle.load(open(logreg_filename, 'rb')),\n",
    "                   pickle.load(open(nn_filename, 'rb')),\n",
    "                   pickle.load(open(rforest_filename, 'rb')),\n",
    "                   pickle.load(open(lgb_filename, 'rb'))\n",
    "                   ]\n",
    "    \n",
    "    models = {'mnb': pickle.load(open(mnb_filename, 'rb')),\n",
    "              'gbm': pickle.load(open(gbm_filename, 'rb')),\n",
    "              'logreg': pickle.load(open(logreg_filename, 'rb')),\n",
    "              'neural': pickle.load(open(nn_filename, 'rb')),\n",
    "              'lgb': pickle.load(open(lgb_filename, 'rb')),\n",
    "              'rforest': pickle.load(open(rforest_filename, 'rb')),\n",
    "              }\n",
    "    \n",
    "    return base_learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[MultinomialNB(alpha=0.25, class_prior=None, fit_prior=True),\n",
       " LogisticRegression(C=0.25, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "               beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "               hidden_layer_sizes=100, learning_rate='constant',\n",
       "               learning_rate_init=0.001, max_fun=15000, max_iter=100,\n",
       "               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "               power_t=0.5, random_state=None, shuffle=True, solver='lbfgs',\n",
       "               tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "               warm_start=False),\n",
       " RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight='balanced',\n",
       "                        criterion='gini', max_depth=20, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=2, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=500,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       " LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "                importance_type='split', learning_rate=0.1, max_depth=15,\n",
       "                min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "                n_estimators=100, n_jobs=-1, num_leaves=10, objective='binary',\n",
       "                random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "                subsample=1.0, subsample_for_bin=200000, subsample_freq=0)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict(model_list):\n",
    "    \"\"\"Fit models in list on training set and return preds\"\"\"\n",
    "    P = np.zeros((y_test.shape[0], len(model_list)))\n",
    "    P = pd.DataFrame(P)\n",
    "\n",
    "    print(\"Fitting models.\")\n",
    "    cols = list()\n",
    "    features = FeatureUnion([('textcounts', ColumnExtractor(cols=textcountscols))\n",
    "                                 ,('pipe1', Pipeline([('cleantext', ColumnExtractor(cols='clean_text'))         \n",
    "                                                     ,('vect', countvect)])),\n",
    "                                ('pipe2', Pipeline([('postext', ColumnExtractor(cols='pos_text'))         \n",
    "                                                     ,('vect', countvect)]))]\n",
    "                                , n_jobs=-1)\n",
    "    for i, (name, model) in enumerate(models.items()):\n",
    "        pipeline = Pipeline([('features', features),('clf', model)])\n",
    "        print(\"%s...\" % name, end=\" \", flush=False)\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        P.iloc[:, i] = pipeline.predict(X_test)\n",
    "        cols.append(name)\n",
    "        print(\"done\")\n",
    "\n",
    "    P.columns = cols\n",
    "    print(\"Done.\\n\")\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_models(P, y):\n",
    "    \"\"\"Score model in prediction DF\"\"\"\n",
    "    print(\"Scoring models.\")\n",
    "    for m in P.columns:\n",
    "        score = f1_score(y, P.loc[:, m],average='weighted')\n",
    "        print(\"%-26s: %.3f\" % (m, score))\n",
    "    print(\"Done.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting models.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-200-f8652bdf688a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_models\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mP\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mscore_models\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mP\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-198-150e17dacd24>\u001b[0m in \u001b[0;36mtrain_predict\u001b[1;34m(model_list)\u001b[0m\n\u001b[0;32m     12\u001b[0m                                                      ,('vect', countvect)]))]\n\u001b[0;32m     13\u001b[0m                                 , n_jobs=-1)\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mpipeline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'features'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'clf'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s...\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "models = get_models()\n",
    "P = train_predict(models)\n",
    "score_models(P, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['target_label']=train_df['label']\n",
    "train_df.drop(['label'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1953, 36)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<9873x1959 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 132627 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_cv1=CountVectorizer(max_df=0.5,min_df=2,ngram_range=(1, 2))\n",
    "pos_cv1=CountVectorizer(max_df=0.5,min_df=2,ngram_range=(1, 3))\n",
    "text_cv1.fit_transform(train_df['clean_text'].values.tolist()+test_df['clean_text'].values.tolist())\n",
    "pos_cv1.fit_transform(train_df['pos_text'].values.tolist()+test_df['pos_text'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_cv=text_cv1.transform(train_df['clean_text'].values.tolist())\n",
    "train_pos_cv=pos_cv1.transform(train_df['pos_text'].values.tolist())\n",
    "train_text_cv=pd.DataFrame(train_text_cv.toarray(), columns=text_cv1.get_feature_names())\n",
    "train_pos_cv=pd.DataFrame(train_pos_cv.toarray(), columns=pos_cv1.get_feature_names())\n",
    "train_df=pd.concat([train_df,train_text_cv,train_pos_cv],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7920, 16604)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train,df_test = train_test_split(train_df, test_size=0.2, random_state=37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=df_train['target_label']\n",
    "y_test=df_test['target_label']\n",
    "X_train=df_train.drop(['target_label','clean_text','pos_text'],axis=1)\n",
    "X_test=df_test.drop(['target_label','clean_text','pos_text'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=train_df.target_label\n",
    "X=train_df.drop(['target_label','clean_text','pos_text'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7920, 16601)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<9873x1959 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 132627 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_cv2=CountVectorizer(max_df=0.5,min_df=2,ngram_range=(1, 2))\n",
    "pos_cv2=CountVectorizer(max_df=0.5,min_df=2,ngram_range=(1, 3))\n",
    "text_cv2.fit_transform(train_df['clean_text'].values.tolist()+test_df['clean_text'].values.tolist())\n",
    "pos_cv2.fit_transform(train_df['pos_text'].values.tolist()+test_df['pos_text'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_cv=text_cv2.transform(test_df['clean_text'].values.tolist())\n",
    "test_pos_cv=pos_cv2.transform(test_df['pos_text'].values.tolist())\n",
    "test_text_cv=pd.DataFrame(test_text_cv.toarray(), columns=text_cv2.get_feature_names())\n",
    "test_pos_cv=pd.DataFrame(test_pos_cv.toarray(), columns=pos_cv2.get_feature_names())\n",
    "test_df=pd.concat([test_df,test_text_cv,test_pos_cv],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_df=test_df.drop(['clean_text','pos_text'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1953, 16601)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:         [classification]\n",
      "n_classes:    [2]\n",
      "metric:       [f1_score]\n",
      "mode:         [oof_pred]\n",
      "n_models:     [5]\n",
      "\n",
      "model  0:     [MultinomialNB]\n",
      "    fold  0:  [0.75326216]\n",
      "    fold  1:  [0.71770894]\n",
      "    fold  2:  [0.73578199]\n",
      "    ----\n",
      "    MEAN:     [0.73558436] + [0.01451521]\n",
      "    FULL:     [0.73549656]\n",
      "\n",
      "    Fitting on full train set...\n",
      "\n",
      "model  1:     [LogisticRegression]\n",
      "    fold  0:  [0.71622702]\n",
      "    fold  1:  [0.68870968]\n",
      "    fold  2:  [0.67325103]\n",
      "    ----\n",
      "    MEAN:     [0.69272924] + [0.01777361]\n",
      "    FULL:     [0.69293038]\n",
      "\n",
      "    Fitting on full train set...\n",
      "\n",
      "model  2:     [MLPClassifier]\n",
      "    fold  0:  [0.67692308]\n",
      "    fold  1:  [0.63628967]\n",
      "    fold  2:  [0.66615027]\n",
      "    ----\n",
      "    MEAN:     [0.65978767] + [0.01718780]\n",
      "    FULL:     [0.65992011]\n",
      "\n",
      "    Fitting on full train set...\n",
      "\n",
      "model  3:     [RandomForestClassifier]\n",
      "    fold  0:  [0.70260448]\n",
      "    fold  1:  [0.68697730]\n",
      "    fold  2:  [0.70377020]\n",
      "    ----\n",
      "    MEAN:     [0.69778399] + [0.00765629]\n",
      "    FULL:     [0.69775821]\n",
      "\n",
      "    Fitting on full train set...\n",
      "\n",
      "model  4:     [LGBMClassifier]\n",
      "    fold  0:  [0.71226415]\n",
      "    fold  1:  [0.67775139]\n",
      "    fold  2:  [0.68999186]\n",
      "    ----\n",
      "    MEAN:     [0.69333580] + [0.01428680]\n",
      "    FULL:     [0.69341126]\n",
      "\n",
      "    Fitting on full train set...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from vecstack import stacking\n",
    "from catboost import CatBoostClassifier\n",
    "#meta_learner = CatBoostClassifier(n_estimators=50)\n",
    "\n",
    "level1_models=get_models()\n",
    "\n",
    "S_train, S_test = stacking(level1_models,X,y,final_test_df,regression=False,needs_proba=False,\n",
    "                           mode='oof_pred',save_dir=None, metric=f1_score, n_folds=3, stratified=True, shuffle=True,random_state=10,verbose=2)\n",
    "meta_learner = xgb.XGBClassifier(random_state=0, n_jobs=-1, learning_rate=0.1, n_estimators=100, max_depth=5)\n",
    "\n",
    "meta_model = meta_learner.fit(S_train, y)\n",
    "#y_pred = model.predict(S_test)\n",
    "y_pred = meta_model.predict(S_test)\n",
    "#stacked_score=f1_score(y_test, y_pred,average='weighted')\n",
    "#print('Final prediction score: [%.8f]' %stacked_score )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"C:\\Users\\gaurav.singh.rawal\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"C:\\Users\\gaurav.singh.rawal\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"C:\\Users\\gaurav.singh.rawal\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"C:\\Users\\gaurav.singh.rawal\\Anaconda3\\lib\\imp.py\", line 242, in load_module\n    return load_dynamic(name, filename, file)\n  File \"C:\\Users\\gaurav.singh.rawal\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\n    return _load(spec)\nImportError: DLL load failed: The specified module could not be found.\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_mod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m                 \u001b[0m_mod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_pywrap_tensorflow_internal'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\imp.py\u001b[0m in \u001b[0;36mload_module\u001b[1;34m(name, file, filename, details)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mload_dynamic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mtype_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mPKG_DIRECTORY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\imp.py\u001b[0m in \u001b[0;36mload_dynamic\u001b[1;34m(name, path, file)\u001b[0m\n\u001b[0;32m    341\u001b[0m             name=name, loader=loader, origin=path)\n\u001b[1;32m--> 342\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed: The specified module could not be found.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-f5402dc0e2f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# NN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscikit_learn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKerasClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mbuild_keras_model_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlosses_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mset_epsilon\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfloatx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mset_floatx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mload_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcast_to_floatx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\load_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tensorflow'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;31m# Try and load external backend.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtfdev\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;31m# Protocol buffers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[1;33m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[1;32m---> 69\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"C:\\Users\\gaurav.singh.rawal\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"C:\\Users\\gaurav.singh.rawal\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"C:\\Users\\gaurav.singh.rawal\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"C:\\Users\\gaurav.singh.rawal\\Anaconda3\\lib\\imp.py\", line 242, in load_module\n    return load_dynamic(name, filename, file)\n  File \"C:\\Users\\gaurav.singh.rawal\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\n    return _load(spec)\nImportError: DLL load failed: The specified module could not be found.\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."
     ]
    }
   ],
   "source": [
    "# NN\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "def build_keras_model_2():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, \n",
    "                    input_dim=X_train.shape[1], \n",
    "                    kernel_initializer='normal', \n",
    "                    activation='relu'))\n",
    "    model.add(Dense(64, \n",
    "                    kernel_initializer='normal', \n",
    "                    activation='relu'))\n",
    "    model.add(Dense(n_classes, \n",
    "                    kernel_initializer='normal', \n",
    "                    activation='softmax'))\n",
    "    model.compile(optimizer='rmsprop', \n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['categorical_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_mnb_model=mnb.set_params(**best_mnb_countvect.best_params_)\n",
    "#nn_pipeline = Pipeline([('features', features),('clf', nn_model)])\n",
    "#meta_model.fit(X,y)\n",
    "#meta_predictions=meta_model.predict(test_df)\n",
    "prediction_df = pd.DataFrame(columns=['label'],data=y_pred)\n",
    "prediction_df = pd.concat([test_data_id,prediction_df],axis=1)\n",
    "prediction_df.to_csv(r'C:\\Users\\gaurav.singh.rawal\\Documents\\Gaurav\\Data Science\\Machine Learning-Predictive Analytics\\ML-Projects\\GIT - ML - Projects\\ML_Projects\\NLP - Sentiment Analysis\\AnalyticsVidhya - Mobile Customer Feedback\\predictions_final.csv',index=False,encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"C:\\Users\\gaurav.singh.rawal\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"C:\\Users\\gaurav.singh.rawal\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"C:\\Users\\gaurav.singh.rawal\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"C:\\Users\\gaurav.singh.rawal\\Anaconda3\\lib\\imp.py\", line 242, in load_module\n    return load_dynamic(name, filename, file)\n  File \"C:\\Users\\gaurav.singh.rawal\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\n    return _load(spec)\nImportError: DLL load failed: The specified module could not be found.\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m   \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0m_mod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m                 \u001b[0m_mod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'_pywrap_tensorflow_internal'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\imp.py\u001b[0m in \u001b[0;36mload_module\u001b[1;34m(name, file, filename, details)\u001b[0m\n\u001b[0;32m    241\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mload_dynamic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mtype_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mPKG_DIRECTORY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\imp.py\u001b[0m in \u001b[0;36mload_dynamic\u001b[1;34m(name, path, file)\u001b[0m\n\u001b[0;32m    341\u001b[0m             name=name, loader=loader, origin=path)\n\u001b[1;32m--> 342\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed: The specified module could not be found.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4ae80da9abf6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;31m# Protocol buffers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[1;33m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[1;32m---> 69\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"C:\\Users\\gaurav.singh.rawal\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"C:\\Users\\gaurav.singh.rawal\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"C:\\Users\\gaurav.singh.rawal\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"C:\\Users\\gaurav.singh.rawal\\Anaconda3\\lib\\imp.py\", line 242, in load_module\n    return load_dynamic(name, filename, file)\n  File \"C:\\Users\\gaurav.singh.rawal\\Anaconda3\\lib\\imp.py\", line 342, in load_dynamic\n    return _load(spec)\nImportError: DLL load failed: The specified module could not be found.\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_mnb_model=mnb.set_params(**best_mnb_countvect.best_params_)\n",
    "#nn_pipeline = Pipeline([('features', features),('clf', nn_model)])\n",
    "meta_model.fit(X,y)\n",
    "meta_predictions=meta_model.predict(test_df)\n",
    "prediction_df = pd.DataFrame(columns=['label'],data=meta_predictions)\n",
    "prediction_df = pd.concat([test_data_id,prediction_df],axis=1)\n",
    "prediction_df.to_csv(r'C:\\Users\\gaurav.singh.rawal\\Documents\\Gaurav\\Data Science\\Machine Learning-Predictive Analytics\\ML-Projects\\GIT - ML - Projects\\ML_Projects\\NLP - Sentiment Analysis\\AnalyticsVidhya - Mobile Customer Feedback\\predictions_final.csv',index=False,encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "ename": "LightGBMError",
     "evalue": "Do not support non-ASCII characters in feature name.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLightGBMError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-182-9fb535299353>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m                             random_state=42)\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mstack_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m#X_test.columns = ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12']\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\mlxtend\\classifier\\stacking_cv_classification.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, sample_weight)\u001b[0m\n\u001b[0;32m    240\u001b[0m                     \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m                     method='predict_proba' if self.use_probas else 'predict')\n\u001b[0m\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_probas\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_predict\u001b[1;34m(estimator, X, y, groups, cv, n_jobs, verbose, fit_params, pre_dispatch, method)\u001b[0m\n\u001b[0;32m    753\u001b[0m     prediction_blocks = parallel(delayed(_fit_and_predict)(\n\u001b[0;32m    754\u001b[0m         clone(estimator), X, y, train, test, verbose, fit_params, method)\n\u001b[1;32m--> 755\u001b[1;33m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[0;32m    756\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m     \u001b[1;31m# Concatenate the predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1002\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1003\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    833\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    834\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 835\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    836\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    752\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    755\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 256\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 256\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_predict\u001b[1;34m(estimator, X, y, train, test, verbose, fit_params, method)\u001b[0m\n\u001b[0;32m    839\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 841\u001b[1;33m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    842\u001b[0m     \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[0;32m    803\u001b[0m                                         \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    804\u001b[0m                                         \u001b[0mcategorical_feature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 805\u001b[1;33m                                         callbacks=callbacks)\n\u001b[0m\u001b[0;32m    806\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    807\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, early_stopping_rounds, verbose, feature_name, categorical_feature, callbacks)\u001b[0m\n\u001b[0;32m    598\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m                               \u001b[0mcategorical_feature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcategorical_feature\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m                               callbacks=callbacks)\n\u001b[0m\u001b[0;32m    601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    226\u001b[0m     \u001b[1;31m# construct booster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m         \u001b[0mbooster\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBooster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_valid_contain_train\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m             \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_train_data_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, params, train_set, model_file, model_str, silent)\u001b[0m\n\u001b[0;32m   1712\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1713\u001b[0m             _safe_call(_LIB.LGBM_BoosterCreate(\n\u001b[1;32m-> 1714\u001b[1;33m                 \u001b[0mtrain_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstruct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1715\u001b[0m                 \u001b[0mc_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1716\u001b[0m                 ctypes.byref(self.handle)))\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mconstruct\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1083\u001b[0m                                 \u001b[0minit_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predictor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m                                 \u001b[0msilent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msilent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1085\u001b[1;33m                                 categorical_feature=self.categorical_feature, params=self.params)\n\u001b[0m\u001b[0;32m   1086\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfree_raw_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1087\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m(self, data, label, reference, weight, group, init_score, predictor, silent, feature_name, categorical_feature, params)\u001b[0m\n\u001b[0;32m    913\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Wrong predictor type {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m         \u001b[1;31m# set feature names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_feature_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init_from_np2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mset_feature_name\u001b[1;34m(self, feature_name)\u001b[0m\n\u001b[0;32m   1366\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m                 \u001b[0mc_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_feature_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1368\u001b[1;33m                 ctypes.c_int(len(feature_name))))\n\u001b[0m\u001b[0;32m   1369\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m_safe_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \"\"\"\n\u001b[0;32m     44\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mLightGBMError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecode_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLGBM_GetLastError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLightGBMError\u001b[0m: Do not support non-ASCII characters in feature name."
     ]
    }
   ],
   "source": [
    "from mlxtend.classifier import StackingCVClassifier\n",
    "\n",
    "xgb_model=xgb.XGBClassifier(random_state=0, n_jobs=-1, learning_rate=0.1, n_estimators=200, max_depth=4)\n",
    "stack_model = StackingCVClassifier(classifiers=level1_models,\n",
    "                            meta_classifier=xgb_model, cv=5,\n",
    "                            use_features_in_secondary=True,\n",
    "                            store_train_meta_features=True,\n",
    "                            shuffle=False,\n",
    "                            random_state=42)\n",
    "\n",
    "stack_model.fit(X_train, y_train)\n",
    "\n",
    "#X_test.columns = ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12']\n",
    "stack_pred = stack.predict(X_test)\n",
    "stacked_score = f1_score(y_test, stack_pred, average='weighted')\n",
    "print('Final prediction score: [%.8f]' %stacked_score )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "feature_names must be unique",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-181-50c197f2304d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#best_mnb_model=mnb.set_params(**best_mnb_countvect.best_params_)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#nn_pipeline = Pipeline([('features', features),('clf', nn_model)])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmeta_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmeta_predictions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmeta_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprediction_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmeta_predictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[0;32m    814\u001b[0m         train_dmatrix = DMatrix(X, label=training_labels, weight=sample_weight,\n\u001b[0;32m    815\u001b[0m                                 \u001b[0mbase_margin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbase_margin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 816\u001b[1;33m                                 missing=self.missing, nthread=self.n_jobs)\n\u001b[0m\u001b[0;32m    817\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         self._Booster = train(xgb_options, train_dmatrix,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread)\u001b[0m\n\u001b[0;32m    556\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_base_margin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_margin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 558\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    559\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_types\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature_types\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mfeature_names\u001b[1;34m(self, feature_names)\u001b[0m\n\u001b[0;32m   1001\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1002\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1003\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'feature_names must be unique'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1004\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_col\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1005\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'feature_names must have the same length as data'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: feature_names must be unique"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_base_learners(base_learners, inp, out, verbose=True):\n",
    "    \"\"\"\n",
    "    Train all base learners in the library.\n",
    "    \"\"\"\n",
    "    if verbose: print(\"Fitting models.\")\n",
    "    for i, (name, m) in enumerate(base_learners.items()):\n",
    "        if verbose: print(\"%s...\" % name, end=\" \", flush=False)\n",
    "        m.fit(inp, out)\n",
    "        if verbose: print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_base_learners(pred_base_learners, inp, verbose=True):\n",
    "    \"\"\"\n",
    "    Generate a prediction matrix.\n",
    "    \"\"\"\n",
    "    P = np.zeros((inp.shape[0], len(pred_base_learners)))\n",
    "\n",
    "    if verbose: print(\"Generating base learner predictions.\")\n",
    "    for i, (name, m) in enumerate(pred_base_learners.items()):\n",
    "        if verbose: print(\"%s...\" % name, end=\" \", flush=False)\n",
    "        p = m.predict(inp)\n",
    "        # With two classes, need only predictions for one class\n",
    "        P[:, i] = p\n",
    "        if verbose: print(\"done\")\n",
    "\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(base_learners, meta_learner, inp, verbose=True):\n",
    "    \"\"\"\n",
    "    Generate predictions from the ensemble.\n",
    "    \"\"\"\n",
    "    P_pred = predict_base_learners(base_learners, inp, verbose=verbose)\n",
    "    return P_pred, meta_learner.predict(P_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "def stacking(base_learners, meta_learner, X, y, generator):\n",
    "    \"\"\"Simple training routine for stacking.\"\"\"\n",
    "\n",
    "    # Train final base learners for test time\n",
    "    print(\"Fitting final base learners...\", end=\"\")\n",
    "    train_base_learners(base_learners, X, y, verbose=False)\n",
    "    print(\"done\")\n",
    "\n",
    "    # Generate predictions for training meta learners\n",
    "    # Outer loop:\n",
    "    print(\"Generating cross-validated predictions...\")\n",
    "    cv_preds, cv_y = [], []\n",
    "    for i, (train_idx, test_idx) in enumerate(generator.split(X)):\n",
    "\n",
    "        fold_xtrain, fold_ytrain = X[train_idx, :], y[train_idx]\n",
    "        fold_xtest, fold_ytest = X[test_idx, :], y[test_idx]\n",
    "\n",
    "        # Inner loop: step 4 and 5\n",
    "        fold_base_learners = {name: clone(model)\n",
    "                              for name, model in base_learners.items()}\n",
    "        train_base_learners(fold_base_learners, fold_xtrain, fold_ytrain, verbose=False)\n",
    "\n",
    "        fold_P_base = predict_base_learners(fold_base_learners, fold_xtest, verbose=False)\n",
    "\n",
    "        cv_preds.append(fold_P_base)\n",
    "        cv_y.append(fold_ytest)\n",
    "        print(\"Fold %i done\" % (i + 1))\n",
    "\n",
    "    print(\"CV-predictions done\")\n",
    "\n",
    "    # Be careful to get rows in the right order\n",
    "    cv_preds = np.vstack(cv_preds)\n",
    "    cv_y = np.hstack(cv_y)\n",
    "\n",
    "    # Train meta learner\n",
    "    print(\"Fitting meta learner...\", end=\"\")\n",
    "    meta_learner.fit(cv_preds, cv_y)\n",
    "    print(\"done\")\n",
    "\n",
    "    return base_learners, meta_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting final base learners...done\n",
      "Generating cross-validated predictions...\n",
      "Fold 1 done\n",
      "Fold 2 done\n",
      "Fold 3 done\n",
      "Fold 4 done\n",
      "Fold 5 done\n",
      "CV-predictions done\n",
      "Fitting meta learner..."
     ]
    },
    {
     "ename": "CatBoostError",
     "evalue": "(The system cannot find the path specified.) c:/program files (x86)/go agent/pipelines/buildmaster/catboost.git/util/system/file.cpp:799: can't open \"catboost_info\\\\tmp\\\\cat_feature_index.107f32f5-6527a0d7-354e0c7c-38492f94.tmp\" with mode WrOnly|CreateAlways|Seq (0x00000034)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCatBoostError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-729c5808061a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Train with stacking\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m cv_base_learners, cv_meta_learner = stacking(\n\u001b[1;32m----> 7\u001b[1;33m     models, clone(meta_learner), X_train.values, y_train.values, KFold(5))\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mP_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensemble_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_base_learners\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv_meta_learner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-60-aaa6962573ac>\u001b[0m in \u001b[0;36mstacking\u001b[1;34m(base_learners, meta_learner, X, y, generator)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;31m# Train meta learner\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Fitting meta learner...\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mmeta_learner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv_preds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"done\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, cat_features, text_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model)\u001b[0m\n\u001b[0;32m   4113\u001b[0m         self._fit(X, y, cat_features, text_features, None, sample_weight, None, None, None, None, baseline, use_best_model,\n\u001b[0;32m   4114\u001b[0m                   \u001b[0meval_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogging_level\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn_description\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric_period\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4115\u001b[1;33m                   silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model)\n\u001b[0m\u001b[0;32m   4116\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, cat_features, text_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model)\u001b[0m\n\u001b[0;32m   1741\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1742\u001b[0m                 \u001b[0mallow_clear_pool\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1743\u001b[1;33m                 \u001b[0mtrain_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"init_model\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1744\u001b[0m             )\n\u001b[0;32m   1745\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\catboost\\core.py\u001b[0m in \u001b[0;36m_train\u001b[1;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[0;32m   1228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1229\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1230\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_object\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_object\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0minit_model\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1231\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_trained_model_attributes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mCatBoostError\u001b[0m: (The system cannot find the path specified.) c:/program files (x86)/go agent/pipelines/buildmaster/catboost.git/util/system/file.cpp:799: can't open \"catboost_info\\\\tmp\\\\cat_feature_index.107f32f5-6527a0d7-354e0c7c-38492f94.tmp\" with mode WrOnly|CreateAlways|Seq (0x00000034)"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "models = get_models()\n",
    "# Train with stacking\n",
    "cv_base_learners, cv_meta_learner = stacking(\n",
    "    models, clone(meta_learner), X_train.values, y_train.values, KFold(5))\n",
    "\n",
    "P_pred, p = ensemble_predict(cv_base_learners, cv_meta_learner, X_test, verbose=False)\n",
    "print(\"\\nEnsemble ROC-AUC score: %.3f\" % f1_score(y_test, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.classifier import StackingClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import model_selection\n",
    "xgb=XGBClassifier({'objective': 'multi:softprob','eta': 0.1,'max_depth': 25,'silent' :1,'num_class' : 3,'eval_metric' : \"mlogloss\",\n",
    "    'min_child_weight': 1,'subsample': 0.8, 'colsample_bytree': 0.3, 'seed':17, 'num_rounds':2000,\n",
    "})\n",
    "\n",
    "features = FeatureUnion([('textcounts', ColumnExtractor(cols=textcountscols))\n",
    "                                 ,('pipe1', Pipeline([('cleantext', ColumnExtractor(cols='clean_text'))         \n",
    "                                                     ,('vect', countvect)])),\n",
    "                                ('pipe2', Pipeline([('postext', ColumnExtractor(cols='pos_text'))         \n",
    "                                                     ,('vect', countvect)]))]\n",
    "                                , n_jobs=-1)\n",
    "sclf = StackingClassifier(classifiers=[mnb_model, rforest_model, logreg_model, linsvs_model],\n",
    "                          use_probas=True,\n",
    "                          average_probas=False,\n",
    "                          meta_classifier=xgb)\n",
    "\n",
    "print('5-fold cross validation:\\n')\n",
    "\n",
    "for clf, label in zip([mnb_model, rforest_model, logreg_model, linsvs_model, sclf], \n",
    "                      ['MNB', \n",
    "                       'Random Forest', \n",
    "                       'Log Reg',\n",
    "                       'Lin SVC',\n",
    "                      'StackingClassifier']):\n",
    "    pipeline = Pipeline([('features', features),('clf', clf)])\n",
    "\n",
    "    scores = model_selection.cross_val_score(pipeline, X, y, \n",
    "                                              cv=5, scoring='f1_weighted')\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" \n",
    "          % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print(sklearn.metrics.SCORERS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegression\n",
    "best_logreg_countvect = grid_vect(logreg, parameters_logreg, X_train, X_test,model_features, parameters_text=parameters_vect)\n",
    "prediction_logreg = best_logreg_countvect.predict(X_test)\n",
    "logreg_score=f1_score(y_test,prediction_logreg,average='weighted')\n",
    "#best_scores['logreg_countvect']=best_logreg_countvect.best_score_\n",
    "#best_models['logreg_countvect']=best_logreg_countvect\n",
    "train_pred2=best_logreg_countvect.predict(X_train)\n",
    "test_pred2=best_logreg_countvect.predict(X_test)\n",
    "print(\"\\nAccuracy score of Log Reg algorithm -----> \" + str(logreg_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultinomialNB\n",
    "model_features=gen_tfidf_features(tfidfvect)\n",
    "best_logreg_tfidf = grid_vect(logreg, parameters_logreg, X_train, X_test,model_features, parameters_text=parameters_tfidf)\n",
    "prediction_logreg=best_logreg_tfidf.predict(X_test)\n",
    "logreg_score=f1_score(y_test,prediction_logreg,average='weighted')\n",
    "#best_scores['mnb_countvect']=best_mnb_countvect.best_score_\n",
    "#best_models['mnb_countvect']=best_mnb_countvect\n",
    "#train_pred1=best_mnb_countvect.predict(X_train)\n",
    "#test_pred1=best_mnb_countvect.predict(X_test)\n",
    "print(\"\\nAccuracy score of Logreg tfidf algorithm -----> \" + str(logreg_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions2=best_logreg_countvect.predict(test_df)\n",
    "prediction_df2 = pd.DataFrame(columns=['lr'],data=test_predictions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBC\n",
    "best_gbm_countvect = grid_vect(gbm, parameters_gbm, X_train, X_test,model_features, parameters_text=parameters_vect)\n",
    "prediction_gbm = best_gbm_countvect.predict(X_test)\n",
    "gbm_score=f1_score(y_test,prediction_gbm,average='weighted')\n",
    "#best_scores['gbm_countvect']=best_gbm_countvect.best_score_\n",
    "#best_models['gbm_countvect']=best_gbm_countvect\n",
    "train_pred4=best_gbm_countvect.predict(X_train)\n",
    "test_pred4=best_gbm_countvect.predict(X_test)\n",
    "print(\"\\nAccuracy score of gbc algorithm -----> \" + str(gbm_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_mnb_model=mnb.set_params(**best_mnb_countvect.best_params_)\n",
    "best_gbm_countvect.fit(X,y)\n",
    "gbm_predictions=best_gbm_countvect.predict(test_df)\n",
    "prediction_df = pd.DataFrame(columns=['sentiment_class'],data=gbm_predictions)\n",
    "prediction_df = pd.concat([test_df_id,prediction_df],axis=1)\n",
    "prediction_df.to_csv(r'C:\\Users\\gaurav.singh.rawal\\Documents\\Gaurav\\Data Science\\Machine Learning-Predictive Analytics\\ML-Projects\\GIT - ML - Projects\\ML_Projects\\NLP - Sentiment Analysis\\HackerRank - Mothers Day\\predictions.csv',index=False,encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from sklearn.ensemble import VotingClassifier\n",
    "countvect_classifier= VotingClassifier(estimators=[('mnb', best_models['mnb_countvect']), ('lr', best_models['logreg_countvect']),\n",
    "                         ('sgd', best_models['sgd_countvect']), ('svc', best_models['svc_countvect']),\n",
    "                         ('gbm', best_models['gbm_countvect'])],voting='hard')\n",
    "countvect_classifier.fit(X_train,y_train)\n",
    "countvect_classifier_predictions=countvect_classifier.predict(X_test)\n",
    "countvect_classifier_predictions_score=100*(f1_score(y_test,countvect_classifier_predictions,average='weighted'))\n",
    "print(\"\\nAccuracy score of Voting Classifier algorithm -----> \" + str(countvect_classifier_predictions_score))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"df_train = pd.concat([pd.DataFrame(train_pred1),pd.DataFrame(train_pred2),\n",
    "                      pd.DataFrame(train_pred3),pd.DataFrame(train_pred4)], axis=1)\n",
    "df_test = pd.concat([pd.DataFrame(test_pred1),pd.DataFrame(test_pred2),pd.DataFrame(test_pred3)\n",
    "                     ,pd.DataFrame(test_pred4)], axis=1)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb=XGBClassifier()\n",
    "#xgb.fit(df_train,y_train)\n",
    "best_xgb_countvect = grid_vect(xgb, parameters_xgb, X_train, X_test,model_features, parameters_text=parameters_vect)\n",
    "prediction_xgb = best_xgb_countvect.predict(X_test)\n",
    "train_pred3=best_xgb_countvect.predict(X_train)\n",
    "test_pred3=best_xgb_countvect.predict(X_test)\n",
    "xgb_score=f1_score(y_test,prediction_xgb,average='weighted')\n",
    "print(\"\\nAccuracy score of XGB algorithm -----> \" + str(xgb_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultinomialNB\n",
    "model_features=gen_tfidf_features(tfidfvect)\n",
    "best_xgb_tfidf = grid_vect(xgb, parameters_xgb, X_train, X_test,model_features, parameters_text=parameters_tfidf)\n",
    "prediction_xgb=best_xgb_tfidf.predict(X_test)\n",
    "xgb_score=f1_score(y_test,prediction_xgb,average='weighted')\n",
    "#best_scores['mnb_countvect']=best_mnb_countvect.best_score_\n",
    "#best_models['mnb_countvect']=best_mnb_countvect\n",
    "#train_pred1=best_mnb_countvect.predict(X_train)\n",
    "#test_pred1=best_mnb_countvect.predict(X_test)\n",
    "print(\"\\nAccuracy score of Logreg tfidf algorithm -----> \" + str(xgb_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_mnb_model=mnb.set_params(**best_mnb_countvect.best_params_)\n",
    "best_xgb_countvect.fit(X,y)\n",
    "test_predictions=best_xgb_countvect.predict(test_df)\n",
    "#print(test_predictions.shape)\n",
    "prediction_df = pd.DataFrame(columns=['label'],data=test_predictions)\n",
    "prediction_df = pd.concat([test_data_id,prediction_df],axis=1)\n",
    "prediction_df.to_csv(r'C:\\Users\\gaurav.singh.rawal\\Documents\\Gaurav\\Data Science\\Machine Learning-Predictive Analytics\\ML-Projects\\GIT - ML - Projects\\ML_Projects\\NLP - Sentiment Analysis\\AnalyticsVidhya - Mobile Customer Feedback\\predictions_xgb.csv',index=False,encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions3=best_xgb_countvect.predict(test_df)\n",
    "prediction_df3 = pd.DataFrame(columns=['xgb'],data=test_predictions3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([pd.DataFrame(train_pred1),pd.DataFrame(train_pred2),pd.DataFrame(train_pred3),pd.DataFrame(train_pred4)], axis=1)\n",
    "df_test = pd.concat([pd.DataFrame(test_pred1),pd.DataFrame(test_pred2),pd.DataFrame(test_pred3),pd.DataFrame(test_pred4)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns=['mnb','lr','xgb','gbm']\n",
    "df_test.columns=['mnb','lr','xgb','gbm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "def hyperparameter_tuner(model,X_train,y_train,hp_list):\n",
    "    \n",
    "    hp_perf=[]\n",
    "    hp_model=RandomizedSearchCV(model,param_distributions=hp_list,n_iter=10,n_jobs=-1, scoring='f1_weighted',verbose=1, cv=5)\n",
    "    hp_model.fit(X_train,y_train)\n",
    "    best_hp_model=hp_model.best_estimator_\n",
    "    best_param=hp_model.best_params_\n",
    "    best_score=hp_model.best_score_  \n",
    "   \n",
    "    return best_hp_model,best_param,best_score\n",
    "\n",
    "xgb=XGBClassifier({\n",
    "    'objective': 'multi:softprob',\n",
    "    'eta': 0.1,\n",
    "    'max_depth': 25,\n",
    "    'silent' :1,\n",
    "    'num_class' : 3,\n",
    "    'eval_metric' : \"mlogloss\",\n",
    "    'min_child_weight': 1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.3,\n",
    "    'seed':17,\n",
    "    'num_rounds':2000,\n",
    "})\n",
    "xgb.fit(df_train,y_train)\n",
    "\n",
    "#best_model,best_params,best_score=hyperparameter_tuner(xgb,df_train,y_train,parameters_xgb)\n",
    "\n",
    "prediction_xgb = xgb.predict(df_test)\n",
    "xgb_score=100*(f1_score(y_test,prediction_xgb,average='weighted'))\n",
    "print(\"\\nAccuracy score of XGB stacking algorithm -----> \" + str(xgb_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "bagging_model = BaggingClassifier(RandomForestClassifier(max_depth=25,min_samples_leaf=2,n_estimators=100,class_weight='balanced'))\n",
    "bagging_model.fit(df_train,y_train)\n",
    "bagging_pred = bagging_model.predict(df_test)\n",
    "bagging_score=f1_score(y_test,bagging_pred,average='weighted')\n",
    "print(\"\\nAccuracy score of Random Forest Bagging algorithm -----> \" + str(bagging_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_mnb_model=mnb.set_params(**best_mnb_countvect.best_params_)\n",
    "#best_mnb_countvect.fit(X,y)\n",
    "#test_predictions=best_mnb_countvect.predict(test_df)\n",
    "#print(test_predictions.shape)\n",
    "final_test=pd.concat([prediction_df1,prediction_df2,prediction_df3],axis=1)\n",
    "final_pred=bagging_model.predict(final_test)\n",
    "prediction_df = pd.DataFrame(columns=['sentiment_class'],data=final_pred)\n",
    "prediction_df = pd.concat([test_df_id,prediction_df],axis=1)\n",
    "prediction_df.to_csv(r'C:\\Users\\gaurav.singh.rawal\\Documents\\Gaurav\\Data Science\\Machine Learning-Predictive Analytics\\ML-Projects\\GIT - ML - Projects\\ML_Projects\\NLP - Sentiment Analysis\\HackerRank - Mothers Day\\final_predictions.csv',index=False,encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_mnb_model=mnb.set_params(**best_mnb_countvect.best_params_)\n",
    "best_mnb_countvect.fit(X,y)\n",
    "test_predictions=best_mnb_countvect.predict(test_df)\n",
    "print(test_predictions.shape)\n",
    "prediction_df = pd.DataFrame(columns=['sentiment_class'],data=test_predictions)\n",
    "prediction_df = pd.concat([test_df_id,prediction_df],axis=1)\n",
    "prediction_df.to_csv(r'C:\\Users\\gaurav.singh.rawal\\Documents\\Gaurav\\Data Science\\Machine Learning-Predictive Analytics\\ML-Projects\\GIT - ML - Projects\\ML_Projects\\NLP - Sentiment Analysis\\HackerRank - Mothers Day\\predictions.csv',index=False,encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
