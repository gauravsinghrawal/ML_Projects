{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "from time import time\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import emoji\n",
    "import itertools\n",
    "import nltk\n",
    "from pprint import pprint\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.set(font_scale=1.3)\n",
    "from autocorrect import Speller\n",
    "from textblob import TextBlob\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split,RandomizedSearchCV,KFold\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import classification_report,f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "import xgboost as xgb\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from contractions import CONTRACTION_MAP\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(37)\n",
    "%matplotlib inline\n",
    "\n",
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv(r\"C:\\Users\\gaurav.singh.rawal\\Documents\\Gaurav\\Data Science\\Machine Learning-Predictive Analytics\\ML-Projects\\GIT - ML - Projects\\ML_Projects\\NLP - Sentiment Analysis\\HackerRank - Mothers Day\\train.csv\")\n",
    "test_data=pd.read_csv(r\"C:\\Users\\gaurav.singh.rawal\\Documents\\Gaurav\\Data Science\\Machine Learning-Predictive Analytics\\ML-Projects\\GIT - ML - Projects\\ML_Projects\\NLP - Sentiment Analysis\\HackerRank - Mothers Day\\test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>original_text</th>\n",
       "      <th>lang</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>original_author</th>\n",
       "      <th>sentiment_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.245025e+18</td>\n",
       "      <td>Happy #MothersDay to all you amazing mothers out there! I know it's hard not being able to see your mothers today but it's on all of us to do what we can to protect the most vulnerable members of our society. #BeatCoronaVirus pic.twitter.com/va4nFjFQ5B</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>BeenXXPired</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.245759e+18</td>\n",
       "      <td>Happy Mothers Day Mum - I'm sorry I can't be there to bring you Mothers day flowers &amp; a cwtch - honestly at this point I'd walk on hot coals to be able to. But I'll be there with bells on as soon as I can be. Love you lots xxx (p.s we need more photos!) https:// photos.app.goo.gl/M3vXBLrsCzD4TE bY7 …</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>FestiveFeeling</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.246087e+18</td>\n",
       "      <td>Happy mothers day To all This doing a mothers days work. Today been quiet but Had time to reflect. Dog walk, finish a jigsaw do the garden, learn few more guitar chords, drunk some strawberry gin and tonic and watch Lee evens on DVD. My favourite place to visit. #isolate pic.twitter.com/GZ0xVvF6f9</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>KrisAllenSak</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.244803e+18</td>\n",
       "      <td>Happy mothers day to this beautiful woman...royalty soothes you mummy jeremy and emerald and more #PrayForRoksie #UltimateLoveNG pic.twitter.com/oeetI22Pvv</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>Queenuchee</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.244876e+18</td>\n",
       "      <td>Remembering the 3 most amazing ladies who made me who I am! My late grandmother iris, mum carol and great grandmother Ethel. Missed but never forgotten! Happy mothers day to all those great mums out there! Love sent to all xxxx pic.twitter.com/xZZZdEybjE</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>brittan17446794</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  \\\n",
       "0  1.245025e+18   \n",
       "1  1.245759e+18   \n",
       "2  1.246087e+18   \n",
       "3  1.244803e+18   \n",
       "4  1.244876e+18   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                   original_text  \\\n",
       "0  Happy #MothersDay to all you amazing mothers out there! I know it's hard not being able to see your mothers today but it's on all of us to do what we can to protect the most vulnerable members of our society. #BeatCoronaVirus pic.twitter.com/va4nFjFQ5B                                                    \n",
       "1  Happy Mothers Day Mum - I'm sorry I can't be there to bring you Mothers day flowers & a cwtch - honestly at this point I'd walk on hot coals to be able to. But I'll be there with bells on as soon as I can be. Love you lots xxx (p.s we need more photos!) https:// photos.app.goo.gl/M3vXBLrsCzD4TE bY7 …   \n",
       "2  Happy mothers day To all This doing a mothers days work. Today been quiet but Had time to reflect. Dog walk, finish a jigsaw do the garden, learn few more guitar chords, drunk some strawberry gin and tonic and watch Lee evens on DVD. My favourite place to visit. #isolate pic.twitter.com/GZ0xVvF6f9      \n",
       "3  Happy mothers day to this beautiful woman...royalty soothes you mummy jeremy and emerald and more #PrayForRoksie #UltimateLoveNG pic.twitter.com/oeetI22Pvv                                                                                                                                                     \n",
       "4  Remembering the 3 most amazing ladies who made me who I am! My late grandmother iris, mum carol and great grandmother Ethel. Missed but never forgotten! Happy mothers day to all those great mums out there! Love sent to all xxxx pic.twitter.com/xZZZdEybjE                                                  \n",
       "\n",
       "  lang retweet_count  original_author  sentiment_class  \n",
       "0  en   0             BeenXXPired      0                \n",
       "1  en   1             FestiveFeeling   0                \n",
       "2  en   0             KrisAllenSak    -1                \n",
       "3  en   0             Queenuchee       0                \n",
       "4  en   0             brittan17446794 -1                "
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3235 entries, 0 to 3234\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   id               3235 non-null   float64\n",
      " 1   original_text    3235 non-null   object \n",
      " 2   lang             3231 non-null   object \n",
      " 3   retweet_count    3231 non-null   object \n",
      " 4   original_author  3235 non-null   object \n",
      " 5   sentiment_class  3235 non-null   int64  \n",
      "dtypes: float64(1), int64(1), object(4)\n",
      "memory usage: 151.8+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_f978777a_a9b7_11ea_814f_d43b045a62ecrow0_col0 {\n",
       "            background-color:  #9e9ac8;\n",
       "            color:  #000000;\n",
       "        }    #T_f978777a_a9b7_11ea_814f_d43b045a62ecrow0_col1 {\n",
       "            background-color:  #3f007d;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_f978777a_a9b7_11ea_814f_d43b045a62ecrow1_col0 {\n",
       "            background-color:  #fcfbfd;\n",
       "            color:  #000000;\n",
       "        }    #T_f978777a_a9b7_11ea_814f_d43b045a62ecrow1_col1 {\n",
       "            background-color:  #fcfbfd;\n",
       "            color:  #000000;\n",
       "        }    #T_f978777a_a9b7_11ea_814f_d43b045a62ecrow2_col0 {\n",
       "            background-color:  #3f007d;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_f978777a_a9b7_11ea_814f_d43b045a62ecrow2_col1 {\n",
       "            background-color:  #fcfbfd;\n",
       "            color:  #000000;\n",
       "        }</style><table id=\"T_f978777a_a9b7_11ea_814f_d43b045a62ec\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >sentiment_class</th>        <th class=\"col_heading level0 col1\" >original_text</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_f978777a_a9b7_11ea_814f_d43b045a62eclevel0_row0\" class=\"row_heading level0 row0\" >1</th>\n",
       "                        <td id=\"T_f978777a_a9b7_11ea_814f_d43b045a62ecrow0_col0\" class=\"data row0 col0\" >0</td>\n",
       "                        <td id=\"T_f978777a_a9b7_11ea_814f_d43b045a62ecrow0_col1\" class=\"data row0 col1\" >1701</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f978777a_a9b7_11ea_814f_d43b045a62eclevel0_row1\" class=\"row_heading level0 row1\" >0</th>\n",
       "                        <td id=\"T_f978777a_a9b7_11ea_814f_d43b045a62ecrow1_col0\" class=\"data row1 col0\" >-1</td>\n",
       "                        <td id=\"T_f978777a_a9b7_11ea_814f_d43b045a62ecrow1_col1\" class=\"data row1 col1\" >769</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f978777a_a9b7_11ea_814f_d43b045a62eclevel0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_f978777a_a9b7_11ea_814f_d43b045a62ecrow2_col0\" class=\"data row2 col0\" >1</td>\n",
       "                        <td id=\"T_f978777a_a9b7_11ea_814f_d43b045a62ecrow2_col1\" class=\"data row2 col1\" >765</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1bbce66e1c8>"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = train_data.groupby('sentiment_class').count()['original_text'].reset_index().sort_values(by='original_text',ascending=False)\n",
    "temp.style.background_gradient(cmap='Purples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1bbcddf7988>"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='sentiment_class',data=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['retweet_count']=train_data['retweet_count'].astype(str)\n",
    "test_data['retweet_count']=test_data['retweet_count'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"retweet_count\"]= train_data[\"retweet_count\"].str.replace(r\"[a-zA-Z$@#&*!''-/]\",'')\n",
    "train_data[\"retweet_count\"]= train_data[\"retweet_count\"].replace(r'^\\s*$', np.nan, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"retweet_count\"]= test_data[\"retweet_count\"].str.replace(r\"[a-zA-Z$@#&*!''-/]\",'')\n",
    "test_data[\"retweet_count\"]= test_data[\"retweet_count\"].replace(r'^\\s*$', np.nan, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['retweet_count']=pd.to_numeric(train_data['retweet_count'])\n",
    "test_data['retweet_count']=pd.to_numeric(test_data['retweet_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.loc[train_data[\"retweet_count\"]<0,'retweet_count']=0\n",
    "train_data.loc[train_data[\"retweet_count\"].isna(),'retweet_count']=0\n",
    "test_data.loc[test_data[\"retweet_count\"]<0,'retweet_count']=0\n",
    "test_data.loc[test_data[\"retweet_count\"].isna(),'retweet_count']=0\n",
    "train_data.loc[train_data[\"retweet_count\"]>1000,'retweet_count']=1000\n",
    "test_data.loc[test_data[\"retweet_count\"]>1000,'retweet_count']=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"retweet_count\"]=train_data[\"retweet_count\"].astype(int)\n",
    "test_data[\"retweet_count\"]=test_data[\"retweet_count\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en                            2994\n",
       " pink Peruvian opal! via      4   \n",
       " Find More                    2   \n",
       "WORLDS OKAYEST MOTHER! &lt    2   \n",
       "&gt                           2   \n",
       "                             ..   \n",
       "0.7231463898                  1   \n",
       "0.1666662023                  1   \n",
       "0.6236458018                  1   \n",
       " very much loved🥰️ …          1   \n",
       "0.0903948317                  1   \n",
       "Name: lang, Length: 232, dtype: int64"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['lang'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['original_text']=train_data['original_text'].apply(expand_contractions)\n",
    "test_data['original_text']=test_data['original_text'].apply(expand_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of words in the original_text ##\n",
    "train_data[\"num_words\"] = train_data[\"original_text\"].apply(lambda x: len(str(x).split()))\n",
    "test_data[\"num_words\"] = test_data[\"original_text\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "## Number of unique words in the original_text ##\n",
    "train_data[\"num_unique_words\"] = train_data[\"original_text\"].apply(lambda x: len(set(str(x).split())))\n",
    "test_data[\"num_unique_words\"] = test_data[\"original_text\"].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "## Number of characters in the original_text ##\n",
    "train_data[\"num_chars\"] = train_data[\"original_text\"].apply(lambda x: len(str(x)))\n",
    "test_data[\"num_chars\"] = test_data[\"original_text\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "## Number of stopwords in the original_text ##\n",
    "train_data[\"num_stopwords\"] = train_data[\"original_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "test_data[\"num_stopwords\"] = test_data[\"original_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "\n",
    "## Number of punctuations in the original_text ##\n",
    "train_data[\"num_punctuations\"] =train_data['original_text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "test_data[\"num_punctuations\"] =test_data['original_text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "\n",
    "## Number of title case words in the original_text ##\n",
    "train_data[\"num_words_upper\"] = train_data[\"original_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "test_data[\"num_words_upper\"] = test_data[\"original_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "\n",
    "## Number of title case words in the original_text ##\n",
    "train_data[\"num_words_title\"] = train_data[\"original_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "test_data[\"num_words_title\"] = test_data[\"original_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "\n",
    "## Average length of the words in the original_text ##\n",
    "train_data[\"mean_word_len\"] = train_data[\"original_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "test_data[\"mean_word_len\"] = test_data[\"original_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>original_text</th>\n",
       "      <th>lang</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>original_author</th>\n",
       "      <th>sentiment_class</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>mean_word_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.245025e+18</td>\n",
       "      <td>Happy #MothersDay to all you amazing mothers out there! I know it is hard not being able to see your mothers today but it is on all of us to do what we can to protect the most vulnerable members of our society. #BeatCoronaVirus pic.twitter.com/va4nFjFQ5B</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>BeenXXPired</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>37</td>\n",
       "      <td>254</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.245759e+18</td>\n",
       "      <td>Happy Mothers Day Mum - I am sorry I cannot be there to bring you Mothers day flowers &amp; a cwtch - honestly at this point I would walk on hot coals to be able to. But I will be there with bells on as soon as I can be. Love you lots xxx (p.s we need more photos!) https:// photos.app.goo.gl/M3vXBLrsCzD4TE bY7 …</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>FestiveFeeling</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>50</td>\n",
       "      <td>309</td>\n",
       "      <td>28</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>3.920635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.246087e+18</td>\n",
       "      <td>Happy mothers day To all This doing a mothers days work. Today been quiet but Had time to reflect. Dog walk, finish a jigsaw do the garden, learn few more guitar chords, drunk some strawberry gin and tonic and watch Lee evens on DVD. My favourite place to visit. #isolate pic.twitter.com/GZ0xVvF6f9</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>KrisAllenSak</td>\n",
       "      <td>-1</td>\n",
       "      <td>51</td>\n",
       "      <td>47</td>\n",
       "      <td>298</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>4.862745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.244803e+18</td>\n",
       "      <td>Happy mothers day to this beautiful woman...royalty soothes you mummy jeremy and emerald and more #PrayForRoksie #UltimateLoveNG pic.twitter.com/oeetI22Pvv</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>Queenuchee</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>155</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.244876e+18</td>\n",
       "      <td>Remembering the 3 most amazing ladies who made me who I am! My late grandmother iris, mum carol and great grandmother Ethel. Missed but never forgotten! Happy mothers day to all those great mums out there! Love sent to all xxxx pic.twitter.com/xZZZdEybjE</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>brittan17446794</td>\n",
       "      <td>-1</td>\n",
       "      <td>42</td>\n",
       "      <td>37</td>\n",
       "      <td>254</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5.071429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  \\\n",
       "0  1.245025e+18   \n",
       "1  1.245759e+18   \n",
       "2  1.246087e+18   \n",
       "3  1.244803e+18   \n",
       "4  1.244876e+18   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                           original_text  \\\n",
       "0  Happy #MothersDay to all you amazing mothers out there! I know it is hard not being able to see your mothers today but it is on all of us to do what we can to protect the most vulnerable members of our society. #BeatCoronaVirus pic.twitter.com/va4nFjFQ5B                                                          \n",
       "1  Happy Mothers Day Mum - I am sorry I cannot be there to bring you Mothers day flowers & a cwtch - honestly at this point I would walk on hot coals to be able to. But I will be there with bells on as soon as I can be. Love you lots xxx (p.s we need more photos!) https:// photos.app.goo.gl/M3vXBLrsCzD4TE bY7 …   \n",
       "2  Happy mothers day To all This doing a mothers days work. Today been quiet but Had time to reflect. Dog walk, finish a jigsaw do the garden, learn few more guitar chords, drunk some strawberry gin and tonic and watch Lee evens on DVD. My favourite place to visit. #isolate pic.twitter.com/GZ0xVvF6f9              \n",
       "3  Happy mothers day to this beautiful woman...royalty soothes you mummy jeremy and emerald and more #PrayForRoksie #UltimateLoveNG pic.twitter.com/oeetI22Pvv                                                                                                                                                             \n",
       "4  Remembering the 3 most amazing ladies who made me who I am! My late grandmother iris, mum carol and great grandmother Ethel. Missed but never forgotten! Happy mothers day to all those great mums out there! Love sent to all xxxx pic.twitter.com/xZZZdEybjE                                                          \n",
       "\n",
       "  lang  retweet_count  original_author  sentiment_class  num_words  \\\n",
       "0  en   0              BeenXXPired      0                45          \n",
       "1  en   1              FestiveFeeling   0                63          \n",
       "2  en   0              KrisAllenSak    -1                51          \n",
       "3  en   0              Queenuchee       0                18          \n",
       "4  en   0              brittan17446794 -1                42          \n",
       "\n",
       "   num_unique_words  num_chars  num_stopwords  num_punctuations  \\\n",
       "0  37                254        27             7                  \n",
       "1  50                309        28             16                 \n",
       "2  47                298        20             11                 \n",
       "3  17                155        6              8                  \n",
       "4  37                254        15             8                  \n",
       "\n",
       "   num_words_upper  num_words_title  mean_word_len  \n",
       "0  1                2                4.666667       \n",
       "1  5                12               3.920635       \n",
       "2  1                8                4.862745       \n",
       "3  0                1                7.666667       \n",
       "4  1                7                5.071429       "
      ]
     },
     "execution_count": 510,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_dist(df, col):\n",
    "    print('Descriptive stats for {}'.format(col))\n",
    "    print('-'*(len(col)+22))\n",
    "    print(df.groupby('sentiment_class')[col].describe())\n",
    "    bins = np.arange(df[col].min(), df[col].max() + 1)\n",
    "    g = sns.FacetGrid(df, col='sentiment_class', size=5, hue='sentiment_class', palette=\"PuBuGn_d\")\n",
    "    g = g.map(sns.distplot, col, kde=False, norm_hist=True, bins=bins)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanText(BaseEstimator, TransformerMixin):\n",
    "    def remove_mentions(self, input_text):\n",
    "        return re.sub(r'@\\w+', '', input_text)\n",
    "    \n",
    "    def remove_urls(self, input_text):\n",
    "        return re.sub(r'http.?://[^\\s]+[\\s]?', '', input_text)\n",
    "    \n",
    "    def emoji_oneword(self, input_text):\n",
    "        # By compressing the underscore, the emoji is kept as one word\n",
    "        return input_text.replace('_','')\n",
    "    \n",
    "    def remove_punctuation(self, input_text):\n",
    "        # Make translation table\n",
    "        punct = string.punctuation\n",
    "        trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space\n",
    "        return input_text.translate(trantab)\n",
    "    def remove_digits(self, input_text):\n",
    "        return re.sub('\\d+', '', input_text)\n",
    "    \n",
    "    def split_words(self,input_text):\n",
    "        return ''.join(re.findall('[A-Z][^A-Z]*', input_text))\n",
    "    \n",
    "    def standardize_words(self,input_text):\n",
    "        return ''.join(''.join(s)[:2] for _, s in itertools.groupby(input_text))\n",
    "    \n",
    "    def to_lower(self, input_text):\n",
    "        return input_text.lower()\n",
    "    \n",
    "    '''def correct_spell(self,input_text):\n",
    "        words = input_text.split() \n",
    "        clean_words = [str(TextBlob(word).correct()) for word in words] \n",
    "        return \" \".join(clean_words)'''\n",
    "    \n",
    "    \"\"\"def correct_spell(self,input_text):\n",
    "        words = input_text.split() \n",
    "        spell = Speller(lang='en') \n",
    "        correct_words = [spell(word) for word in words] \n",
    "        return \" \".join(correct_words) \"\"\"\n",
    "    '''def remove_meaningless(self,input_text):\n",
    "        words = set(nltk.corpus.words.words())\n",
    "        return \" \".join(w for w in nltk.wordpunct_tokenize(input_text) if w.lower() in words or not w.isalpha())'''\n",
    "    \n",
    "    def remove_stopwords(self, input_text):\n",
    "        stopwords_list = stopwords.words('english')\n",
    "        # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "        whitelist = [\"n't\", \"not\", \"no\"]\n",
    "        words = input_text.split() \n",
    "        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
    "        return \" \".join(clean_words) \n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        clean_X = X.apply(self.remove_mentions).apply(self.remove_urls).apply(self.emoji_oneword).apply(self.remove_punctuation).apply(self.remove_digits).apply(self.split_words).apply(self.standardize_words).apply(self.to_lower).apply(self.remove_stopwords)\n",
    "        return clean_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = CleanText()\n",
    "train_data['original_text'] = ct.fit_transform(train_data.original_text)\n",
    "test_data['original_text']=ct.fit_transform(test_data.original_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text_df):\n",
    "    text_df[\"clean_text\"]= text_df[\"original_text\"].str.replace(\"mum\", \"mother\", case = False)\n",
    "    text_df[\"clean_text\"]= text_df[\"clean_text\"].str.replace(\"mom\", \"mother\", case = False)\n",
    "    text_df[\"clean_text\"]= text_df[\"clean_text\"].str.replace(\"mothersday\",\"mother day\") \n",
    "    text_df[\"clean_text\"]= text_df[\"clean_text\"].str.replace(\"httpswww\",\"\") \n",
    "    text_df[\"clean_text\"]= text_df[\"clean_text\"].str.replace(\"http\",\"\") \n",
    "    text_df[\"clean_text\"]= text_df[\"clean_text\"].str.replace(\"u\",\"\") \n",
    "    return text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=clean_text(train_data)\n",
    "test_df=clean_text(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.WordPunctTokenizer()\n",
    "def pos_tagger(input_text):\n",
    "    text = tokenizer.tokenize(input_text)\n",
    "    tagged_text = nltk.pos_tag(text)\n",
    "    new_text = \"\"\n",
    "    for tag in tagged_text:\n",
    "        new_text = new_text + \" \" + tag[1]\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['pos_text'] = train_df['clean_text'].apply(pos_tagger)\n",
    "test_df['pos_text'] = test_df['clean_text'].apply(pos_tagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(input_text):\n",
    "        porter = PorterStemmer()\n",
    "        words = input_text.split() \n",
    "        stemmed_words = [porter.stem(word) for word in words]\n",
    "        return \" \".join(stemmed_words)\n",
    "    \n",
    "def lemmatize_text(input_text):\n",
    "        word_list = nltk.word_tokenize(input_text)\n",
    "        lemmatizer = WordNetLemmatizer() \n",
    "        lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "\n",
    "        return lemmatized_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['clean_text'] = train_df['clean_text'].apply(stemming)\n",
    "test_df['clean_text'] = test_df['clean_text'].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['clean_text'] = train_df['clean_text'].apply(lemmatize_text)\n",
    "test_df['clean_text'] = test_df['clean_text'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnExtractor(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "    def transform(self, X, **transform_params):\n",
    "        return X[self.cols]\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_df.sentiment_class\n",
    "X = train_df.drop(['id','original_text','sentiment_class','lang','original_author'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>mean_word_len</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>pos_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>46</td>\n",
       "      <td>275</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>3.928571</td>\n",
       "      <td>yeah cook potato year old mean threw bag spd toilet happi mother day made breakfast time thoght cool draw nake ladi dachshnd overhead projector psycholog class</td>\n",
       "      <td>NN VBD NNS NNS JJ JJ NN NN JJ NN JJ NNS NN VBD NN NN JJ NN VBG JJ NN NN JJ NN NN NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>192</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7.772727</td>\n",
       "      <td>happi mother day mother step mother grandmoth dad cover parent role mother day motheringsnday s twitter com soapandglori tat</td>\n",
       "      <td>JJ NNS NN NNS VBP NNS NNS VBP JJ JJ NNS RB NN JJ JJ NN NN NN NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>35</td>\n",
       "      <td>237</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>4.409091</td>\n",
       "      <td>love peopl k howev awok saw mother day trend woke american immedi happi mother day mother across way god bless love share sacrific make</td>\n",
       "      <td>NN NNS VBP RB RB VBD CC NN VBG VBD JJ RB JJ NNS NN NNS IN NN NN NN IN NN NNS VBP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>28</td>\n",
       "      <td>203</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>happi st birthday happi mother ’ day mother can ’ t see today keep safe shame mani other ignor advic covidー mother day motheringsnday pic twitter com vsvmkmfptr</td>\n",
       "      <td>JJ NN NN JJ NN JJ JJ NN NN MD VB JJ NN NN VB JJ NN JJ NNS VBG NN NN NN NN VBD JJ NN NN NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>34</td>\n",
       "      <td>200</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.153846</td>\n",
       "      <td>happi mother day wonderfl mother world live present alway reli mother rock famili whatev life throw</td>\n",
       "      <td>JJ NNS NN IN NNS NN VBP NN RB RB JJ NN NNS WDT NN NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1382</th>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>24</td>\n",
       "      <td>202</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6.807692</td>\n",
       "      <td>happi mother ’ day crazi bitch i ’ m sorri covid bloodi rine love drag qeen lovehaterelationship iloveyomorethanninawest mother day pic twitter com koozccyf</td>\n",
       "      <td>JJ NN JJ JJ NN JJ NN NN VBP NN NN NN NN VBD JJR NN NNS JJ JJ NN NN VBP NN NN NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1383</th>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>36</td>\n",
       "      <td>271</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>5.974359</td>\n",
       "      <td>mother can not get bingo broght bingo top prize incld terri chocol orang kinder happi hippo pack trkish delight mother fave happymoth day mother day instagram com ctnnm igshid kaehoi</td>\n",
       "      <td>NN NN VB JJ JJ NN NN NNS VBD JJ NN NN NN JJ NN NN JJ NNS NNS VBP DT NN RB NN JJ NN NN NN NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1384</th>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>34</td>\n",
       "      <td>226</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4.675000</td>\n",
       "      <td>despit bllshit today one favorit day lonng time danc dont took photo drive daphn chine qiz happi mother ’ day fckcovid mother day famili</td>\n",
       "      <td>IN VBN NN CD NN NNS VBP NN JJ NNS VBD NNS JJ NN JJ NN JJ NN JJ JJ NN VB RB NN NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385</th>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>270</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>7.468750</td>\n",
       "      <td>happi mother day firstli qeen danielesllivan loveyowithallmyheart also wish special ladi wonderfl mother day mother mothermi instagram com dqjnqhzlov tllkkowcjdbba vdan igshid qtlpjnf</td>\n",
       "      <td>JJ NNS NN RB JJ NN NN RB VBG JJ NNS VBP NNS NN NNS VBP JJ NN NN NN NNS VBP NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1386</th>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>33</td>\n",
       "      <td>253</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>5.864865</td>\n",
       "      <td>mother day walk live tini villag rral lincolnshir walk mile withot see sol happi mother ’ day amaz woman know socialdistanc mentalhealth doingorbest mother day pic twitter com nmohzz</td>\n",
       "      <td>NNS NN VBP JJ JJ NN JJ NN NN NNS RB VBG NN JJ NN JJ JJ NN JJ NNS VBP VBG NN JJS NN NN VBP NN NN NN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1387 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      retweet_count  num_words  num_unique_words  num_chars  num_stopwords  \\\n",
       "0     0              56         46                275        24              \n",
       "1     0              22         22                192        6               \n",
       "2     0              44         35                237        20              \n",
       "3     1              30         28                203        9               \n",
       "4     0              39         34                200        20              \n",
       "...  ..              ..         ..                ...        ..              \n",
       "1382  0              26         24                202        9               \n",
       "1383  0              39         36                271        12              \n",
       "1384  0              40         34                226        18              \n",
       "1385  0              32         27                270        10              \n",
       "1386  0              37         33                253        13              \n",
       "\n",
       "      num_punctuations  num_words_upper  num_words_title  mean_word_len  \\\n",
       "0     10                7                10               3.928571        \n",
       "1     13                0                3                7.772727        \n",
       "2     9                 3                9                4.409091        \n",
       "3     10                1                5                5.800000        \n",
       "4     2                 0                3                4.153846        \n",
       "...  ..                ..               ..                     ...        \n",
       "1382  7                 1                3                6.807692        \n",
       "1383  19                0                11               5.974359        \n",
       "1384  11                0                5                4.675000        \n",
       "1385  20                1                9                7.468750        \n",
       "1386  9                 1                8                5.864865        \n",
       "\n",
       "                                                                                                                                                                                   clean_text  \\\n",
       "0     yeah cook potato year old mean threw bag spd toilet happi mother day made breakfast time thoght cool draw nake ladi dachshnd overhead projector psycholog class                           \n",
       "1     happi mother day mother step mother grandmoth dad cover parent role mother day motheringsnday s twitter com soapandglori tat                                                              \n",
       "2     love peopl k howev awok saw mother day trend woke american immedi happi mother day mother across way god bless love share sacrific make                                                   \n",
       "3     happi st birthday happi mother ’ day mother can ’ t see today keep safe shame mani other ignor advic covidー mother day motheringsnday pic twitter com vsvmkmfptr                          \n",
       "4     happi mother day wonderfl mother world live present alway reli mother rock famili whatev life throw                                                                                       \n",
       "...                                                                                                   ...                                                                                       \n",
       "1382  happi mother ’ day crazi bitch i ’ m sorri covid bloodi rine love drag qeen lovehaterelationship iloveyomorethanninawest mother day pic twitter com koozccyf                              \n",
       "1383  mother can not get bingo broght bingo top prize incld terri chocol orang kinder happi hippo pack trkish delight mother fave happymoth day mother day instagram com ctnnm igshid kaehoi    \n",
       "1384  despit bllshit today one favorit day lonng time danc dont took photo drive daphn chine qiz happi mother ’ day fckcovid mother day famili                                                  \n",
       "1385  happi mother day firstli qeen danielesllivan loveyowithallmyheart also wish special ladi wonderfl mother day mother mothermi instagram com dqjnqhzlov tllkkowcjdbba vdan igshid qtlpjnf   \n",
       "1386  mother day walk live tini villag rral lincolnshir walk mile withot see sol happi mother ’ day amaz woman know socialdistanc mentalhealth doingorbest mother day pic twitter com nmohzz    \n",
       "\n",
       "                                                                                                 pos_text  \n",
       "0      NN VBD NNS NNS JJ JJ NN NN JJ NN JJ NNS NN VBD NN NN JJ NN VBG JJ NN NN JJ NN NN NN                 \n",
       "1      JJ NNS NN NNS VBP NNS NNS VBP JJ JJ NNS RB NN JJ JJ NN NN NN NNS                                    \n",
       "2      NN NNS VBP RB RB VBD CC NN VBG VBD JJ RB JJ NNS NN NNS IN NN NN NN IN NN NNS VBP                    \n",
       "3      JJ NN NN JJ NN JJ JJ NN NN MD VB JJ NN NN VB JJ NN JJ NNS VBG NN NN NN NN VBD JJ NN NN NN           \n",
       "4      JJ NNS NN IN NNS NN VBP NN RB RB JJ NN NNS WDT NN NNS                                               \n",
       "...                                                      ...                                               \n",
       "1382   JJ NN JJ JJ NN JJ NN NN VBP NN NN NN NN VBD JJR NN NNS JJ JJ NN NN VBP NN NN NN                     \n",
       "1383   NN NN VB JJ JJ NN NN NNS VBD JJ NN NN NN JJ NN NN JJ NNS NNS VBP DT NN RB NN JJ NN NN NN NN         \n",
       "1384   IN VBN NN CD NN NNS VBP NN JJ NNS VBD NNS JJ NN JJ NN JJ NN JJ JJ NN VB RB NN NN                    \n",
       "1385   JJ NNS NN RB JJ NN NN RB VBG JJ NNS VBP NNS NN NNS VBP JJ NN NN NN NNS VBP NN                       \n",
       "1386   NNS NN VBP JJ JJ NN JJ NN NN NNS RB VBG NN JJ NN JJ JJ NN JJ NNS VBP VBG NN JJS NN NN VBP NN NN NN  \n",
       "\n",
       "[1387 rows x 11 columns]"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_id=test_df.id\n",
    "test_df.drop(['id','original_text','lang','original_author'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>original_text</th>\n",
       "      <th>lang</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>original_author</th>\n",
       "      <th>sentiment_class</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>mean_word_len</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>pos_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.245025e+18</td>\n",
       "      <td>happy mothersday amazing mothers know hard not able see mothers today us protect vulnerable members society beatcoronavirus pic twitter com vanfjfqb</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>BeenXXPired</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>37</td>\n",
       "      <td>254</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>happi mother day amaz mother know hard not abl see mother today s protect vlnerabl member societi beatcoronavir pic twitter com vanfjfqb</td>\n",
       "      <td>JJ NN NN VBG NNS VBP RB RB JJ NN NNS NN VBP NN JJ NNS VBP NNS VBP NN NN NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.245759e+18</td>\n",
       "      <td>happy mothers day mum sorry cannot bring mothers day flowers cwtch honestly point would walk hot coals able bells soon love lots xx need photos https photos app goo gl mvxblrsczdte</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>FestiveFeeling</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>50</td>\n",
       "      <td>309</td>\n",
       "      <td>28</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>3.920635</td>\n",
       "      <td>happi mother day mother sorri can not bring mother day flower cwtch honestli point wold walk hot coal abl bell soon love lot xx need photo s photo app goo gl mvxblrsczdte</td>\n",
       "      <td>JJ NNS NN NN NN NN VBG NNS NN NNS VBP RB JJ JJ NN JJ NNS JJ NNS RB VBP NNS VBP VBP NNS VB NNS JJ JJ NN NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.246087e+18</td>\n",
       "      <td>happy mothers day mothers days work today quiet time reflect dog walk finish jigsaw garden learn guitar chords drunk strawberry gin tonic watch lee evens dvd favourite place visit isolate pic twitter com gzxvvff</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>KrisAllenSak</td>\n",
       "      <td>-1</td>\n",
       "      <td>51</td>\n",
       "      <td>47</td>\n",
       "      <td>298</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>4.862745</td>\n",
       "      <td>happi mother day mother day work today qiet time reflect dog walk finish jigsaw garden learn gitar chord drnk strawberri gin tonic watch lee even dvd favorit place visit isol pic twitter com gzxvvff</td>\n",
       "      <td>JJ NNS NN NNS NNS VBP NN JJ NN JJ NN VBP JJ NN NN NN NN NNS VBP JJ NN JJ NN NN VBZ JJ JJ NN NN NN NN NN NN NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.244803e+18</td>\n",
       "      <td>happy mothers day beautiful woman royalty soothes mummy jeremy emerald prayforroksie ultimateloveng pic twitter com oeetipvv</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>Queenuchee</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>155</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7.666667</td>\n",
       "      <td>happi mother day beatifl woman royalti sooth mothermi jeremi emerald prayforroksi ltimateloveng pic twitter com oeetipvv</td>\n",
       "      <td>JJ NNS NN IN NN NN NNS VBP NN NN NN NN NN NN NN NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.244876e+18</td>\n",
       "      <td>remembering amazing ladies made late grandmother iris mum carol great grandmother ethel missed never forgotten happy mothers day great mums love sent xx pic twitter com xzzdeybje</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>brittan17446794</td>\n",
       "      <td>-1</td>\n",
       "      <td>42</td>\n",
       "      <td>37</td>\n",
       "      <td>254</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5.071429</td>\n",
       "      <td>rememb amaz ladi made late grandmoth iri mother carol great grandmoth ethel miss never forgotten happi mother day great mother love sent xx pic twitter com xzzdeybj</td>\n",
       "      <td>VBG VBG NNS VBN JJ NN NN NN NN JJ NN NN VBD RB VBN JJ NNS NN JJ NNS VBP VBD NNP NN NN NN NN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  \\\n",
       "0  1.245025e+18   \n",
       "1  1.245759e+18   \n",
       "2  1.246087e+18   \n",
       "3  1.244803e+18   \n",
       "4  1.244876e+18   \n",
       "\n",
       "                                                                                                                                                                                                         original_text  \\\n",
       "0  happy mothersday amazing mothers know hard not able see mothers today us protect vulnerable members society beatcoronavirus pic twitter com vanfjfqb                                                                  \n",
       "1  happy mothers day mum sorry cannot bring mothers day flowers cwtch honestly point would walk hot coals able bells soon love lots xx need photos https photos app goo gl mvxblrsczdte                                  \n",
       "2  happy mothers day mothers days work today quiet time reflect dog walk finish jigsaw garden learn guitar chords drunk strawberry gin tonic watch lee evens dvd favourite place visit isolate pic twitter com gzxvvff   \n",
       "3  happy mothers day beautiful woman royalty soothes mummy jeremy emerald prayforroksie ultimateloveng pic twitter com oeetipvv                                                                                          \n",
       "4  remembering amazing ladies made late grandmother iris mum carol great grandmother ethel missed never forgotten happy mothers day great mums love sent xx pic twitter com xzzdeybje                                    \n",
       "\n",
       "  lang  retweet_count  original_author  sentiment_class  num_words  \\\n",
       "0  en   0              BeenXXPired      0                45          \n",
       "1  en   1              FestiveFeeling   0                63          \n",
       "2  en   0              KrisAllenSak    -1                51          \n",
       "3  en   0              Queenuchee       0                18          \n",
       "4  en   0              brittan17446794 -1                42          \n",
       "\n",
       "   num_unique_words  num_chars  num_stopwords  num_punctuations  \\\n",
       "0  37                254        27             7                  \n",
       "1  50                309        28             16                 \n",
       "2  47                298        20             11                 \n",
       "3  17                155        6              8                  \n",
       "4  37                254        15             8                  \n",
       "\n",
       "   num_words_upper  num_words_title  mean_word_len  \\\n",
       "0  1                2                4.666667        \n",
       "1  5                12               3.920635        \n",
       "2  1                8                4.862745        \n",
       "3  0                1                7.666667        \n",
       "4  1                7                5.071429        \n",
       "\n",
       "                                                                                                                                                                                               clean_text  \\\n",
       "0  happi mother day amaz mother know hard not abl see mother today s protect vlnerabl member societi beatcoronavir pic twitter com vanfjfqb                                                                 \n",
       "1  happi mother day mother sorri can not bring mother day flower cwtch honestli point wold walk hot coal abl bell soon love lot xx need photo s photo app goo gl mvxblrsczdte                               \n",
       "2  happi mother day mother day work today qiet time reflect dog walk finish jigsaw garden learn gitar chord drnk strawberri gin tonic watch lee even dvd favorit place visit isol pic twitter com gzxvvff   \n",
       "3  happi mother day beatifl woman royalti sooth mothermi jeremi emerald prayforroksi ltimateloveng pic twitter com oeetipvv                                                                                 \n",
       "4  rememb amaz ladi made late grandmoth iri mother carol great grandmoth ethel miss never forgotten happi mother day great mother love sent xx pic twitter com xzzdeybj                                     \n",
       "\n",
       "                                                                                                         pos_text  \n",
       "0   JJ NN NN VBG NNS VBP RB RB JJ NN NNS NN VBP NN JJ NNS VBP NNS VBP NN NN NN                                     \n",
       "1   JJ NNS NN NN NN NN VBG NNS NN NNS VBP RB JJ JJ NN JJ NNS JJ NNS RB VBP NNS VBP VBP NNS VB NNS JJ JJ NN NN      \n",
       "2   JJ NNS NN NNS NNS VBP NN JJ NN JJ NN VBP JJ NN NN NN NN NNS VBP JJ NN JJ NN NN VBZ JJ JJ NN NN NN NN NN NN NN  \n",
       "3   JJ NNS NN IN NN NN NNS VBP NN NN NN NN NN NN NN NN                                                             \n",
       "4   VBG VBG NNS VBN JJ NN NN NN NN JJ NN NN VBD RB VBN JJ NNS NN JJ NNS VBP VBD NNP NN NN NN NN                    "
      ]
     },
     "execution_count": 525,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html\n",
    "def gen_model_features(vect=None, is_w2v=None):\n",
    "    \n",
    "    textcountscols = ['num_words','num_unique_words','num_chars','num_stopwords'\n",
    "                      ,'num_punctuations','num_words_upper','num_words_title','mean_word_len']\n",
    "    \n",
    "    if is_w2v:\n",
    "        w2vcols = []\n",
    "        for i in range(SIZE):\n",
    "            w2vcols.append(i)\n",
    "        features = FeatureUnion([('textcounts', ColumnExtractor(cols=textcountscols))\n",
    "                                 , ('w2v', ColumnExtractor(cols=w2vcols))], n_jobs=-1)\n",
    "    else:\n",
    "        features = FeatureUnion([('textcounts', ColumnExtractor(cols=textcountscols))\n",
    "                                 ,('pipe1', Pipeline([('cleantext', ColumnExtractor(cols='clean_text'))         \n",
    "                                                     ,('vect', vect)])),\n",
    "                                ('pipe2', Pipeline([('postext', ColumnExtractor(cols='pos_text'))         \n",
    "                                                     ,('vect', vect)]))]\n",
    "                                , n_jobs=-1)\n",
    "    return features\n",
    "\n",
    "def grid_vect(clf, parameters_clf, X_train, X_test, features, parameters_text=None):\n",
    "    pipeline = Pipeline([('features', features),('clf', clf)])\n",
    "    \n",
    "    # Join the parameters dictionaries together\n",
    "    parameters = dict()\n",
    "    if parameters_text:\n",
    "        parameters.update(parameters_text)\n",
    "    parameters.update(parameters_clf)\n",
    "    # Make sure you have scikit-learn version 0.19 or higher to use multiple scoring metrics\n",
    "    grid_search_model = RandomizedSearchCV(pipeline, parameters, n_jobs=-1, scoring='f1_weighted',verbose=1, cv=5,random_state=42)\n",
    "    \n",
    "    print(\"Performing grid search...\")\n",
    "    grid_search_model.fit(X_train, y_train)\n",
    "    print(\"Best CV score: %0.3f\" % grid_search_model.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search_model.best_estimator_.get_params()\n",
    "    #final_model=grid_search_model.best_estimator_\n",
    "    #final_model.fit(X,y)\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "        \n",
    "    print(\"Test score with best_estimator_: %0.3f\" % grid_search_model.best_estimator_.score(X_test, y_test))\n",
    "                 \n",
    "    return grid_search_model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid settings for the vectorizers (Count and TFIDF)\n",
    "parameters_vect = {\n",
    "    'features__pipe1__vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'features__pipe1__vect__ngram_range': ((1, 1), (1, 2),(1,3),(2,2)),\n",
    "    'features__pipe1__vect__min_df': (1,2,3,4,5),\n",
    "    'features__pipe2__vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'features__pipe2__vect__ngram_range': ((1, 1), (1, 2),(1,3),(2,2)),\n",
    "    'features__pipe2__vect__min_df': (1,2,3,4,5)\n",
    "}\n",
    "# Parameter grid settings for MultinomialNB\n",
    "parameters_mnb = {\n",
    "    'clf__alpha': (0.01,0.1, 0.25, 0.5, 0.75),\n",
    "    'clf__fit_prior' : (True,False)\n",
    "}\n",
    "# Parameter grid settings for LogisticRegression\n",
    "parameters_logreg = {\n",
    "    'clf__C': (0.25, 0.5, 1.0),\n",
    "    'clf__penalty': ('l1', 'l2'),\n",
    "    'clf__fit_intercept': (True,False),\n",
    "    'clf__class_weight':['balanced',None]\n",
    "}\n",
    "parameters_sgd = {\n",
    "    'clf__alpha': (0.00001, 0.000001),\n",
    "    'clf__penalty': ('l1', 'l2','elasticnet'),\n",
    "    'clf__max_iter': (10, 25, 50, 75, 100),\n",
    "    'clf__class_weight':['balanced',None]\n",
    "}\n",
    "parameters_svc = {'clf__kernel': ['linear', 'rbf'],\n",
    "                    'clf__gamma':[0.1,1,10],\n",
    "                    'clf__C':[0.1,1,0.001,0.0001],\n",
    "                  'clf__class_weight':['balanced',None]\n",
    "                 }\n",
    "parameters_linsvc = {'clf__loss' : ['hinge', 'squared_hinge'],\n",
    "                    'clf__penalty': ('l1', 'l2'),\n",
    "                    'clf__class_weight':['balanced',None],\n",
    "                    'clf__max_iter':[100,500],\n",
    "                    'clf__C':[0.1,1,0.001,0.0001]\n",
    "                     \n",
    "                 }\n",
    "\n",
    "parameters_gbm = {'clf__loss':['deviance','exponential'],\n",
    "         'clf__learning_rate': [0.05,0.1],\n",
    "         'clf__max_depth':[5,10,20],\n",
    "         'clf__n_estimators': [10,25,50]\n",
    "        }\n",
    "parameters_xgb = {'clf__learning_rate':[0.01,0.1],\n",
    "         'clf__n_estimators':[50,100,500],\n",
    "         'clf__max_depth':[5,10,15,20],\n",
    "        'clf__class_weight':['balanced',None],\n",
    "         'clf__objective':'reg:linear'\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_scores={}\n",
    "best_models={}\n",
    "mnb = MultinomialNB()\n",
    "logreg = LogisticRegression()\n",
    "sgd=SGDClassifier()\n",
    "svc=SVC()\n",
    "linsvc = LinearSVC()\n",
    "gbm=GradientBoostingClassifier()\n",
    "xgb_model=xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  35 tasks      | elapsed:   20.3s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   21.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.410\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.5\n",
      "\tclf__fit_prior: True\n",
      "\tfeatures__pipe1__vect__max_df: 0.5\n",
      "\tfeatures__pipe1__vect__min_df: 2\n",
      "\tfeatures__pipe1__vect__ngram_range: (1, 3)\n",
      "\tfeatures__pipe2__vect__max_df: 0.5\n",
      "\tfeatures__pipe2__vect__min_df: 1\n",
      "\tfeatures__pipe2__vect__ngram_range: (1, 3)\n",
      "Test score with best_estimator_: 0.457\n",
      "\n",
      "Accuracy score of Multinomial naive bayes algorithm -----> 40.64910127716197\n"
     ]
    }
   ],
   "source": [
    "countvect = CountVectorizer()\n",
    "tfidfvect = TfidfTransformer()\n",
    "# MultinomialNB\n",
    "model_features=gen_model_features(countvect)\n",
    "best_mnb_countvect = grid_vect(mnb, parameters_mnb, X_train, X_test,model_features, parameters_text=parameters_vect)\n",
    "prediction_mnb=best_mnb_countvect.predict(X_test)\n",
    "mnb_score=100*(f1_score(y_test,prediction_mnb,average='weighted'))\n",
    "#best_scores['mnb_countvect']=best_mnb_countvect.best_score_\n",
    "#best_models['mnb_countvect']=best_mnb_countvect\n",
    "train_pred1=best_mnb_countvect.predict(X_train)\n",
    "test_pred1=best_mnb_countvect.predict(X_test)\n",
    "print(\"\\nAccuracy score of Multinomial naive bayes algorithm -----> \" + str(mnb_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1387,)\n"
     ]
    }
   ],
   "source": [
    "#best_mnb_model=mnb.set_params(**best_mnb_countvect.best_params_)\n",
    "best_mnb_countvect.fit(X,y)\n",
    "test_predictions=best_mnb_countvect.predict(test_df)\n",
    "print(test_predictions.shape)\n",
    "prediction_df = pd.DataFrame(columns=['sentiment_class'],data=test_predictions)\n",
    "prediction_df = pd.concat([test_df_id,prediction_df],axis=1)\n",
    "prediction_df.to_csv(r'C:\\Users\\gaurav.singh.rawal\\Documents\\Gaurav\\Data Science\\Machine Learning-Predictive Analytics\\ML-Projects\\GIT - ML - Projects\\ML_Projects\\NLP - Sentiment Analysis\\HackerRank - Mothers Day\\predictions.csv',index=False,encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    5.7s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    6.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.409\n",
      "Best parameters set:\n",
      "\tclf__C: 0.5\n",
      "\tclf__class_weight: None\n",
      "\tclf__fit_intercept: False\n",
      "\tclf__penalty: 'l2'\n",
      "\tfeatures__pipe1__vect__max_df: 0.5\n",
      "\tfeatures__pipe1__vect__min_df: 4\n",
      "\tfeatures__pipe1__vect__ngram_range: (1, 3)\n",
      "\tfeatures__pipe2__vect__max_df: 1.0\n",
      "\tfeatures__pipe2__vect__min_df: 2\n",
      "\tfeatures__pipe2__vect__ngram_range: (1, 3)\n",
      "Test score with best_estimator_: 0.495\n",
      "\n",
      "Accuracy score of Log Reg algorithm -----> 38.72164788020047\n"
     ]
    }
   ],
   "source": [
    "# LogisticRegression\n",
    "best_logreg_countvect = grid_vect(logreg, parameters_logreg, X_train, X_test,model_features, parameters_text=parameters_vect)\n",
    "prediction_logreg = best_logreg_countvect.predict(X_test)\n",
    "logreg_score=100*(f1_score(y_test,prediction_logreg,average='weighted'))\n",
    "#best_scores['logreg_countvect']=best_logreg_countvect.best_score_\n",
    "#best_models['logreg_countvect']=best_logreg_countvect\n",
    "train_pred2=best_logreg_countvect.predict(X_train)\n",
    "test_pred2=best_logreg_countvect.predict(X_test)\n",
    "print(\"\\nAccuracy score of Log Reg algorithm -----> \" + str(logreg_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    5.1s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    8.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.327\n",
      "Best parameters set:\n",
      "\tclf__alpha: 1e-06\n",
      "\tclf__class_weight: None\n",
      "\tclf__max_iter: 100\n",
      "\tclf__penalty: 'l1'\n",
      "\tfeatures__pipe1__vect__max_df: 1.0\n",
      "\tfeatures__pipe1__vect__min_df: 2\n",
      "\tfeatures__pipe1__vect__ngram_range: (1, 1)\n",
      "\tfeatures__pipe2__vect__max_df: 1.0\n",
      "\tfeatures__pipe2__vect__min_df: 4\n",
      "\tfeatures__pipe2__vect__ngram_range: (1, 1)\n",
      "Test score with best_estimator_: 0.522\n",
      "\n",
      "Accuracy score of SGD algorithm -----> 35.99540091227806\n"
     ]
    }
   ],
   "source": [
    "# SGD\n",
    "best_sgd_countvect = grid_vect(sgd, parameters_sgd, X_train, X_test,model_features, parameters_text=parameters_vect)\n",
    "prediction_sgd = best_sgd_countvect.predict(X_test)\n",
    "sgd_score=100*(f1_score(y_test,prediction_sgd,average='weighted'))\n",
    "#best_scores['sgd_countvect']=best_sgd_countvect.best_score_\n",
    "#best_models['sgd_countvect']=best_sgd_countvect\n",
    "train_pred3=best_sgd_countvect.predict(X_train)\n",
    "test_pred3=best_sgd_countvect.predict(X_test)\n",
    "print(\"\\nAccuracy score of SGD algorithm -----> \" + str(sgd_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   30.2s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   45.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.364\n",
      "Best parameters set:\n",
      "\tclf__C: 1\n",
      "\tclf__class_weight: 'balanced'\n",
      "\tclf__gamma: 1\n",
      "\tclf__kernel: 'rbf'\n",
      "\tfeatures__pipe1__vect__max_df: 0.5\n",
      "\tfeatures__pipe1__vect__min_df: 4\n",
      "\tfeatures__pipe1__vect__ngram_range: (1, 3)\n",
      "\tfeatures__pipe2__vect__max_df: 1.0\n",
      "\tfeatures__pipe2__vect__min_df: 2\n",
      "\tfeatures__pipe2__vect__ngram_range: (1, 3)\n",
      "Test score with best_estimator_: 0.524\n",
      "\n",
      "Accuracy score of SVC algorithm -----> 36.30244529886354\n"
     ]
    }
   ],
   "source": [
    "# SVC\n",
    "best_svc_countvect = grid_vect(svc, parameters_svc, X_train, X_test,model_features, parameters_text=parameters_vect)\n",
    "prediction_svc = best_svc_countvect.predict(X_test)\n",
    "svc_score=100*(f1_score(y_test,prediction_svc,average='weighted'))\n",
    "#best_scores['svc_countvect']=best_svc_countvect.best_score_\n",
    "#best_models['svc_countvect']=best_svc_countvect\n",
    "train_pred4=best_svc_countvect.predict(X_train)\n",
    "test_pred4=best_svc_countvect.predict(X_test)\n",
    "print(\"\\nAccuracy score of SVC algorithm -----> \" + str(svc_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   17.0s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   22.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.368\n",
      "Best parameters set:\n",
      "\tclf__learning_rate: 0.05\n",
      "\tclf__loss: 'deviance'\n",
      "\tclf__max_depth: 10\n",
      "\tclf__n_estimators: 25\n",
      "\tfeatures__pipe1__vect__max_df: 0.75\n",
      "\tfeatures__pipe1__vect__min_df: 1\n",
      "\tfeatures__pipe1__vect__ngram_range: (2, 2)\n",
      "\tfeatures__pipe2__vect__max_df: 0.5\n",
      "\tfeatures__pipe2__vect__min_df: 4\n",
      "\tfeatures__pipe2__vect__ngram_range: (2, 2)\n",
      "Test score with best_estimator_: 0.519\n",
      "\n",
      "Accuracy score of gbc algorithm -----> 36.719117162714916\n"
     ]
    }
   ],
   "source": [
    "# GBC\n",
    "best_gbm_countvect = grid_vect(gbm, parameters_gbm, X_train, X_test,model_features, parameters_text=parameters_vect)\n",
    "prediction_gbm = best_gbm_countvect.predict(X_test)\n",
    "gbm_score=100*(f1_score(y_test,prediction_gbm,average='weighted'))\n",
    "#best_scores['gbm_countvect']=best_gbm_countvect.best_score_\n",
    "#best_models['gbm_countvect']=best_gbm_countvect\n",
    "train_pred5=best_gbm_countvect.predict(X_train)\n",
    "test_pred5=best_gbm_countvect.predict(X_test)\n",
    "print(\"\\nAccuracy score of gbc algorithm -----> \" + str(gbm_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_mnb_model=mnb.set_params(**best_mnb_countvect.best_params_)\n",
    "best_gbm_countvect.fit(X,y)\n",
    "gbm_predictions=best_gbm_countvect.predict(test_df)\n",
    "prediction_df = pd.DataFrame(columns=['sentiment_class'],data=gbm_predictions)\n",
    "prediction_df = pd.concat([test_df_id,prediction_df],axis=1)\n",
    "prediction_df.to_csv(r'C:\\Users\\gaurav.singh.rawal\\Documents\\Gaurav\\Data Science\\Machine Learning-Predictive Analytics\\ML-Projects\\GIT - ML - Projects\\ML_Projects\\NLP - Sentiment Analysis\\HackerRank - Mothers Day\\predictions.csv',index=False,encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    3.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 0.364\n",
      "Best parameters set:\n",
      "\tclf__C: 0.0001\n",
      "\tclf__class_weight: 'balanced'\n",
      "\tclf__loss: 'hinge'\n",
      "\tclf__max_iter: 500\n",
      "\tclf__penalty: 'l2'\n",
      "\tfeatures__pipe__vect__max_df: 0.75\n",
      "\tfeatures__pipe__vect__min_df: 4\n",
      "\tfeatures__pipe__vect__ngram_range: (1, 1)\n",
      "Test score with best_estimator_: 0.513\n",
      "\n",
      "Accuracy score of gbc algorithm -----> 35.93260213797531\n"
     ]
    }
   ],
   "source": [
    "# GBC\n",
    "best_linsvc_countvect = grid_vect(linsvc, parameters_linsvc, X_train, X_test,model_features, parameters_text=parameters_vect)\n",
    "prediction_linsvc = best_linsvc_countvect.predict(X_test)\n",
    "linsvc_score=100*(f1_score(y_test,prediction_linsvc,average='weighted'))\n",
    "#best_scores['gbm_countvect']=best_gbm_countvect.best_score_\n",
    "#best_models['gbm_countvect']=best_gbm_countvect\n",
    "train_pred5=best_gbm_countvect.predict(X_train)\n",
    "test_pred5=best_gbm_countvect.predict(X_test)\n",
    "print(\"\\nAccuracy score of gbc algorithm -----> \" + str(linsvc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_mnb_model=mnb.set_params(**best_mnb_countvect.best_params_)\n",
    "best_linsvc_countvect.fit(X,y)\n",
    "linsvc_predictions=best_linsvc_countvect.predict(test_df)\n",
    "prediction_df = pd.DataFrame(columns=['sentiment_class'],data=linsvc_predictions)\n",
    "prediction_df = pd.concat([test_df_id,prediction_df],axis=1)\n",
    "prediction_df.to_csv(r'C:\\Users\\gaurav.singh.rawal\\Documents\\Gaurav\\Data Science\\Machine Learning-Predictive Analytics\\ML-Projects\\GIT - ML - Projects\\ML_Projects\\NLP - Sentiment Analysis\\HackerRank - Mothers Day\\predictions.csv',index=False,encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    3.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    6.0s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    9.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    4.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   14.1s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   20.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   13.0s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   35.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy score of Voting Classifier algorithm -----> 38.736922949489525\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "countvect_classifier= VotingClassifier(estimators=[('mnb', best_models['mnb_countvect']), ('lr', best_models['logreg_countvect']),\n",
    "                         ('sgd', best_models['sgd_countvect']), ('svc', best_models['svc_countvect']),\n",
    "                         ('gbm', best_models['gbm_countvect'])],voting='hard')\n",
    "countvect_classifier.fit(X_train,y_train)\n",
    "countvect_classifier_predictions=countvect_classifier.predict(X_test)\n",
    "countvect_classifier_predictions_score=100*(f1_score(y_test,countvect_classifier_predictions,average='weighted'))\n",
    "print(\"\\nAccuracy score of Voting Classifier algorithm -----> \" + str(countvect_classifier_predictions_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([pd.DataFrame(train_pred1),pd.DataFrame(train_pred2),\n",
    "                      pd.DataFrame(train_pred3),pd.DataFrame(train_pred4),pd.DataFrame(train_pred5)], axis=1)\n",
    "df_test = pd.concat([pd.DataFrame(test_pred1),pd.DataFrame(test_pred2),pd.DataFrame(test_pred3)\n",
    "                     ,pd.DataFrame(test_pred4),pd.DataFrame(test_pred5)], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns=['mnb','lr','sgd','svc','gbm']\n",
    "df_test.columns=['mnb','lr','sgd','svc','gbm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2588, 5)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy score of XGB stacking algorithm -----> 40.41467204614164\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "def hyperparameter_tuner(model,X_train,y_train,hp_list):\n",
    "    \n",
    "    hp_perf=[]\n",
    "    hp_model=RandomizedSearchCV(model,param_distributions=hp_list,n_iter=10,n_jobs=-1, scoring='f1_weighted',verbose=1, cv=5)\n",
    "    hp_model.fit(X_train,y_train)\n",
    "    best_hp_model=hp_model.best_estimator_\n",
    "    best_param=hp_model.best_params_\n",
    "    best_score=hp_model.best_score_  \n",
    "   \n",
    "    return best_hp_model,best_param,best_score\n",
    "\n",
    "xgb=XGBClassifier({\n",
    "    'objective': 'multi:softprob',\n",
    "    'eta': 0.1,\n",
    "    'max_depth': 3,\n",
    "    'silent' :1,\n",
    "    'num_class' : 3,\n",
    "    'eval_metric' : \"mlogloss\",\n",
    "    'min_child_weight': 1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.3,\n",
    "    'seed':17,\n",
    "    'num_rounds':2000,\n",
    "})\n",
    "xgb.fit(df_train,y_train)\n",
    "\n",
    "#best_model,best_params,best_score=hyperparameter_tuner(xgb,df_train,y_train,parameters_xgb)\n",
    "\n",
    "prediction_xgb = xgb.predict(df_test)\n",
    "xgb_score=100*(f1_score(y_test,prediction_xgb,average='weighted'))\n",
    "print(\"\\nAccuracy score of XGB stacking algorithm -----> \" + str(xgb_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy score of Random Forest Bagging algorithm -----> 40.13882393849355\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = BaggingClassifier(RandomForestClassifier(max_depth=25,min_samples_leaf=2,n_estimators=25))\n",
    "model.fit(df_train,y_train)\n",
    "bagging_pred = model.predict(df_test)\n",
    "bagging_score=100*(f1_score(y_test,bagging_pred,average='weighted'))\n",
    "print(\"\\nAccuracy score of Random Forest Bagging algorithm -----> \" + str(bagging_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
