{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\gaurav.singh.rawal\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "from time import time\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import emoji\n",
    "from pprint import pprint\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.set(font_scale=1.3)\n",
    "from sklearn import model_selection\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer,TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split,RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import classification_report,f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, Lasso\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "import xgboost as xgb\n",
    "import gensim\n",
    "from contractions import CONTRACTION_MAP\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(37)\n",
    "color = sns.color_palette()\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv(r\"C:\\Users\\gaurav.singh.rawal\\Documents\\Gaurav\\Data Science\\Machine Learning-Predictive Analytics\\ML-Projects\\GIT - ML - Projects\\ML_Projects\\NLP - Sentiment Analysis\\HackerRank - Mothers Day\\train.csv\")\n",
    "test_data=pd.read_csv(r\"C:\\Users\\gaurav.singh.rawal\\Documents\\Gaurav\\Data Science\\Machine Learning-Predictive Analytics\\ML-Projects\\GIT - ML - Projects\\ML_Projects\\NLP - Sentiment Analysis\\HackerRank - Mothers Day\\test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1304, 14)"
      ]
     },
     "execution_count": 577,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_ec53c6c8_a89a_11ea_aba5_d43b045a62ecrow0_col0 {\n",
       "            background-color:  #9e9ac8;\n",
       "            color:  #000000;\n",
       "        }    #T_ec53c6c8_a89a_11ea_aba5_d43b045a62ecrow0_col1 {\n",
       "            background-color:  #3f007d;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_ec53c6c8_a89a_11ea_aba5_d43b045a62ecrow1_col0 {\n",
       "            background-color:  #fcfbfd;\n",
       "            color:  #000000;\n",
       "        }    #T_ec53c6c8_a89a_11ea_aba5_d43b045a62ecrow1_col1 {\n",
       "            background-color:  #fcfbfd;\n",
       "            color:  #000000;\n",
       "        }    #T_ec53c6c8_a89a_11ea_aba5_d43b045a62ecrow2_col0 {\n",
       "            background-color:  #3f007d;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_ec53c6c8_a89a_11ea_aba5_d43b045a62ecrow2_col1 {\n",
       "            background-color:  #fcfbfd;\n",
       "            color:  #000000;\n",
       "        }</style><table id=\"T_ec53c6c8_a89a_11ea_aba5_d43b045a62ec\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >sentiment_class</th>        <th class=\"col_heading level0 col1\" >original_text</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_ec53c6c8_a89a_11ea_aba5_d43b045a62eclevel0_row0\" class=\"row_heading level0 row0\" >1</th>\n",
       "                        <td id=\"T_ec53c6c8_a89a_11ea_aba5_d43b045a62ecrow0_col0\" class=\"data row0 col0\" >0</td>\n",
       "                        <td id=\"T_ec53c6c8_a89a_11ea_aba5_d43b045a62ecrow0_col1\" class=\"data row0 col1\" >1701</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_ec53c6c8_a89a_11ea_aba5_d43b045a62eclevel0_row1\" class=\"row_heading level0 row1\" >0</th>\n",
       "                        <td id=\"T_ec53c6c8_a89a_11ea_aba5_d43b045a62ecrow1_col0\" class=\"data row1 col0\" >-1</td>\n",
       "                        <td id=\"T_ec53c6c8_a89a_11ea_aba5_d43b045a62ecrow1_col1\" class=\"data row1 col1\" >769</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_ec53c6c8_a89a_11ea_aba5_d43b045a62eclevel0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_ec53c6c8_a89a_11ea_aba5_d43b045a62ecrow2_col0\" class=\"data row2 col0\" >1</td>\n",
       "                        <td id=\"T_ec53c6c8_a89a_11ea_aba5_d43b045a62ecrow2_col1\" class=\"data row2 col1\" >765</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1cc063f2908>"
      ]
     },
     "execution_count": 530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = train_data.groupby('sentiment_class').count()['original_text'].reset_index().sort_values(by='original_text',ascending=False)\n",
    "temp.style.background_gradient(cmap='Purples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1cc06e29e48>"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='sentiment_class',data=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3235 entries, 0 to 3234\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   id               3235 non-null   float64\n",
      " 1   original_text    3235 non-null   object \n",
      " 2   lang             3231 non-null   object \n",
      " 3   retweet_count    3231 non-null   object \n",
      " 4   original_author  3235 non-null   object \n",
      " 5   sentiment_class  3235 non-null   int64  \n",
      "dtypes: float64(1), int64(1), object(4)\n",
      "memory usage: 151.8+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['retweet_count']=train_data['retweet_count'].astype(str)\n",
    "test_data['retweet_count']=test_data['retweet_count'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"retweet_count\"]= train_data[\"retweet_count\"].str.replace(r\"[a-zA-Z$@#&*!''-/]\",'')\n",
    "train_data[\"retweet_count\"]= train_data[\"retweet_count\"].replace(r'^\\s*$', np.nan, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"retweet_count\"]= test_data[\"retweet_count\"].str.replace(r\"[a-zA-Z$@#&*!''-/]\",'')\n",
    "test_data[\"retweet_count\"]= test_data[\"retweet_count\"].replace(r'^\\s*$', np.nan, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['retweet_count']=pd.to_numeric(train_data['retweet_count'])\n",
    "test_data['retweet_count']=pd.to_numeric(test_data['retweet_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.loc[train_data[\"retweet_count\"]<0,'retweet_count']=0\n",
    "train_data.loc[train_data[\"retweet_count\"].isna(),'retweet_count']=0\n",
    "test_data.loc[test_data[\"retweet_count\"]<0,'retweet_count']=0\n",
    "test_data.loc[test_data[\"retweet_count\"].isna(),'retweet_count']=0\n",
    "train_data.loc[train_data[\"retweet_count\"]>100,'retweet_count']=100\n",
    "test_data.loc[test_data[\"retweet_count\"]>100,'retweet_count']=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"retweet_count\"]=train_data[\"retweet_count\"].astype(int)\n",
    "test_data[\"retweet_count\"]=test_data[\"retweet_count\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['original_text']=train_data['original_text'].apply(expand_contractions)\n",
    "test_data['original_text']=test_data['original_text'].apply(expand_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>original_text</th>\n",
       "      <th>lang</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>original_author</th>\n",
       "      <th>sentiment_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.245025e+18</td>\n",
       "      <td>Happy #MothersDay to all you amazing mothers out there! I know it is hard not being able to see your mothers today but it is on all of us to do what we can to protect the most vulnerable members of our society. #BeatCoronaVirus pic.twitter.com/va4nFjFQ5B</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>BeenXXPired</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.245759e+18</td>\n",
       "      <td>Happy Mothers Day Mum - I am sorry I cannot be there to bring you Mothers day flowers &amp; a cwtch - honestly at this point I would walk on hot coals to be able to. But I will be there with bells on as soon as I can be. Love you lots xxx (p.s we need more photos!) https:// photos.app.goo.gl/M3vXBLrsCzD4TE bY7 …</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>FestiveFeeling</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.246087e+18</td>\n",
       "      <td>Happy mothers day To all This doing a mothers days work. Today been quiet but Had time to reflect. Dog walk, finish a jigsaw do the garden, learn few more guitar chords, drunk some strawberry gin and tonic and watch Lee evens on DVD. My favourite place to visit. #isolate pic.twitter.com/GZ0xVvF6f9</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>KrisAllenSak</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.244803e+18</td>\n",
       "      <td>Happy mothers day to this beautiful woman...royalty soothes you mummy jeremy and emerald and more #PrayForRoksie #UltimateLoveNG pic.twitter.com/oeetI22Pvv</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>Queenuchee</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.244876e+18</td>\n",
       "      <td>Remembering the 3 most amazing ladies who made me who I am! My late grandmother iris, mum carol and great grandmother Ethel. Missed but never forgotten! Happy mothers day to all those great mums out there! Love sent to all xxxx pic.twitter.com/xZZZdEybjE</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>brittan17446794</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  \\\n",
       "0  1.245025e+18   \n",
       "1  1.245759e+18   \n",
       "2  1.246087e+18   \n",
       "3  1.244803e+18   \n",
       "4  1.244876e+18   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                           original_text  \\\n",
       "0  Happy #MothersDay to all you amazing mothers out there! I know it is hard not being able to see your mothers today but it is on all of us to do what we can to protect the most vulnerable members of our society. #BeatCoronaVirus pic.twitter.com/va4nFjFQ5B                                                          \n",
       "1  Happy Mothers Day Mum - I am sorry I cannot be there to bring you Mothers day flowers & a cwtch - honestly at this point I would walk on hot coals to be able to. But I will be there with bells on as soon as I can be. Love you lots xxx (p.s we need more photos!) https:// photos.app.goo.gl/M3vXBLrsCzD4TE bY7 …   \n",
       "2  Happy mothers day To all This doing a mothers days work. Today been quiet but Had time to reflect. Dog walk, finish a jigsaw do the garden, learn few more guitar chords, drunk some strawberry gin and tonic and watch Lee evens on DVD. My favourite place to visit. #isolate pic.twitter.com/GZ0xVvF6f9              \n",
       "3  Happy mothers day to this beautiful woman...royalty soothes you mummy jeremy and emerald and more #PrayForRoksie #UltimateLoveNG pic.twitter.com/oeetI22Pvv                                                                                                                                                             \n",
       "4  Remembering the 3 most amazing ladies who made me who I am! My late grandmother iris, mum carol and great grandmother Ethel. Missed but never forgotten! Happy mothers day to all those great mums out there! Love sent to all xxxx pic.twitter.com/xZZZdEybjE                                                          \n",
       "\n",
       "  lang  retweet_count  original_author  sentiment_class  \n",
       "0  en   0              BeenXXPired      0                \n",
       "1  en   1              FestiveFeeling   0                \n",
       "2  en   0              KrisAllenSak    -1                \n",
       "3  en   0              Queenuchee       0                \n",
       "4  en   0              brittan17446794 -1                "
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of words in the original_text ##\n",
    "train_data[\"num_words\"] = train_data[\"original_text\"].apply(lambda x: len(str(x).split()))\n",
    "test_data[\"num_words\"] = test_data[\"original_text\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "## Number of unique words in the original_text ##\n",
    "train_data[\"num_unique_words\"] = train_data[\"original_text\"].apply(lambda x: len(set(str(x).split())))\n",
    "test_data[\"num_unique_words\"] = test_data[\"original_text\"].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "## Number of characters in the original_text ##\n",
    "train_data[\"num_chars\"] = train_data[\"original_text\"].apply(lambda x: len(str(x)))\n",
    "test_data[\"num_chars\"] = test_data[\"original_text\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "## Number of stopwords in the original_text ##\n",
    "train_data[\"num_stopwords\"] = train_data[\"original_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "test_data[\"num_stopwords\"] = test_data[\"original_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "\n",
    "## Number of punctuations in the original_text ##\n",
    "train_data[\"num_punctuations\"] =train_data['original_text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "test_data[\"num_punctuations\"] =test_data['original_text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "\n",
    "## Number of title case words in the original_text ##\n",
    "train_data[\"num_words_upper\"] = train_data[\"original_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "test_data[\"num_words_upper\"] = test_data[\"original_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "\n",
    "## Number of title case words in the original_text ##\n",
    "train_data[\"num_words_title\"] = train_data[\"original_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "test_data[\"num_words_title\"] = test_data[\"original_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "\n",
    "## Average length of the words in the original_text ##\n",
    "train_data[\"mean_word_len\"] = train_data[\"original_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "test_data[\"mean_word_len\"] = test_data[\"original_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"cols_to_drop = ['id','original_text','sentiment_class','lang','original_author']\\ntrain_y = train_data.sentiment_class\\ntrain_X = train_data.drop(cols_to_drop, axis=1)\\ntest_X = test_data.drop(['id','original_text','lang','original_author'], axis=1)\""
      ]
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''cols_to_drop = ['id','original_text','sentiment_class','lang','original_author']\n",
    "train_y = train_data.sentiment_class\n",
    "train_X = train_data.drop(cols_to_drop, axis=1)\n",
    "test_X = test_data.drop(['id','original_text','lang','original_author'], axis=1)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def runXGB(train_X, train_y, test_X, test_y=None, test_X2=None, seed_val=0, child=1, colsample=0.3):\\n    param = {}\\n    param[\\'objective\\'] = \\'multi:softprob\\'\\n    param[\\'eta\\'] = 0.1\\n    param[\\'max_depth\\'] = 3\\n    param[\\'num_class\\'] = 3\\n    param[\\'eval_metric\\'] = \"mlogloss\"\\n    param[\\'min_child_weight\\'] = child\\n    param[\\'subsample\\'] = 0.8\\n    param[\\'colsample_bytree\\'] = colsample\\n    param[\\'seed\\'] = seed_val\\n    num_rounds = 2000\\n\\n    plst = list(param.items())\\n    xgtrain = xgb.DMatrix(train_X, label=train_y)\\n\\n    if test_y is not None:\\n        xgtest = xgb.DMatrix(test_X, label=test_y)\\n        watchlist = [ (xgtrain,\\'train\\'), (xgtest, \\'test\\') ]\\n        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=50, verbose_eval=20)\\n    else:\\n        xgtest = xgb.DMatrix(test_X)\\n        model = xgb.train(plst, xgtrain, num_rounds)\\n\\n    pred_test_y = model.predict(xgtest, ntree_limit = model.best_ntree_limit)\\n    if test_X2 is not None:\\n        xgtest2 = xgb.DMatrix(test_X2)\\n        pred_test_y2 = model.predict(xgtest2, ntree_limit = model.best_ntree_limit)\\n    return pred_test_y, pred_test_y2, model'"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def runXGB(train_X, train_y, test_X, test_y=None, test_X2=None, seed_val=0, child=1, colsample=0.3):\n",
    "    param = {}\n",
    "    param['objective'] = 'multi:softprob'\n",
    "    param['eta'] = 0.1\n",
    "    param['max_depth'] = 3\n",
    "    param['num_class'] = 3\n",
    "    param['eval_metric'] = \"mlogloss\"\n",
    "    param['min_child_weight'] = child\n",
    "    param['subsample'] = 0.8\n",
    "    param['colsample_bytree'] = colsample\n",
    "    param['seed'] = seed_val\n",
    "    num_rounds = 2000\n",
    "\n",
    "    plst = list(param.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=50, verbose_eval=20)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(xgtest, ntree_limit = model.best_ntree_limit)\n",
    "    if test_X2 is not None:\n",
    "        xgtest2 = xgb.DMatrix(test_X2)\n",
    "        pred_test_y2 = model.predict(xgtest2, ntree_limit = model.best_ntree_limit)\n",
    "    return pred_test_y, pred_test_y2, model'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\\ncv_scores = []\\npred_full_test = 0\\npred_train = np.zeros([train_data.shape[0], 3])\\nfor dev_index, val_index in kf.split(train_X):\\n    dev_X, val_X = train_X.loc[dev_index], train_X.loc[val_index]\\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\\n    pred_val_y, pred_test_y, model = runXGB(dev_X, dev_y, val_X, val_y, test_X, seed_val=0)\\n    pred_full_test = pred_full_test + pred_test_y\\n    pred_train[val_index,:] = pred_val_y\\n    cv_scores.append(100*(f1_score(val_y,pred_val_y,average=\\'weighted\\')))\\n    break\\nprint(\"cv scores : \", cv_scores)'"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train_data.shape[0], 3])\n",
    "for dev_index, val_index in kf.split(train_X):\n",
    "    dev_X, val_X = train_X.loc[dev_index], train_X.loc[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    pred_val_y, pred_test_y, model = runXGB(dev_X, dev_y, val_X, val_y, test_X, seed_val=0)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y\n",
    "    cv_scores.append(100*(f1_score(val_y,pred_val_y,average='weighted')))\n",
    "    break\n",
    "print(\"cv scores : \", cv_scores)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanText(BaseEstimator, TransformerMixin):\n",
    "    def remove_mentions(self, input_text):\n",
    "        return re.sub(r'@\\w+', '', input_text)\n",
    "    \n",
    "    def remove_urls(self, input_text):\n",
    "        return re.sub(r'http.?://[^\\s]+[\\s]?', '', input_text)\n",
    "    \n",
    "    def emoji_oneword(self, input_text):\n",
    "        # By compressing the underscore, the emoji is kept as one word\n",
    "        return input_text.replace('_','')\n",
    "    \n",
    "    def remove_punctuation(self, input_text):\n",
    "        # Make translation table\n",
    "        punct = string.punctuation\n",
    "        trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space\n",
    "        return input_text.translate(trantab)\n",
    "    def remove_digits(self, input_text):\n",
    "        return re.sub('\\d+', '', input_text)\n",
    "    \n",
    "    def to_lower(self, input_text):\n",
    "        return input_text.lower()\n",
    "    \n",
    "    def remove_stopwords(self, input_text):\n",
    "        stopwords_list = stopwords.words('english')\n",
    "        # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "        whitelist = [\"n't\", \"not\", \"no\",\"never\"]\n",
    "        words = input_text.split() \n",
    "        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
    "        return \" \".join(clean_words) \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    def transform(self, X, **transform_params):\n",
    "        clean_X = X.apply(self.remove_mentions).apply(self.remove_urls).apply(self.emoji_oneword).apply(self.remove_punctuation).apply(self.remove_digits).apply(self.to_lower).apply(self.remove_stopwords)\n",
    "        return clean_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = CleanText()\n",
    "train_data['original_text'] = ct.fit_transform(train_data.original_text)\n",
    "test_data['original_text']=ct.fit_transform(test_data.original_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['original_text'] = train_data['original_text'].astype(str)\n",
    "test_data['original_text'] = test_data['original_text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "def clean_text(raw_text):\n",
    "    return \" \".join(w for w in nltk.wordpunct_tokenize(raw_text) \\\n",
    "     if w.lower() in words or not w.isalpha())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['original_text'] = train_data['original_text'].apply(clean_text)\n",
    "test_data['original_text'] = test_data['original_text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from langdetect import detect\\ndef clean_lang(df):\\n    #df.loc[((detect(df[\\'original_text\\'])==\\'en\\') and df[\\'lang\\']!=\\'en\\'),\\'lang\\'] = \\'en\\'\\n    for row_df in df:\\n        if ((detect(row_df[1])==\\'en\\') and row_df[2]!=\\'en\\'):\\n            row_df[2]=\"\"+\\'en\\'\\n            \\n    return df '"
      ]
     },
     "execution_count": 551,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from langdetect import detect\n",
    "def clean_lang(df):\n",
    "    #df.loc[((detect(df['original_text'])=='en') and df['lang']!='en'),'lang'] = 'en'\n",
    "    for row_df in df:\n",
    "        if ((detect(row_df[1])=='en') and row_df[2]!='en'):\n",
    "            row_df[2]=\"\"+'en'\n",
    "            \n",
    "    return df '''   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train_data[train_data['lang']=='en']\n",
    "test_data=test_data[test_data['lang']=='en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "def stem_text(input_text):\n",
    "    words = input_text.split()\n",
    "    return \" \".join([porter.stem(word) for word in words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['original_text'] = train_data['original_text'].apply(stem_text)\n",
    "test_data['original_text'] = test_data['original_text'].apply(stem_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(input_text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(input_text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['original_text'] = train_data['original_text'].apply(lemmatize_text).astype(str)\n",
    "test_data['original_text'] = test_data['original_text'].apply(lemmatize_text).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text_df):\n",
    "    text_df[\"clean_text\"]= text_df[\"original_text\"].str.replace(\"mum\", \"mother\", case = False)\n",
    "    text_df[\"clean_text\"]= text_df[\"clean_text\"].str.replace(\"mom\", \"mother\", case = False)\n",
    "    text_df[\"clean_text\"]= text_df[\"clean_text\"].str.replace(\"mothersday\",\"mother day\") \n",
    "    text_df[\"clean_text\"]= text_df[\"clean_text\"].str.replace(\"httpswww\",\"\") \n",
    "    text_df[\"clean_text\"]= text_df[\"clean_text\"].str.replace(\"http\",\"\") \n",
    "    text_df[\"clean_text\"]= text_df[\"clean_text\"].str.replace(\"u\",\"\") \n",
    "    return text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=clean_text(train_data)\n",
    "test_df=clean_text(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1304, 14)"
      ]
     },
     "execution_count": 559,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnExtractor(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "    def transform(self, X, **transform_params):\n",
    "        return X[self.cols]\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=train_df.reset_index()\n",
    "test_df=test_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=1304, step=1)\n"
     ]
    }
   ],
   "source": [
    "print(test_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fit transform the tfidf vectorizer ###\n",
    "tfidf = TfidfVectorizer(norm=None,token_pattern=r\"(?u)\\b\\w+\\b\", stop_words=None, sublinear_tf=True, max_df=0.5, analyzer='word', \n",
    "           ngram_range=(1,2))\n",
    "tfidf_score = tfidf.fit_transform(train_df['clean_text'])\n",
    "train_tfidf = pd.DataFrame(tfidf_score.toarray(),columns=tfidf.get_feature_names())\n",
    "train_df = pd.concat([train_df,train_tfidf], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2994, 24071)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>original_text</th>\n",
       "      <th>lang</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>original_author</th>\n",
       "      <th>sentiment_class</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>...</th>\n",
       "      <th>yov love</th>\n",
       "      <th>yr</th>\n",
       "      <th>yr cri</th>\n",
       "      <th>yr old</th>\n",
       "      <th>zero</th>\n",
       "      <th>zero wonder</th>\n",
       "      <th>zodiac</th>\n",
       "      <th>zodiac sign</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zoom pic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.245025e+18</td>\n",
       "      <td>['happi', 'amaz', 'know', 'hard', 'not', 'abl', 'see', 'today', 'u', 'protect', 'vulner', 'societi', 'pic', 'twitter']</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>BeenXXPired</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>37</td>\n",
       "      <td>254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.245759e+18</td>\n",
       "      <td>['happi', 'day', 'mum', 'sorri', 'cannot', 'bring', 'day', 'honestli', 'point', 'would', 'walk', 'hot', 'abl', 'soon', 'love', 'lot', 'need', 'goo']</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>FestiveFeeling</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>50</td>\n",
       "      <td>309</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.246087e+18</td>\n",
       "      <td>['happi', 'day', 'day', 'work', 'today', 'quiet', 'time', 'reflect', 'dog', 'walk', 'finish', 'garden', 'learn', 'guitar', 'drunk', 'strawberri', 'gin', 'tonic', 'watch', 'lee', 'even', 'place', 'visit', 'isol', 'pic', 'twitter']</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>KrisAllenSak</td>\n",
       "      <td>-1</td>\n",
       "      <td>51</td>\n",
       "      <td>47</td>\n",
       "      <td>298</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.244803e+18</td>\n",
       "      <td>['happi', 'day', 'beauti', 'woman', 'royalti', 'mummi', 'emerald', 'pic', 'twitter']</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>Queenuchee</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>155</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.244876e+18</td>\n",
       "      <td>['amaz', 'ladi', 'made', 'late', 'grandmoth', 'iri', 'mum', 'carol', 'great', 'grandmoth', 'ethel', 'never', 'forgotten', 'happi', 'day', 'great', 'love', 'sent', 'pic', 'twitter']</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>brittan17446794</td>\n",
       "      <td>-1</td>\n",
       "      <td>42</td>\n",
       "      <td>37</td>\n",
       "      <td>254</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24071 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index            id  \\\n",
       "0  0      1.245025e+18   \n",
       "1  1      1.245759e+18   \n",
       "2  2      1.246087e+18   \n",
       "3  3      1.244803e+18   \n",
       "4  4      1.244876e+18   \n",
       "\n",
       "                                                                                                                                                                                                                           original_text  \\\n",
       "0  ['happi', 'amaz', 'know', 'hard', 'not', 'abl', 'see', 'today', 'u', 'protect', 'vulner', 'societi', 'pic', 'twitter']                                                                                                                  \n",
       "1  ['happi', 'day', 'mum', 'sorri', 'cannot', 'bring', 'day', 'honestli', 'point', 'would', 'walk', 'hot', 'abl', 'soon', 'love', 'lot', 'need', 'goo']                                                                                    \n",
       "2  ['happi', 'day', 'day', 'work', 'today', 'quiet', 'time', 'reflect', 'dog', 'walk', 'finish', 'garden', 'learn', 'guitar', 'drunk', 'strawberri', 'gin', 'tonic', 'watch', 'lee', 'even', 'place', 'visit', 'isol', 'pic', 'twitter']   \n",
       "3  ['happi', 'day', 'beauti', 'woman', 'royalti', 'mummi', 'emerald', 'pic', 'twitter']                                                                                                                                                    \n",
       "4  ['amaz', 'ladi', 'made', 'late', 'grandmoth', 'iri', 'mum', 'carol', 'great', 'grandmoth', 'ethel', 'never', 'forgotten', 'happi', 'day', 'great', 'love', 'sent', 'pic', 'twitter']                                                    \n",
       "\n",
       "  lang  retweet_count  original_author  sentiment_class  num_words  \\\n",
       "0  en   0              BeenXXPired      0                45          \n",
       "1  en   1              FestiveFeeling   0                63          \n",
       "2  en   0              KrisAllenSak    -1                51          \n",
       "3  en   0              Queenuchee       0                18          \n",
       "4  en   0              brittan17446794 -1                42          \n",
       "\n",
       "   num_unique_words  num_chars  ...  yov love   yr  yr cri  yr old  zero  \\\n",
       "0  37                254        ...  0.0       0.0  0.0     0.0     0.0    \n",
       "1  50                309        ...  0.0       0.0  0.0     0.0     0.0    \n",
       "2  47                298        ...  0.0       0.0  0.0     0.0     0.0    \n",
       "3  17                155        ...  0.0       0.0  0.0     0.0     0.0    \n",
       "4  37                254        ...  0.0       0.0  0.0     0.0     0.0    \n",
       "\n",
       "  zero wonder  zodiac  zodiac sign  zoom  zoom pic  \n",
       "0  0.0         0.0     0.0          0.0   0.0       \n",
       "1  0.0         0.0     0.0          0.0   0.0       \n",
       "2  0.0         0.0     0.0          0.0   0.0       \n",
       "3  0.0         0.0     0.0          0.0   0.0       \n",
       "4  0.0         0.0     0.0          0.0   0.0       \n",
       "\n",
       "[5 rows x 24071 columns]"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tfidf_score = tfidf.transform(test_df['clean_text'])\n",
    "test_tfidf = pd.DataFrame(test_tfidf_score.toarray(),columns=tfidf.get_feature_names())\n",
    "test_df = pd.concat([test_df,test_tfidf], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=1304, step=1)\n"
     ]
    }
   ],
   "source": [
    "print(test_tfidf.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>original_text</th>\n",
       "      <th>lang</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>original_author</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>...</th>\n",
       "      <th>yov love</th>\n",
       "      <th>yr</th>\n",
       "      <th>yr cri</th>\n",
       "      <th>yr old</th>\n",
       "      <th>zero</th>\n",
       "      <th>zero wonder</th>\n",
       "      <th>zodiac</th>\n",
       "      <th>zodiac sign</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zoom pic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.246628e+18</td>\n",
       "      <td>['yeah', 'potato', 'old', 'mean', 'threw', 'bag', 'toilet', 'happi', 'day', 'made', 'breakfast', 'time', 'thought', 'cool', 'draw', 'nake', 'ladi', 'dachshund', 'overhead', 'projector', 'psycholog', 'class']</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>LToddWood</td>\n",
       "      <td>56</td>\n",
       "      <td>46</td>\n",
       "      <td>275</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.245898e+18</td>\n",
       "      <td>['happi', 'day', 'step', 'cover', 'parent', 'twitter']</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>iiarushii</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>192</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.244717e+18</td>\n",
       "      <td>['love', 'peopl', 'howev', 'awok', 'saw', 'woke', 'immedi', 'happi', 'day', 'across', 'way', 'god', 'bless', 'love', 'share', 'make']</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>andreaanderegg</td>\n",
       "      <td>44</td>\n",
       "      <td>35</td>\n",
       "      <td>237</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.245730e+18</td>\n",
       "      <td>['happi', 'st', 'birthday', 'happi', 'mother', '’', 's', 'day', 'mum', 'can', '’', 't', 'see', 'today', 'keep', 'safe', 'shame', 'mani', 'advic', 'pic', 'twitter']</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>TheBookTweeters</td>\n",
       "      <td>30</td>\n",
       "      <td>28</td>\n",
       "      <td>203</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.244636e+18</td>\n",
       "      <td>['happi', 'day', 'wonder', 'world', 'live', 'present', 'alway', 'reli', 'mother', 'rock', 'whatev', 'life']</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>andreaanderegg</td>\n",
       "      <td>39</td>\n",
       "      <td>34</td>\n",
       "      <td>200</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>1.244048e+18</td>\n",
       "      <td>['happi', 'day', 'thank', 'support', 'mother', 'daughter', 'spend', 'rest', 'unburden', 'made', 'everi', 'day', 'feel', 'like', 'could', 'last', 'lent', 'faith', 'pic', 'twitter']</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>iamxander5</td>\n",
       "      <td>41</td>\n",
       "      <td>39</td>\n",
       "      <td>271</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1.243919e+18</td>\n",
       "      <td>['happi', 'ruth', 'happi', 'day', 'love', 'peac', 'beauti', 'day', 'beauti', 'friend', 'stay', 'super', 'safe', 'love', 'alway', 'pic', 'twitter']</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>HoneyDewListApp</td>\n",
       "      <td>25</td>\n",
       "      <td>23</td>\n",
       "      <td>165</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>1.246245e+18</td>\n",
       "      <td>['happi', 'day', 'amaz', 'care', 'love', 'much', 'give', 'best', 'thank', 'god', 'twitter', 'st']</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>LToddWood</td>\n",
       "      <td>35</td>\n",
       "      <td>32</td>\n",
       "      <td>211</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>1.246139e+18</td>\n",
       "      <td>['happi', 'day', 'mum', 'mani', 'could', 'not', 'visit', 'phone', 'good', 'day', 'better', 'time', 'pic', 'twitter']</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>ShowBuzzDaily</td>\n",
       "      <td>33</td>\n",
       "      <td>31</td>\n",
       "      <td>180</td>\n",
       "      <td>17</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>1.244255e+18</td>\n",
       "      <td>['happi', 'mother', 'afternoon', 'bed', 'new', 'book', 'later', 'bliss']</td>\n",
       "      <td>en</td>\n",
       "      <td>2</td>\n",
       "      <td>AngelsFreak7</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>210</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>1.244233e+18</td>\n",
       "      <td>['happi', 'day', 'step', 'foster', 'love', 'mum', 'say', 'love', 'blind', 'etern', 'grate', 'not', 'leav', 'everi', 'next', 'beyond', 'love', 'pic', 'twitter']</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>merc8156</td>\n",
       "      <td>59</td>\n",
       "      <td>50</td>\n",
       "      <td>308</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>1.244249e+18</td>\n",
       "      <td>['happi', 'day', 'anoth', 'mother', 'still', 'mother', 'spoken', 'pic', 'twitter']</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>Chandan79311642</td>\n",
       "      <td>25</td>\n",
       "      <td>21</td>\n",
       "      <td>156</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>1.245385e+18</td>\n",
       "      <td>['happi', 'day', 'fantast', 'garden', 'plant', 'blue', 'daisi', 'bush', 'nativ', 'south', 'pretti', 'littl', 'delight', 'appear', 'march', 'may', 'pic', 'twitter']</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>nikkus15</td>\n",
       "      <td>44</td>\n",
       "      <td>42</td>\n",
       "      <td>301</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>1.245410e+18</td>\n",
       "      <td>['ago', 'terribl', 'cancer', 'day', 'hard', 'year', 'took', 'nurs', 'home', 'dad', 'happi', 'whole', 'build']</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>laroyal06</td>\n",
       "      <td>57</td>\n",
       "      <td>49</td>\n",
       "      <td>280</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>1.245083e+18</td>\n",
       "      <td>['happi', 'day', 'even', 'no', 'remain', 'heart', 'forev', 'love', 'remaind', 'life', 'rest', 'pic', 'twitter']</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>JewelryName</td>\n",
       "      <td>29</td>\n",
       "      <td>26</td>\n",
       "      <td>163</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>1.246114e+18</td>\n",
       "      <td>['happi', 'mari', 'ye', 'call', 'first', 'name', 'thank', 'lucki', 'enough', 'mari', 'today', 'think', 'cannot', 'spend', 'togeth', 'pic', 'twitter']</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>dianesdangles</td>\n",
       "      <td>30</td>\n",
       "      <td>28</td>\n",
       "      <td>196</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>1.244642e+18</td>\n",
       "      <td>['great', 'everyon', 'thank', 'everyth', 'smile', 'happi', 'pic', 'twitter']</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>HollyBo49793261</td>\n",
       "      <td>34</td>\n",
       "      <td>32</td>\n",
       "      <td>287</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1.246143e+18</td>\n",
       "      <td>['never', 'thank', 'u', 'world', 'done', 'countless', 'time', 'give', 'u', 'joy', 'happi', 'day', 'love']</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>MyMommyWorld</td>\n",
       "      <td>37</td>\n",
       "      <td>33</td>\n",
       "      <td>197</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>1.244614e+18</td>\n",
       "      <td>['happi', 'mother', '’', 's', 'day', 'beauti', 'ladi', 'awesom', 'mammi', 'two', 'love', 'moon', 'back', 'spoon']</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>LToddWood</td>\n",
       "      <td>34</td>\n",
       "      <td>28</td>\n",
       "      <td>239</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>1.245671e+18</td>\n",
       "      <td>['five', 'year', 'old', 'made', 'cut', 'wood', 'everyth', 'primari', 'school', 'amaz', 'teacher', 'passion', 'see', 'curriculum', 'full', 'support', 'happi', 'day', 'pic', 'twitter']</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>UenoyaMafuYuki</td>\n",
       "      <td>49</td>\n",
       "      <td>44</td>\n",
       "      <td>268</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 24070 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    index            id  \\\n",
       "0   0      1.246628e+18   \n",
       "1   1      1.245898e+18   \n",
       "2   2      1.244717e+18   \n",
       "3   3      1.245730e+18   \n",
       "4   4      1.244636e+18   \n",
       "5   5      1.244048e+18   \n",
       "6   6      1.243919e+18   \n",
       "7   7      1.246245e+18   \n",
       "8   8      1.246139e+18   \n",
       "9   9      1.244255e+18   \n",
       "10  10     1.244233e+18   \n",
       "11  11     1.244249e+18   \n",
       "12  12     1.245385e+18   \n",
       "13  13     1.245410e+18   \n",
       "14  14     1.245083e+18   \n",
       "15  15     1.246114e+18   \n",
       "16  16     1.244642e+18   \n",
       "17  18     1.246143e+18   \n",
       "18  19     1.244614e+18   \n",
       "19  20     1.245671e+18   \n",
       "\n",
       "                                                                                                                                                                                                      original_text  \\\n",
       "0   ['yeah', 'potato', 'old', 'mean', 'threw', 'bag', 'toilet', 'happi', 'day', 'made', 'breakfast', 'time', 'thought', 'cool', 'draw', 'nake', 'ladi', 'dachshund', 'overhead', 'projector', 'psycholog', 'class']   \n",
       "1   ['happi', 'day', 'step', 'cover', 'parent', 'twitter']                                                                                                                                                            \n",
       "2   ['love', 'peopl', 'howev', 'awok', 'saw', 'woke', 'immedi', 'happi', 'day', 'across', 'way', 'god', 'bless', 'love', 'share', 'make']                                                                             \n",
       "3   ['happi', 'st', 'birthday', 'happi', 'mother', '’', 's', 'day', 'mum', 'can', '’', 't', 'see', 'today', 'keep', 'safe', 'shame', 'mani', 'advic', 'pic', 'twitter']                                               \n",
       "4   ['happi', 'day', 'wonder', 'world', 'live', 'present', 'alway', 'reli', 'mother', 'rock', 'whatev', 'life']                                                                                                       \n",
       "5   ['happi', 'day', 'thank', 'support', 'mother', 'daughter', 'spend', 'rest', 'unburden', 'made', 'everi', 'day', 'feel', 'like', 'could', 'last', 'lent', 'faith', 'pic', 'twitter']                               \n",
       "6   ['happi', 'ruth', 'happi', 'day', 'love', 'peac', 'beauti', 'day', 'beauti', 'friend', 'stay', 'super', 'safe', 'love', 'alway', 'pic', 'twitter']                                                                \n",
       "7   ['happi', 'day', 'amaz', 'care', 'love', 'much', 'give', 'best', 'thank', 'god', 'twitter', 'st']                                                                                                                 \n",
       "8   ['happi', 'day', 'mum', 'mani', 'could', 'not', 'visit', 'phone', 'good', 'day', 'better', 'time', 'pic', 'twitter']                                                                                              \n",
       "9   ['happi', 'mother', 'afternoon', 'bed', 'new', 'book', 'later', 'bliss']                                                                                                                                          \n",
       "10  ['happi', 'day', 'step', 'foster', 'love', 'mum', 'say', 'love', 'blind', 'etern', 'grate', 'not', 'leav', 'everi', 'next', 'beyond', 'love', 'pic', 'twitter']                                                   \n",
       "11  ['happi', 'day', 'anoth', 'mother', 'still', 'mother', 'spoken', 'pic', 'twitter']                                                                                                                                \n",
       "12  ['happi', 'day', 'fantast', 'garden', 'plant', 'blue', 'daisi', 'bush', 'nativ', 'south', 'pretti', 'littl', 'delight', 'appear', 'march', 'may', 'pic', 'twitter']                                               \n",
       "13  ['ago', 'terribl', 'cancer', 'day', 'hard', 'year', 'took', 'nurs', 'home', 'dad', 'happi', 'whole', 'build']                                                                                                     \n",
       "14  ['happi', 'day', 'even', 'no', 'remain', 'heart', 'forev', 'love', 'remaind', 'life', 'rest', 'pic', 'twitter']                                                                                                   \n",
       "15  ['happi', 'mari', 'ye', 'call', 'first', 'name', 'thank', 'lucki', 'enough', 'mari', 'today', 'think', 'cannot', 'spend', 'togeth', 'pic', 'twitter']                                                             \n",
       "16  ['great', 'everyon', 'thank', 'everyth', 'smile', 'happi', 'pic', 'twitter']                                                                                                                                      \n",
       "17  ['never', 'thank', 'u', 'world', 'done', 'countless', 'time', 'give', 'u', 'joy', 'happi', 'day', 'love']                                                                                                         \n",
       "18  ['happi', 'mother', '’', 's', 'day', 'beauti', 'ladi', 'awesom', 'mammi', 'two', 'love', 'moon', 'back', 'spoon']                                                                                                 \n",
       "19  ['five', 'year', 'old', 'made', 'cut', 'wood', 'everyth', 'primari', 'school', 'amaz', 'teacher', 'passion', 'see', 'curriculum', 'full', 'support', 'happi', 'day', 'pic', 'twitter']                            \n",
       "\n",
       "   lang  retweet_count  original_author  num_words  num_unique_words  \\\n",
       "0   en   0              LToddWood        56         46                 \n",
       "1   en   0              iiarushii        22         22                 \n",
       "2   en   0              andreaanderegg   44         35                 \n",
       "3   en   1              TheBookTweeters  30         28                 \n",
       "4   en   0              andreaanderegg   39         34                 \n",
       "5   en   0              iamxander5       41         39                 \n",
       "6   en   0              HoneyDewListApp  25         23                 \n",
       "7   en   0              LToddWood        35         32                 \n",
       "8   en   0              ShowBuzzDaily    33         31                 \n",
       "9   en   2              AngelsFreak7     22         21                 \n",
       "10  en   0              merc8156         59         50                 \n",
       "11  en   0              Chandan79311642  25         21                 \n",
       "12  en   0              nikkus15         44         42                 \n",
       "13  en   0              laroyal06        57         49                 \n",
       "14  en   0              JewelryName      29         26                 \n",
       "15  en   0              dianesdangles    30         28                 \n",
       "16  en   0              HollyBo49793261  34         32                 \n",
       "17  en   0              MyMommyWorld     37         33                 \n",
       "18  en   0              LToddWood        34         28                 \n",
       "19  en   0              UenoyaMafuYuki   49         44                 \n",
       "\n",
       "    num_chars  num_stopwords  ...  yov love   yr  yr cri  yr old zero  \\\n",
       "0   275        24             ...  0.0       0.0  0.0     0.0     0.0   \n",
       "1   192        6              ...  0.0       0.0  0.0     0.0     0.0   \n",
       "2   237        20             ...  0.0       0.0  0.0     0.0     0.0   \n",
       "3   203        9              ...  0.0       0.0  0.0     0.0     0.0   \n",
       "4   200        20             ...  0.0       0.0  0.0     0.0     0.0   \n",
       "5   271        16             ...  0.0       0.0  0.0     0.0     0.0   \n",
       "6   165        5              ...  0.0       0.0  0.0     0.0     0.0   \n",
       "7   211        15             ...  0.0       0.0  0.0     0.0     0.0   \n",
       "8   180        17             ...  0.0       0.0  0.0     0.0     0.0   \n",
       "9   210        6              ...  0.0       0.0  0.0     0.0     0.0   \n",
       "10  308        31             ...  0.0       0.0  0.0     0.0     0.0   \n",
       "11  156        8              ...  0.0       0.0  0.0     0.0     0.0   \n",
       "12  301        13             ...  0.0       0.0  0.0     0.0     0.0   \n",
       "13  280        23             ...  0.0       0.0  0.0     0.0     0.0   \n",
       "14  163        16             ...  0.0       0.0  0.0     0.0     0.0   \n",
       "15  196        12             ...  0.0       0.0  0.0     0.0     0.0   \n",
       "16  287        12             ...  0.0       0.0  0.0     0.0     0.0   \n",
       "17  197        19             ...  0.0       0.0  0.0     0.0     0.0   \n",
       "18  239        11             ...  0.0       0.0  0.0     0.0     0.0   \n",
       "19  268        23             ...  0.0       0.0  0.0     0.0     0.0   \n",
       "\n",
       "    zero wonder  zodiac  zodiac sign  zoom  zoom pic  \n",
       "0   0.0          0.0     0.0          0.0   0.0       \n",
       "1   0.0          0.0     0.0          0.0   0.0       \n",
       "2   0.0          0.0     0.0          0.0   0.0       \n",
       "3   0.0          0.0     0.0          0.0   0.0       \n",
       "4   0.0          0.0     0.0          0.0   0.0       \n",
       "5   0.0          0.0     0.0          0.0   0.0       \n",
       "6   0.0          0.0     0.0          0.0   0.0       \n",
       "7   0.0          0.0     0.0          0.0   0.0       \n",
       "8   0.0          0.0     0.0          0.0   0.0       \n",
       "9   0.0          0.0     0.0          0.0   0.0       \n",
       "10  0.0          0.0     0.0          0.0   0.0       \n",
       "11  0.0          0.0     0.0          0.0   0.0       \n",
       "12  0.0          0.0     0.0          0.0   0.0       \n",
       "13  0.0          0.0     0.0          0.0   0.0       \n",
       "14  0.0          0.0     0.0          0.0   0.0       \n",
       "15  0.0          0.0     0.0          0.0   0.0       \n",
       "16  0.0          0.0     0.0          0.0   0.0       \n",
       "17  0.0          0.0     0.0          0.0   0.0       \n",
       "18  0.0          0.0     0.0          0.0   0.0       \n",
       "19  0.0          0.0     0.0          0.0   0.0       \n",
       "\n",
       "[20 rows x 24070 columns]"
      ]
     },
     "execution_count": 568,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_id=test_df.id\n",
    "test_df=test_df.drop(['id','original_text','lang','original_author','clean_text','index'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_df.sentiment_class\n",
    "X = train_df.drop(['id','original_text','sentiment_class','lang','original_author','clean_text','index'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_scores={}\n",
    "best_models={}\n",
    "mnb = MultinomialNB(alpha=0.1)\n",
    "bnb = BernoulliNB()\n",
    "linsvc = LinearSVC()\n",
    "logreg = LogisticRegression()\n",
    "sgd=SGDClassifier()\n",
    "svc=SVC()\n",
    "gbm=GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid settings for the vectorizers (Count and TFIDF)\n",
    "parameters_vect = {\n",
    "    'features__pipe__vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'features__pipe__vect__ngram_range': ((1, 1), (1, 2),(1,3),(2,2)),\n",
    "    'features__pipe__vect__min_df': (1,2,3,4,5)\n",
    "}\n",
    "# Parameter grid settings for MultinomialNB\n",
    "'''parameters_mnb = {\n",
    "    'alpha': (0.01,0.1,0.25,0.5,0.75),\n",
    "    'fit_prior' : (True,False)\n",
    "}'''\n",
    "# Parameter grid settings for LogisticRegression\n",
    "parameters_logreg = {\n",
    "    'clf__C': (0.25, 0.5, 1.0),\n",
    "    'clf__penalty': ('l1', 'l2'),\n",
    "    'clf__fit_intercept': (True,False)\n",
    "}\n",
    "parameters_sgd = {\n",
    "    'clf__alpha': (0.00001, 0.000001),\n",
    "    'clf__penalty': ('l1', 'l2','elasticnet'),\n",
    "    'clf__max_iter': (10, 25, 50, 75, 100)\n",
    "}\n",
    "parameters_svc = {'clf__kernel': ['linear', 'rbf'],\n",
    "                    'clf__gamma':[0.1,1,10],\n",
    "                    'clf__C':[0.1,1,0.001,0.0001]\n",
    "                 }\n",
    "parameters_gbm = {'clf__loss':['deviance','exponential'],\n",
    "         'clf__learning_rate': [0.05,0.1],\n",
    "         'clf__max_depth':[5,10,20],\n",
    "         'clf__n_estimators': [10,25,50]\n",
    "        }\n",
    "parameters_xgb = {'clf__learning_rate':[0.01,0.1],\n",
    "         'clf__n_estimators':[50,100,500],\n",
    "         'clf__max_depth':[5,10,15,20],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "def grid_vect(model, X_train, X_test, y_train, y_test, parameters_clf=None):\n",
    "    #pipeline = Pipeline([('features', features),('clf', clf)])\n",
    "    \n",
    "    # Join the parameters dictionaries together\n",
    "    if parameters_clf is not None:\n",
    "        parameters = dict()\n",
    "        parameters.update(parameters_clf)\n",
    "        # Make sure you have scikit-learn version 0.19 or higher to use multiple scoring metrics\n",
    "        grid_search_model = RandomizedSearchCV(model, parameters, n_jobs=-1, scoring='f1_weighted',verbose=1, cv=5,random_state=42)\n",
    "        print(\"Performing grid search...\")\n",
    "        grid_search_model.fit(X_train, y_train)\n",
    "        print(\"Best CV score: %0.3f\" % grid_search_model.best_score_)\n",
    "        print(\"Best parameters set:\")\n",
    "        best_parameters = grid_search_model.best_estimator_.get_params()\n",
    "        for param_name in sorted(parameters.keys()):\n",
    "            print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "        \n",
    "        print(\"Test score with best_estimator_: %0.3f\" % grid_search_model.best_estimator_.score(X_test, y_test))\n",
    "        return grid_search_model\n",
    "    else:\n",
    "        scores = cross_val_score(model, X_train, y_train, cv=5,scoring='precision_macro')\n",
    "        print(\"Test score with cross validation are: \")\n",
    "        print(scores)\n",
    "        print(\"Test score with MNB: %0.3f\" % (100*(model.fit(X_train, y_train).score(X_test, y_test))))\n",
    "        model.fit(X_train,y_train)\n",
    "        return model\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>mean_word_len</th>\n",
       "      <th>2060</th>\n",
       "      <th>...</th>\n",
       "      <th>yov love</th>\n",
       "      <th>yr</th>\n",
       "      <th>yr cri</th>\n",
       "      <th>yr old</th>\n",
       "      <th>zero</th>\n",
       "      <th>zero wonder</th>\n",
       "      <th>zodiac</th>\n",
       "      <th>zodiac sign</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zoom pic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2912</th>\n",
       "      <td>3</td>\n",
       "      <td>57</td>\n",
       "      <td>53</td>\n",
       "      <td>275</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>3.842105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2241</th>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>37</td>\n",
       "      <td>280</td>\n",
       "      <td>19</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>5.244444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>40</td>\n",
       "      <td>293</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5.681818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>32</td>\n",
       "      <td>43</td>\n",
       "      <td>41</td>\n",
       "      <td>274</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>5.395349</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775</th>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>171</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>8.052632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24062 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      retweet_count  num_words  num_unique_words  num_chars  num_stopwords  \\\n",
       "2912  3              57         53                275        25              \n",
       "2241  0              45         37                280        19              \n",
       "257   0              44         40                293        18              \n",
       "454   32             43         41                274        17              \n",
       "775   0              19         18                171        4               \n",
       "\n",
       "      num_punctuations  num_words_upper  num_words_title  mean_word_len  2060  \\\n",
       "2912  4                 2                7                3.842105       0.0    \n",
       "2241  23                1                9                5.244444       0.0    \n",
       "257   15                0                6                5.681818       0.0    \n",
       "454   18                2                8                5.395349       0.0    \n",
       "775   9                 0                7                8.052632       0.0    \n",
       "\n",
       "      ...  yov love   yr  yr cri  yr old  zero  zero wonder  zodiac  \\\n",
       "2912  ...  0.0       0.0  0.0     0.0     0.0   0.0          0.0      \n",
       "2241  ...  0.0       0.0  0.0     0.0     0.0   0.0          0.0      \n",
       "257   ...  0.0       0.0  0.0     0.0     0.0   0.0          0.0      \n",
       "454   ...  0.0       0.0  0.0     0.0     0.0   0.0          0.0      \n",
       "775   ...  0.0       0.0  0.0     0.0     0.0   0.0          0.0      \n",
       "\n",
       "      zodiac sign  zoom  zoom pic  \n",
       "2912  0.0          0.0   0.0       \n",
       "2241  0.0          0.0   0.0       \n",
       "257   0.0          0.0   0.0       \n",
       "454   0.0          0.0   0.0       \n",
       "775   0.0          0.0   0.0       \n",
       "\n",
       "[5 rows x 24062 columns]"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>mean_word_len</th>\n",
       "      <th>2060</th>\n",
       "      <th>...</th>\n",
       "      <th>yov love</th>\n",
       "      <th>yr</th>\n",
       "      <th>yr cri</th>\n",
       "      <th>yr old</th>\n",
       "      <th>zero</th>\n",
       "      <th>zero wonder</th>\n",
       "      <th>zodiac</th>\n",
       "      <th>zodiac sign</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zoom pic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>46</td>\n",
       "      <td>275</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>3.928571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>192</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7.772727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>35</td>\n",
       "      <td>237</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>4.409091</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>28</td>\n",
       "      <td>203</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>34</td>\n",
       "      <td>200</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.153846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24062 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   retweet_count  num_words  num_unique_words  num_chars  num_stopwords  \\\n",
       "0  0              56         46                275        24              \n",
       "1  0              22         22                192        6               \n",
       "2  0              44         35                237        20              \n",
       "3  1              30         28                203        9               \n",
       "4  0              39         34                200        20              \n",
       "\n",
       "   num_punctuations  num_words_upper  num_words_title  mean_word_len  2060  \\\n",
       "0  10                7                10               3.928571       0.0    \n",
       "1  13                0                3                7.772727       0.0    \n",
       "2  9                 3                9                4.409091       0.0    \n",
       "3  10                1                5                5.800000       0.0    \n",
       "4  2                 0                3                4.153846       0.0    \n",
       "\n",
       "   ...  yov love   yr  yr cri  yr old  zero  zero wonder  zodiac  zodiac sign  \\\n",
       "0  ...  0.0       0.0  0.0     0.0     0.0   0.0          0.0     0.0           \n",
       "1  ...  0.0       0.0  0.0     0.0     0.0   0.0          0.0     0.0           \n",
       "2  ...  0.0       0.0  0.0     0.0     0.0   0.0          0.0     0.0           \n",
       "3  ...  0.0       0.0  0.0     0.0     0.0   0.0          0.0     0.0           \n",
       "4  ...  0.0       0.0  0.0     0.0     0.0   0.0          0.0     0.0           \n",
       "\n",
       "   zoom  zoom pic  \n",
       "0  0.0   0.0       \n",
       "1  0.0   0.0       \n",
       "2  0.0   0.0       \n",
       "3  0.0   0.0       \n",
       "4  0.0   0.0       \n",
       "\n",
       "[5 rows x 24062 columns]"
      ]
     },
     "execution_count": 576,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score with cross validation are: \n",
      "[0.39442026 0.36307185 0.36887035 0.3213841  0.34555627]\n",
      "Test score with MNB: 30.885\n",
      "\n",
      "Accuracy score of Multinomial naive bayes algorithm -----> 31.569974532150358\n"
     ]
    }
   ],
   "source": [
    "best_mnb_countvect = grid_vect(mnb,X_train,X_test,y_train,y_test,None)\n",
    "prediction_mnb=best_mnb_countvect.predict(X_test)\n",
    "mnb_score=100*(f1_score(y_test,prediction_mnb,average='weighted'))\n",
    "train_pred1=best_mnb_countvect.predict(X_train)\n",
    "test_pred1=best_mnb_countvect.predict(X_test)\n",
    "print(\"\\nAccuracy score of Multinomial naive bayes algorithm -----> \" + str(mnb_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_bnb_countvect = grid_vect(bnb,X_train, X_test,model_features,parameters_mnb,\n",
    "                               parameters_countvect=parameters_vect,parameters_tfidf=parameters_tfidf)\n",
    "prediction_bnb=best_bnb_countvect.predict(X_test)\n",
    "bnb_score=100*(f1_score(y_test,prediction_bnb,average='weighted'))\n",
    "best_scores['mnb_countvect']=best_bnb_countvect.best_score_\n",
    "best_models['mnb_countvect']=best_bnb_countvect\n",
    "train_pred1=best_bnb_countvect.predict(X_train)\n",
    "test_pred1=best_bnb_countvect.predict(X_test)\n",
    "print(\"\\nAccuracy score of Multinomial naive bayes algorithm -----> \" + str(bnb_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_mnb_model=mnb.set_params(**best_mnb_countvect.best_params_)\n",
    "best_mnb_countvect.fit(X,y)\n",
    "test_predictions=best_mnb_countvect.predict(test_df)\n",
    "prediction_df = pd.DataFrame(columns=['sentiment_class'],data=test_predictions)\n",
    "prediction_df = pd.concat([test_df_id,prediction_df],axis=1)\n",
    "prediction_df.to_csv(r'C:\\Users\\gaurav.singh.rawal\\Documents\\Gaurav\\Data Science\\Machine Learning-Predictive Analytics\\ML-Projects\\GIT - ML - Projects\\ML_Projects\\NLP - Sentiment Analysis\\HackerRank - Mothers Day\\predictions.csv',index=False,encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegression\n",
    "best_losso_countvect = grid_vect(linsvc,X_train, X_test,model_features,parameters_linsvc,\n",
    "                                 parameters_countvect=parameters_vect,parameters_tfidf=parameters_tfidf)\n",
    "prediction_losso = best_losso_countvect.predict(X_test)\n",
    "losso_score=100*(f1_score(y_test,prediction_losso,average='weighted'))\n",
    "best_scores['losso_countvect']=best_losso_countvect.best_score_\n",
    "best_models['losso_countvect']=best_losso_countvect\n",
    "train_pred2=best_losso_countvect.predict(X_train)\n",
    "test_pred2=best_losso_countvect.predict(X_test)\n",
    "print(\"\\nAccuracy score of Lasso algorithm -----> \" + str(losso_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD\n",
    "best_sgd_countvect = grid_vect(sgd, parameters_sgd, X_train, X_test,model_features, parameters_text=parameters_vect)\n",
    "prediction_sgd = best_sgd_countvect.predict(X_test)\n",
    "sgd_score=100*(f1_score(y_test,prediction_sgd,average='weighted'))\n",
    "best_scores['sgd_countvect']=best_sgd_countvect.best_score_\n",
    "best_models['sgd_countvect']=best_sgd_countvect\n",
    "train_pred3=best_sgd_countvect.predict(X_train)\n",
    "test_pred3=best_sgd_countvect.predict(X_test)\n",
    "print(\"\\nAccuracy score of SGD algorithm -----> \" + str(sgd_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC\n",
    "best_svc_countvect = grid_vect(svc, parameters_svc, X_train, X_test,model_features, parameters_text=parameters_vect)\n",
    "prediction_svc = best_svc_countvect.predict(X_test)\n",
    "svc_score=100*(f1_score(y_test,prediction_svc,average='weighted'))\n",
    "best_scores['svc_countvect']=best_svc_countvect.best_score_\n",
    "best_models['svc_countvect']=best_svc_countvect\n",
    "train_pred4=best_svc_countvect.predict(X_train)\n",
    "test_pred4=best_svc_countvect.predict(X_test)\n",
    "print(\"\\nAccuracy score of SVC algorithm -----> \" + str(svc_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBC\n",
    "best_gbm_countvect = grid_vect(gbm, parameters_gbm, X_train, X_test,model_features, parameters_text=parameters_vect)\n",
    "prediction_gbm = best_gbm_countvect.predict(X_test)\n",
    "gbm_score=100*(f1_score(y_test,prediction_gbm,average='weighted'))\n",
    "best_scores['gbm_countvect']=best_gbm_countvect.best_score_\n",
    "best_models['gbm_countvect']=best_gbm_countvect\n",
    "train_pred5=best_gbm_countvect.predict(X_train)\n",
    "test_pred5=best_gbm_countvect.predict(X_test)\n",
    "print(\"\\nAccuracy score of gbc algorithm -----> \" + str(gbm_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "countvect_classifier= VotingClassifier(estimators=[('mnb', best_models['mnb_countvect']), ('lr', best_models['logreg_countvect']),\n",
    "                         ('sgd', best_models['sgd_countvect']), ('svc', best_models['svc_countvect']),\n",
    "                         ('gbm', best_models['gbm_countvect'])],voting='hard')\n",
    "countvect_classifier.fit(X_train,y_train)\n",
    "countvect_classifier_predictions=countvect_classifier.predict(X_test)\n",
    "countvect_classifier_predictions_score=100*(f1_score(y_test,countvect_classifier_predictions,average='weighted'))\n",
    "print(\"\\nAccuracy score of Voting Classifier algorithm -----> \" + str(countvect_classifier_predictions_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([pd.DataFrame(train_pred1),pd.DataFrame(train_pred2),\n",
    "                      pd.DataFrame(train_pred3),pd.DataFrame(train_pred4),pd.DataFrame(train_pred5)], axis=1)\n",
    "df_test = pd.concat([pd.DataFrame(test_pred1),pd.DataFrame(test_pred2),pd.DataFrame(test_pred3)\n",
    "                     ,pd.DataFrame(test_pred4),pd.DataFrame(test_pred5)], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns=['mnb','lr','sgd','svc','gbm']\n",
    "df_test.columns=['mnb','lr','sgd','svc','gbm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "def hyperparameter_tuner(model,X_train,y_train,hp_list):\n",
    "    \n",
    "    hp_perf=[]\n",
    "    hp_model=RandomizedSearchCV(model,param_distributions=hp_list,n_iter=10,n_jobs=-1, scoring='f1_weighted',verbose=1, cv=5)\n",
    "    hp_model.fit(X_train,y_train)\n",
    "    best_hp_model=hp_model.best_estimator_\n",
    "    best_param=hp_model.best_params_\n",
    "    best_score=hp_model.best_score_  \n",
    "   \n",
    "    return best_hp_model,best_param,best_score\n",
    "\n",
    "xgb=XGBClassifier({\n",
    "    'objective': 'multi:softprob',\n",
    "    'eta': 0.1,\n",
    "    'max_depth': 3,\n",
    "    'silent' :1,\n",
    "    'num_class' : 3,\n",
    "    'eval_metric' : \"mlogloss\",\n",
    "    'min_child_weight': 1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.3,\n",
    "    'seed':17,\n",
    "    'num_rounds':2000,\n",
    "})\n",
    "xgb.fit(df_train,y_train)\n",
    "\n",
    "#best_model,best_params,best_score=hyperparameter_tuner(xgb,df_train,y_train,parameters_xgb)\n",
    "\n",
    "prediction_xgb = xgb.predict(df_test)\n",
    "xgb_score=100*(f1_score(y_test,prediction_xgb,average='weighted'))\n",
    "print(\"\\nAccuracy score of XGB stacking algorithm -----> \" + str(xgb_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = BaggingClassifier(RandomForestClassifier(max_depth=25,min_samples_leaf=2,n_estimators=25))\n",
    "model.fit(df_train,y_train)\n",
    "bagging_pred = model.predict(df_test)\n",
    "bagging_score=100*(f1_score(y_test,bagging_pred,average='weighted'))\n",
    "print(\"\\nAccuracy score of Random Forest Bagging algorithm -----> \" + str(bagging_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
