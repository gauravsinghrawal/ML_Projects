{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "from time import time\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import emoji\n",
    "from pprint import pprint\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.set(font_scale=1.3)\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.svm import LinearSVC\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv(r\"C:\\Users\\gaurav.singh.rawal\\Documents\\Gaurav\\Data Science\\Machine Learning-Predictive Analytics\\ML-Projects\\GIT - ML - Projects\\ML_Projects\\NLP - Sentiment Analysis\\HackerRank - Mothers Day\\train.csv\")\n",
    "test_data=pd.read_csv(r\"C:\\Users\\gaurav.singh.rawal\\Documents\\Gaurav\\Data Science\\Machine Learning-Predictive Analytics\\ML-Projects\\GIT - ML - Projects\\ML_Projects\\NLP - Sentiment Analysis\\HackerRank - Mothers Day\\test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>original_text</th>\n",
       "      <th>lang</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>original_author</th>\n",
       "      <th>sentiment_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.245025e+18</td>\n",
       "      <td>Happy #MothersDay to all you amazing mothers out there! I know it's hard not being able to see your mothers today but it's on all of us to do what we can to protect the most vulnerable members of our society. #BeatCoronaVirus pic.twitter.com/va4nFjFQ5B</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>BeenXXPired</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.245759e+18</td>\n",
       "      <td>Happy Mothers Day Mum - I'm sorry I can't be there to bring you Mothers day flowers &amp; a cwtch - honestly at this point I'd walk on hot coals to be able to. But I'll be there with bells on as soon as I can be. Love you lots xxx (p.s we need more photos!) https:// photos.app.goo.gl/M3vXBLrsCzD4TE bY7 …</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>FestiveFeeling</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.246087e+18</td>\n",
       "      <td>Happy mothers day To all This doing a mothers days work. Today been quiet but Had time to reflect. Dog walk, finish a jigsaw do the garden, learn few more guitar chords, drunk some strawberry gin and tonic and watch Lee evens on DVD. My favourite place to visit. #isolate pic.twitter.com/GZ0xVvF6f9</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>KrisAllenSak</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.244803e+18</td>\n",
       "      <td>Happy mothers day to this beautiful woman...royalty soothes you mummy jeremy and emerald and more #PrayForRoksie #UltimateLoveNG pic.twitter.com/oeetI22Pvv</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>Queenuchee</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.244876e+18</td>\n",
       "      <td>Remembering the 3 most amazing ladies who made me who I am! My late grandmother iris, mum carol and great grandmother Ethel. Missed but never forgotten! Happy mothers day to all those great mums out there! Love sent to all xxxx pic.twitter.com/xZZZdEybjE</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>brittan17446794</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  \\\n",
       "0  1.245025e+18   \n",
       "1  1.245759e+18   \n",
       "2  1.246087e+18   \n",
       "3  1.244803e+18   \n",
       "4  1.244876e+18   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                   original_text  \\\n",
       "0  Happy #MothersDay to all you amazing mothers out there! I know it's hard not being able to see your mothers today but it's on all of us to do what we can to protect the most vulnerable members of our society. #BeatCoronaVirus pic.twitter.com/va4nFjFQ5B                                                    \n",
       "1  Happy Mothers Day Mum - I'm sorry I can't be there to bring you Mothers day flowers & a cwtch - honestly at this point I'd walk on hot coals to be able to. But I'll be there with bells on as soon as I can be. Love you lots xxx (p.s we need more photos!) https:// photos.app.goo.gl/M3vXBLrsCzD4TE bY7 …   \n",
       "2  Happy mothers day To all This doing a mothers days work. Today been quiet but Had time to reflect. Dog walk, finish a jigsaw do the garden, learn few more guitar chords, drunk some strawberry gin and tonic and watch Lee evens on DVD. My favourite place to visit. #isolate pic.twitter.com/GZ0xVvF6f9      \n",
       "3  Happy mothers day to this beautiful woman...royalty soothes you mummy jeremy and emerald and more #PrayForRoksie #UltimateLoveNG pic.twitter.com/oeetI22Pvv                                                                                                                                                     \n",
       "4  Remembering the 3 most amazing ladies who made me who I am! My late grandmother iris, mum carol and great grandmother Ethel. Missed but never forgotten! Happy mothers day to all those great mums out there! Love sent to all xxxx pic.twitter.com/xZZZdEybjE                                                  \n",
       "\n",
       "  lang retweet_count  original_author  sentiment_class  \n",
       "0  en   0             BeenXXPired      0                \n",
       "1  en   1             FestiveFeeling   0                \n",
       "2  en   0             KrisAllenSak    -1                \n",
       "3  en   0             Queenuchee       0                \n",
       "4  en   0             brittan17446794 -1                "
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3235 entries, 0 to 3234\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   id               3235 non-null   float64\n",
      " 1   original_text    3235 non-null   object \n",
      " 2   lang             3231 non-null   object \n",
      " 3   retweet_count    3231 non-null   object \n",
      " 4   original_author  3235 non-null   object \n",
      " 5   sentiment_class  3235 non-null   int64  \n",
      "dtypes: float64(1), int64(1), object(4)\n",
      "memory usage: 151.8+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_e9de2ef0_a048_11ea_8e01_d43b045a62ecrow0_col0 {\n",
       "            background-color:  #9e9ac8;\n",
       "            color:  #000000;\n",
       "        }    #T_e9de2ef0_a048_11ea_8e01_d43b045a62ecrow0_col1 {\n",
       "            background-color:  #3f007d;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_e9de2ef0_a048_11ea_8e01_d43b045a62ecrow1_col0 {\n",
       "            background-color:  #fcfbfd;\n",
       "            color:  #000000;\n",
       "        }    #T_e9de2ef0_a048_11ea_8e01_d43b045a62ecrow1_col1 {\n",
       "            background-color:  #fcfbfd;\n",
       "            color:  #000000;\n",
       "        }    #T_e9de2ef0_a048_11ea_8e01_d43b045a62ecrow2_col0 {\n",
       "            background-color:  #3f007d;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_e9de2ef0_a048_11ea_8e01_d43b045a62ecrow2_col1 {\n",
       "            background-color:  #fcfbfd;\n",
       "            color:  #000000;\n",
       "        }</style><table id=\"T_e9de2ef0_a048_11ea_8e01_d43b045a62ec\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >sentiment_class</th>        <th class=\"col_heading level0 col1\" >original_text</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_e9de2ef0_a048_11ea_8e01_d43b045a62eclevel0_row0\" class=\"row_heading level0 row0\" >1</th>\n",
       "                        <td id=\"T_e9de2ef0_a048_11ea_8e01_d43b045a62ecrow0_col0\" class=\"data row0 col0\" >0</td>\n",
       "                        <td id=\"T_e9de2ef0_a048_11ea_8e01_d43b045a62ecrow0_col1\" class=\"data row0 col1\" >1701</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e9de2ef0_a048_11ea_8e01_d43b045a62eclevel0_row1\" class=\"row_heading level0 row1\" >0</th>\n",
       "                        <td id=\"T_e9de2ef0_a048_11ea_8e01_d43b045a62ecrow1_col0\" class=\"data row1 col0\" >-1</td>\n",
       "                        <td id=\"T_e9de2ef0_a048_11ea_8e01_d43b045a62ecrow1_col1\" class=\"data row1 col1\" >769</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_e9de2ef0_a048_11ea_8e01_d43b045a62eclevel0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_e9de2ef0_a048_11ea_8e01_d43b045a62ecrow2_col0\" class=\"data row2 col0\" >1</td>\n",
       "                        <td id=\"T_e9de2ef0_a048_11ea_8e01_d43b045a62ecrow2_col1\" class=\"data row2 col1\" >765</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2d67661f108>"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = train_data.groupby('sentiment_class').count()['original_text'].reset_index().sort_values(by='original_text',ascending=False)\n",
    "temp.style.background_gradient(cmap='Purples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2d676645108>"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='sentiment_class',data=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCounts(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def count_regex(self, pattern, tweet):\n",
    "        return len(re.findall(pattern, tweet))\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        # fit method is used when specific operations need to be done on the train data, but not on the test data\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        count_words = X.apply(lambda x: self.count_regex(r'\\w+', x)) \n",
    "        count_mentions = X.apply(lambda x: self.count_regex(r'@\\w+', x))\n",
    "        count_hashtags = X.apply(lambda x: self.count_regex(r'#\\w+', x))\n",
    "        count_capital_words = X.apply(lambda x: self.count_regex(r'\\b[A-Z]{2,}\\b', x))\n",
    "        count_excl_quest_marks = X.apply(lambda x: self.count_regex(r'!|\\?', x))\n",
    "        count_urls = X.apply(lambda x: self.count_regex(r'http.?://[^\\s]+[\\s]?', x))\n",
    "        # We will replace the emoji symbols with a description, which makes using a regex for counting easier\n",
    "        # Moreover, it will result in having more words in the tweet\n",
    "        count_emojis = X.apply(lambda x: emoji.demojize(x)).apply(lambda x: self.count_regex(r':[a-z_&]+:', x))\n",
    "        \n",
    "        df = pd.DataFrame({'count_words': count_words\n",
    "                           , 'count_mentions': count_mentions\n",
    "                           , 'count_hashtags': count_hashtags\n",
    "                           , 'count_capital_words': count_capital_words\n",
    "                           , 'count_excl_quest_marks': count_excl_quest_marks\n",
    "                           , 'count_urls': count_urls\n",
    "                           , 'count_emojis': count_emojis\n",
    "                          })\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = TextCounts()\n",
    "df_eda = tc.fit_transform(train_data.original_text)\n",
    "train_data=pd.concat([train_data,df_eda],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>original_text</th>\n",
       "      <th>lang</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>original_author</th>\n",
       "      <th>sentiment_class</th>\n",
       "      <th>count_words</th>\n",
       "      <th>count_mentions</th>\n",
       "      <th>count_hashtags</th>\n",
       "      <th>count_capital_words</th>\n",
       "      <th>count_excl_quest_marks</th>\n",
       "      <th>count_urls</th>\n",
       "      <th>count_emojis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.245025e+18</td>\n",
       "      <td>Happy #MothersDay to all you amazing mothers out there! I know it's hard not being able to see your mothers today but it's on all of us to do what we can to protect the most vulnerable members of our society. #BeatCoronaVirus pic.twitter.com/va4nFjFQ5B</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>BeenXXPired</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.245759e+18</td>\n",
       "      <td>Happy Mothers Day Mum - I'm sorry I can't be there to bring you Mothers day flowers &amp; a cwtch - honestly at this point I'd walk on hot coals to be able to. But I'll be there with bells on as soon as I can be. Love you lots xxx (p.s we need more photos!) https:// photos.app.goo.gl/M3vXBLrsCzD4TE bY7 …</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>FestiveFeeling</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.246087e+18</td>\n",
       "      <td>Happy mothers day To all This doing a mothers days work. Today been quiet but Had time to reflect. Dog walk, finish a jigsaw do the garden, learn few more guitar chords, drunk some strawberry gin and tonic and watch Lee evens on DVD. My favourite place to visit. #isolate pic.twitter.com/GZ0xVvF6f9</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>KrisAllenSak</td>\n",
       "      <td>-1</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.244803e+18</td>\n",
       "      <td>Happy mothers day to this beautiful woman...royalty soothes you mummy jeremy and emerald and more #PrayForRoksie #UltimateLoveNG pic.twitter.com/oeetI22Pvv</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>Queenuchee</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.244876e+18</td>\n",
       "      <td>Remembering the 3 most amazing ladies who made me who I am! My late grandmother iris, mum carol and great grandmother Ethel. Missed but never forgotten! Happy mothers day to all those great mums out there! Love sent to all xxxx pic.twitter.com/xZZZdEybjE</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>brittan17446794</td>\n",
       "      <td>-1</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  \\\n",
       "0  1.245025e+18   \n",
       "1  1.245759e+18   \n",
       "2  1.246087e+18   \n",
       "3  1.244803e+18   \n",
       "4  1.244876e+18   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                   original_text  \\\n",
       "0  Happy #MothersDay to all you amazing mothers out there! I know it's hard not being able to see your mothers today but it's on all of us to do what we can to protect the most vulnerable members of our society. #BeatCoronaVirus pic.twitter.com/va4nFjFQ5B                                                    \n",
       "1  Happy Mothers Day Mum - I'm sorry I can't be there to bring you Mothers day flowers & a cwtch - honestly at this point I'd walk on hot coals to be able to. But I'll be there with bells on as soon as I can be. Love you lots xxx (p.s we need more photos!) https:// photos.app.goo.gl/M3vXBLrsCzD4TE bY7 …   \n",
       "2  Happy mothers day To all This doing a mothers days work. Today been quiet but Had time to reflect. Dog walk, finish a jigsaw do the garden, learn few more guitar chords, drunk some strawberry gin and tonic and watch Lee evens on DVD. My favourite place to visit. #isolate pic.twitter.com/GZ0xVvF6f9      \n",
       "3  Happy mothers day to this beautiful woman...royalty soothes you mummy jeremy and emerald and more #PrayForRoksie #UltimateLoveNG pic.twitter.com/oeetI22Pvv                                                                                                                                                     \n",
       "4  Remembering the 3 most amazing ladies who made me who I am! My late grandmother iris, mum carol and great grandmother Ethel. Missed but never forgotten! Happy mothers day to all those great mums out there! Love sent to all xxxx pic.twitter.com/xZZZdEybjE                                                  \n",
       "\n",
       "  lang retweet_count  original_author  sentiment_class  count_words  \\\n",
       "0  en   0             BeenXXPired      0                48            \n",
       "1  en   1             FestiveFeeling   0                65            \n",
       "2  en   0             KrisAllenSak    -1                54            \n",
       "3  en   0             Queenuchee       0                22            \n",
       "4  en   0             brittan17446794 -1                45            \n",
       "\n",
       "   count_mentions  count_hashtags  count_capital_words  \\\n",
       "0  0               2               0                     \n",
       "1  0               0               0                     \n",
       "2  0               1               1                     \n",
       "3  0               2               0                     \n",
       "4  0               0               0                     \n",
       "\n",
       "   count_excl_quest_marks  count_urls  count_emojis  \n",
       "0  1                       0           0             \n",
       "1  1                       0           0             \n",
       "2  0                       0           0             \n",
       "3  0                       0           0             \n",
       "4  3                       0           0             "
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_dist(df, col):\n",
    "    print('Descriptive stats for {}'.format(col))\n",
    "    print('-'*(len(col)+22))\n",
    "    print(df.groupby('sentiment_class')[col].describe())\n",
    "    bins = np.arange(df[col].min(), df[col].max() + 1)\n",
    "    g = sns.FacetGrid(df, col='sentiment_class', size=5, hue='sentiment_class', palette=\"PuBuGn_d\")\n",
    "    g = g.map(sns.distplot, col, kde=False, norm_hist=True, bins=bins)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanText(BaseEstimator, TransformerMixin):\n",
    "    def remove_mentions(self, input_text):\n",
    "        return re.sub(r'@\\w+', '', input_text)\n",
    "    \n",
    "    def remove_urls(self, input_text):\n",
    "        return re.sub(r'http.?://[^\\s]+[\\s]?', '', input_text)\n",
    "    \n",
    "    def emoji_oneword(self, input_text):\n",
    "        # By compressing the underscore, the emoji is kept as one word\n",
    "        return input_text.replace('_','')\n",
    "    \n",
    "    def remove_punctuation(self, input_text):\n",
    "        # Make translation table\n",
    "        punct = string.punctuation\n",
    "        trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space\n",
    "        return input_text.translate(trantab)\n",
    "    def remove_digits(self, input_text):\n",
    "        return re.sub('\\d+', '', input_text)\n",
    "    \n",
    "    def to_lower(self, input_text):\n",
    "        return input_text.lower()\n",
    "    \n",
    "    def remove_stopwords(self, input_text):\n",
    "        stopwords_list = stopwords.words('english')\n",
    "        # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "        whitelist = [\"n't\", \"not\", \"no\"]\n",
    "        words = input_text.split() \n",
    "        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
    "        return \" \".join(clean_words) \n",
    "    \n",
    "    def stemming(self, input_text):\n",
    "        porter = PorterStemmer()\n",
    "        words = input_text.split() \n",
    "        stemmed_words = [porter.stem(word) for word in words]\n",
    "        return \" \".join(stemmed_words)\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        clean_X = X.apply(self.remove_mentions).apply(self.remove_urls).apply(self.emoji_oneword).apply(self.remove_punctuation).apply(self.remove_digits).apply(self.to_lower).apply(self.remove_stopwords).apply(self.stemming)\n",
    "        return clean_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = CleanText()\n",
    "train_data['original_text'] = ct.fit_transform(train_data.original_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow0_col1 {\n",
       "            background-color:  #08306b;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow1_col1 {\n",
       "            background-color:  #083776;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow2_col1 {\n",
       "            background-color:  #08519c;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow3_col1 {\n",
       "            background-color:  #4090c5;\n",
       "            color:  #000000;\n",
       "        }    #T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow4_col1 {\n",
       "            background-color:  #87bddc;\n",
       "            color:  #000000;\n",
       "        }    #T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow5_col1 {\n",
       "            background-color:  #9cc9e1;\n",
       "            color:  #000000;\n",
       "        }    #T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow6_col1 {\n",
       "            background-color:  #9cc9e1;\n",
       "            color:  #000000;\n",
       "        }    #T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow7_col1 {\n",
       "            background-color:  #a5cde3;\n",
       "            color:  #000000;\n",
       "        }    #T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow8_col1 {\n",
       "            background-color:  #c1d9ed;\n",
       "            color:  #000000;\n",
       "        }    #T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow9_col1 {\n",
       "            background-color:  #dbe9f6;\n",
       "            color:  #000000;\n",
       "        }    #T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow10_col1 {\n",
       "            background-color:  #dce9f6;\n",
       "            color:  #000000;\n",
       "        }    #T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow11_col1 {\n",
       "            background-color:  #e8f1fa;\n",
       "            color:  #000000;\n",
       "        }    #T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow12_col1 {\n",
       "            background-color:  #ebf3fb;\n",
       "            color:  #000000;\n",
       "        }    #T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow13_col1 {\n",
       "            background-color:  #f3f8fe;\n",
       "            color:  #000000;\n",
       "        }    #T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow14_col1 {\n",
       "            background-color:  #f3f8fe;\n",
       "            color:  #000000;\n",
       "        }    #T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow15_col1 {\n",
       "            background-color:  #f5f9fe;\n",
       "            color:  #000000;\n",
       "        }    #T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow16_col1 {\n",
       "            background-color:  #f6faff;\n",
       "            color:  #000000;\n",
       "        }    #T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow17_col1 {\n",
       "            background-color:  #f6faff;\n",
       "            color:  #000000;\n",
       "        }    #T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow18_col1 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }    #T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow19_col1 {\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }</style><table id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62ec\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Common_words</th>        <th class=\"col_heading level0 col1\" >count</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62eclevel0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow0_col0\" class=\"data row0 col0\" >mother</td>\n",
       "                        <td id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow0_col1\" class=\"data row0 col1\" >3936</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62eclevel0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow1_col0\" class=\"data row1 col0\" >day</td>\n",
       "                        <td id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow1_col1\" class=\"data row1 col1\" >3830</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62eclevel0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow2_col0\" class=\"data row2 col0\" >happi</td>\n",
       "                        <td id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow2_col1\" class=\"data row2 col1\" >3471</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62eclevel0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow3_col0\" class=\"data row3 col0\" >com</td>\n",
       "                        <td id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow3_col1\" class=\"data row3 col1\" >2602</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62eclevel0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow4_col0\" class=\"data row4 col0\" >mothersday</td>\n",
       "                        <td id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow4_col1\" class=\"data row4 col1\" >1876</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62eclevel0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "                        <td id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow5_col0\" class=\"data row5 col0\" >twitter</td>\n",
       "                        <td id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow5_col1\" class=\"data row5 col1\" >1705</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62eclevel0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "                        <td id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow6_col0\" class=\"data row6 col0\" >mum</td>\n",
       "                        <td id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow6_col1\" class=\"data row6 col1\" >1693</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62eclevel0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "                        <td id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow7_col0\" class=\"data row7 col0\" >pic</td>\n",
       "                        <td id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow7_col1\" class=\"data row7 col1\" >1593</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62eclevel0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "                        <td id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow8_col0\" class=\"data row8 col0\" >love</td>\n",
       "                        <td id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow8_col1\" class=\"data row8 col1\" >1290</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62eclevel0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "                        <td id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow9_col0\" class=\"data row9 col0\" >instagram</td>\n",
       "                        <td id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow9_col1\" class=\"data row9 col1\" >835</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62eclevel0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "                        <td id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow10_col0\" class=\"data row10 col0\" >igshid</td>\n",
       "                        <td id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow10_col1\" class=\"data row10 col1\" >818</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62eclevel0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "                        <td id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow11_col0\" class=\"data row11 col0\" >mother’</td>\n",
       "                        <td id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow11_col1\" class=\"data row11 col1\" >595</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62eclevel0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "                        <td id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow12_col0\" class=\"data row12 col0\" >today</td>\n",
       "                        <td id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow12_col1\" class=\"data row12 col1\" >537</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62eclevel0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "                        <td id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow13_col0\" class=\"data row13 col0\" >us</td>\n",
       "                        <td id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow13_col1\" class=\"data row13 col1\" >400</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62eclevel0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "                        <td id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow14_col0\" class=\"data row14 col0\" >thank</td>\n",
       "                        <td id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow14_col1\" class=\"data row14 col1\" >394</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62eclevel0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "                        <td id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow15_col0\" class=\"data row15 col0\" >wish</td>\n",
       "                        <td id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow15_col1\" class=\"data row15 col1\" >374</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62eclevel0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "                        <td id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow16_col0\" class=\"data row16 col0\" >one</td>\n",
       "                        <td id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow16_col1\" class=\"data row16 col1\" >344</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62eclevel0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "                        <td id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow17_col0\" class=\"data row17 col0\" >amaz</td>\n",
       "                        <td id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow17_col1\" class=\"data row17 col1\" >337</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62eclevel0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "                        <td id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow18_col0\" class=\"data row18 col0\" >not</td>\n",
       "                        <td id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow18_col1\" class=\"data row18 col1\" >336</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62eclevel0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "                        <td id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow19_col0\" class=\"data row19 col0\" >hope</td>\n",
       "                        <td id=\"T_f2c7b8e2_a048_11ea_824f_d43b045a62ecrow19_col1\" class=\"data row19 col1\" >322</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2d676f6d508>"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['temp_list'] = train_data['original_text'].apply(lambda x:str(x).split())\n",
    "top = Counter([item for sublist in train_data['temp_list'] for item in sublist])\n",
    "temp = pd.DataFrame(top.most_common(20))\n",
    "temp.columns = ['Common_words','count']\n",
    "temp.style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"clean_text\"]= train_data[\"original_text\"].str.replace(\"mum\", \"mother\", case = False)\n",
    "train_data[\"clean_text\"]= train_data[\"clean_text\"].str.replace(\"mom\", \"mother\", case = False)\n",
    "train_data[\"clean_text\"]= train_data[\"clean_text\"].str.replace(\"mothersday\",\"mother day\") \n",
    "train_data[\"clean_text\"]= train_data[\"clean_text\"].str.replace(\"httpswww\",\"\") \n",
    "train_data[\"clean_text\"]= train_data[\"clean_text\"].str.replace(\"http\",\"\") \n",
    "train_data[\"clean_text\"]= train_data[\"clean_text\"].str.replace(\"u\",\"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnExtractor(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "    def transform(self, X, **transform_params):\n",
    "        return X[self.cols]\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_data.drop(['id','original_text','sentiment_class','lang','original_author'],axis=1), train_data.sentiment_class, test_size=0.2, random_state=37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html\n",
    "def grid_vect(clf, parameters_clf, X_train, X_test, parameters_text=None, vect=None, is_w2v=False):\n",
    "    \n",
    "    textcountscols = ['count_capital_words','count_emojis','count_excl_quest_marks','count_hashtags'\n",
    "                      ,'count_mentions','count_urls','count_words']\n",
    "    \n",
    "    if is_w2v:\n",
    "        w2vcols = []\n",
    "        for i in range(SIZE):\n",
    "            w2vcols.append(i)\n",
    "        features = FeatureUnion([('textcounts', ColumnExtractor(cols=textcountscols))\n",
    "                                 , ('w2v', ColumnExtractor(cols=w2vcols))]\n",
    "                                , n_jobs=-1)\n",
    "    else:\n",
    "        features = FeatureUnion([('textcounts', ColumnExtractor(cols=textcountscols))\n",
    "                                 , ('pipe', Pipeline([('cleantext', ColumnExtractor(cols='clean_text')), ('vect', vect)]))]\n",
    "                                , n_jobs=-1)\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('features', features)\n",
    "        , ('clf', clf)\n",
    "    ])\n",
    "    \n",
    "    # Join the parameters dictionaries together\n",
    "    parameters = dict()\n",
    "    if parameters_text:\n",
    "        parameters.update(parameters_text)\n",
    "    parameters.update(parameters_clf)\n",
    "    # Make sure you have scikit-learn version 0.19 or higher to use multiple scoring metrics\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, cv=5)\n",
    "    \n",
    "    print(\"Performing grid search...\")\n",
    "    t0 = time()\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "    print(\"Best CV score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "        \n",
    "    print(\"Test score with best_estimator_: %0.3f\" % grid_search.best_estimator_.score(X_test, y_test))\n",
    "    print(\"\\n\")\n",
    "    print(\"Classification Report Test Data\")\n",
    "    print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))\n",
    "                        \n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid settings for the vectorizers (Count and TFIDF)\n",
    "parameters_vect = {\n",
    "    'features__pipe__vect__max_df': (0.25, 0.5, 0.75),\n",
    "    'features__pipe__vect__ngram_range': ((1, 1), (1, 2),(1,3)),\n",
    "    'features__pipe__vect__min_df': (1,2)\n",
    "}\n",
    "# Parameter grid settings for MultinomialNB\n",
    "parameters_mnb = {\n",
    "    'clf__alpha': (0.25, 0.5, 0.75)\n",
    "}\n",
    "# Parameter grid settings for LogisticRegression\n",
    "parameters_logreg = {\n",
    "    'clf__C': (0.25, 0.5, 1.0),\n",
    "    'clf__penalty': ('l1', 'l2')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB()\n",
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['features', 'clf']\n",
      "parameters:\n",
      "{'clf__alpha': (0.25, 0.5, 0.75),\n",
      " 'features__pipe__vect__max_df': (0.25, 0.5, 0.75),\n",
      " 'features__pipe__vect__min_df': (1, 2),\n",
      " 'features__pipe__vect__ngram_range': ((1, 1), (1, 2), (1, 3))}\n",
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  35 tasks      | elapsed:   17.8s\n",
      "[Parallel(n_jobs=-1)]: Done 185 tasks      | elapsed:   31.3s\n",
      "[Parallel(n_jobs=-1)]: Done 270 out of 270 | elapsed:   39.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 41.981s\n",
      "\n",
      "Best CV score: 0.524\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.75\n",
      "\tfeatures__pipe__vect__max_df: 0.25\n",
      "\tfeatures__pipe__vect__min_df: 1\n",
      "\tfeatures__pipe__vect__ngram_range: (1, 2)\n",
      "Test score with best_estimator_: 0.519\n",
      "\n",
      "\n",
      "Classification Report Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00       160\n",
      "           0       0.52      0.99      0.68       339\n",
      "           1       0.25      0.01      0.03       148\n",
      "\n",
      "    accuracy                           0.52       647\n",
      "   macro avg       0.26      0.33      0.24       647\n",
      "weighted avg       0.33      0.52      0.36       647\n",
      "\n",
      "Performing grid search...\n",
      "pipeline: ['features', 'clf']\n",
      "parameters:\n",
      "{'clf__C': (0.25, 0.5, 1.0),\n",
      " 'clf__penalty': ('l1', 'l2'),\n",
      " 'features__pipe__vect__max_df': (0.25, 0.5, 0.75),\n",
      " 'features__pipe__vect__min_df': (1, 2),\n",
      " 'features__pipe__vect__ngram_range': ((1, 1), (1, 2), (1, 3))}\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 540 out of 540 | elapsed:  3.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 243.654s\n",
      "\n",
      "Best CV score: 0.482\n",
      "Best parameters set:\n",
      "\tclf__C: 0.25\n",
      "\tclf__penalty: 'l2'\n",
      "\tfeatures__pipe__vect__max_df: 0.25\n",
      "\tfeatures__pipe__vect__min_df: 1\n",
      "\tfeatures__pipe__vect__ngram_range: (1, 3)\n",
      "Test score with best_estimator_: 0.453\n",
      "\n",
      "\n",
      "Classification Report Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.18      0.06      0.09       160\n",
      "           0       0.51      0.80      0.62       339\n",
      "           1       0.21      0.08      0.12       148\n",
      "\n",
      "    accuracy                           0.45       647\n",
      "   macro avg       0.30      0.31      0.27       647\n",
      "weighted avg       0.36      0.45      0.37       647\n",
      "\n"
     ]
    }
   ],
   "source": [
    "countvect = CountVectorizer()\n",
    "# MultinomialNB\n",
    "best_mnb_countvect = grid_vect(mnb, parameters_mnb, X_train, X_test, parameters_text=parameters_vect, vect=countvect)\n",
    "# LogisticRegression\n",
    "best_logreg_countvect = grid_vect(logreg, parameters_logreg, X_train, X_test, parameters_text=parameters_vect, vect=countvect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_logreg = best_logreg_countvect.predict(X_test)\n",
    "prediction_mnb=best_mnb_countvect.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_score=100*(f1_score(y_test,prediction_mnb,average='weighted'))\n",
    "logreg_score=100*(f1_score(y_test,prediction_logreg,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy score of Multinomial naive bayes algorithm -----> 36.410800773840016\n",
      "\n",
      "Accuracy score of Log Reg algorithm -----> 37.2755037490733\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAccuracy score of Multinomial naive bayes algorithm -----> \" + str(mnb_score))\n",
    "print(\"\\nAccuracy score of Log Reg algorithm -----> \" + str(logreg_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 50\n",
    "X_train['clean_text_wordlist'] = X_train.clean_text.apply(lambda x : word_tokenize(x))\n",
    "X_test['clean_text_wordlist'] = X_test.clean_text.apply(lambda x : word_tokenize(x))\n",
    "model = gensim.models.Word2Vec(X_train.clean_text_wordlist, min_count=1, size=SIZE, window=5, workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_w2v_vector(w2v_dict, tweet):\n",
    "    list_of_word_vectors = [w2v_dict[w] for w in tweet if w in w2v_dict.vocab.keys()]\n",
    "    \n",
    "    if len(list_of_word_vectors) == 0:\n",
    "        result = [0.0]*SIZE\n",
    "    else:\n",
    "        result = np.sum(list_of_word_vectors, axis=0) / len(list_of_word_vectors)\n",
    "        \n",
    "    return result\n",
    "X_train_w2v = X_train['clean_text_wordlist'].apply(lambda x: compute_avg_w2v_vector(model.wv, x))\n",
    "X_test_w2v = X_test['clean_text_wordlist'].apply(lambda x: compute_avg_w2v_vector(model.wv, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2v = pd.DataFrame(X_train_w2v.values.tolist(), index= X_train.index)\n",
    "X_test_w2v = pd.DataFrame(X_test_w2v.values.tolist(), index= X_test.index)\n",
    "# Concatenate with the TextCounts variables\n",
    "X_train_w2v = pd.concat([X_train_w2v, X_train.drop(['clean_text', 'clean_text_wordlist'], axis=1)], axis=1)\n",
    "X_test_w2v = pd.concat([X_test_w2v, X_test.drop(['clean_text', 'clean_text_wordlist'], axis=1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['features', 'clf']\n",
      "parameters:\n",
      "{'clf__C': (0.25, 0.5, 1.0), 'clf__penalty': ('l1', 'l2')}\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    1.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.212s\n",
      "\n",
      "Best CV score: 0.527\n",
      "Best parameters set:\n",
      "\tclf__C: 0.25\n",
      "\tclf__penalty: 'l2'\n",
      "Test score with best_estimator_: 0.524\n",
      "\n",
      "\n",
      "Classification Report Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00       160\n",
      "           0       0.52      1.00      0.69       339\n",
      "           1       0.00      0.00      0.00       148\n",
      "\n",
      "    accuracy                           0.52       647\n",
      "   macro avg       0.17      0.33      0.23       647\n",
      "weighted avg       0.27      0.52      0.36       647\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_logreg_w2v = grid_vect(logreg, parameters_logreg, X_train_w2v, X_test_w2v, is_w2v=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy score of Log Reg algorithm -----> 37.2755037490733\n"
     ]
    }
   ],
   "source": [
    "prediction_w2v_logreg=best_logreg_w2v.predict(X_test_w2v)\n",
    "logreg_w2v_score=100*(f1_score(y_test,prediction_w2v_logreg,average='weighted'))\n",
    "print(\"\\nAccuracy score of Log Reg algorithm -----> \" + str(logreg_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid settings for the vectorizers (Count and TFIDF)\n",
    "parameters_vect = None\n",
    "# Parameter grid settings for MultinomialNB\n",
    "parameters_mnb = {\n",
    "    'clf__alpha': (0.25, 0.5, 0.75)\n",
    "}\n",
    "# Parameter grid settings for LogisticRegression\n",
    "parameters_logreg = {\n",
    "    'clf__C': (0.25, 0.5, 1.0),\n",
    "    'clf__penalty': ('l1', 'l2')\n",
    "}\n",
    "\n",
    "parameters_svc = {\n",
    "    'clf__penalty': ('l1', 'l2'),\n",
    "    'clf__loss': ('hinge','squared_hinge'),\n",
    "    'clf__dual':(True,False),\n",
    "    'clf__C': (0.25, 0.5, 1.0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html\n",
    "def grid_vect(clf, parameters_clf, X_train, X_test, parameters_text=None, vect=None, is_w2v=False):\n",
    "    \n",
    "    textcountscols = ['count_capital_words','count_emojis','count_excl_quest_marks','count_hashtags'\n",
    "                      ,'count_mentions','count_urls','count_words']\n",
    "    \n",
    "    if is_w2v:\n",
    "        w2vcols = []\n",
    "        for i in range(SIZE):\n",
    "            w2vcols.append(i)\n",
    "        features = FeatureUnion([('textcounts', ColumnExtractor(cols=textcountscols))\n",
    "                                 , ('w2v', ColumnExtractor(cols=w2vcols))]\n",
    "                                , n_jobs=-1)\n",
    "    else:\n",
    "        features = FeatureUnion([('textcounts', ColumnExtractor(cols=textcountscols))\n",
    "                                 , ('pipe', Pipeline([('cleantext', ColumnExtractor(cols='clean_text')), ('vect', vect)]))]\n",
    "                                , n_jobs=-1)\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('features', features)\n",
    "        , ('clf', clf)\n",
    "    ])\n",
    "    \n",
    "    # Join the parameters dictionaries together\n",
    "    parameters = dict()\n",
    "    if parameters_text:\n",
    "        parameters.update(parameters_text)\n",
    "    parameters.update(parameters_clf)\n",
    "    # Make sure you have scikit-learn version 0.19 or higher to use multiple scoring metrics\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, cv=5)\n",
    "    \n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "    t0 = time()\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "    print(\"Best CV score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "        \n",
    "    print(\"Test score with best_estimator_: %0.3f\" % grid_search.best_estimator_.score(X_test, y_test))\n",
    "    print(\"\\n\")\n",
    "    print(\"Classification Report Test Data\")\n",
    "    print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))\n",
    "                        \n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['features', 'clf']\n",
      "parameters:\n",
      "{'clf__alpha': (0.25, 0.5, 0.75)}\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:    1.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.821s\n",
      "\n",
      "Best CV score: 0.525\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.75\n",
      "Test score with best_estimator_: 0.521\n",
      "\n",
      "\n",
      "Classification Report Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00       160\n",
      "           0       0.52      0.99      0.69       339\n",
      "           1       0.00      0.00      0.00       148\n",
      "\n",
      "    accuracy                           0.52       647\n",
      "   macro avg       0.17      0.33      0.23       647\n",
      "weighted avg       0.27      0.52      0.36       647\n",
      "\n",
      "Performing grid search...\n",
      "pipeline: ['features', 'clf']\n",
      "parameters:\n",
      "{'clf__C': (0.25, 0.5, 1.0), 'clf__penalty': ('l1', 'l2')}\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed:    4.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 6.062s\n",
      "\n",
      "Best CV score: 0.526\n",
      "Best parameters set:\n",
      "\tclf__C: 0.25\n",
      "\tclf__penalty: 'l2'\n",
      "Test score with best_estimator_: 0.524\n",
      "\n",
      "\n",
      "Classification Report Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00       160\n",
      "           0       0.52      1.00      0.69       339\n",
      "           1       0.00      0.00      0.00       148\n",
      "\n",
      "    accuracy                           0.52       647\n",
      "   macro avg       0.17      0.33      0.23       647\n",
      "weighted avg       0.27      0.52      0.36       647\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf_vect = TfidfVectorizer(min_df=3,max_features=None,strip_accents='unicode',analyzer='word',token_pattern=r'\\w{1,}',\n",
    "ngram_range=(1, 2),use_idf=1,smooth_idf=1,sublinear_tf=1,stop_words='english')\n",
    "# MultinomialNB\n",
    "best_mnb_countvect = grid_vect(mnb, parameters_mnb, X_train, X_test, parameters_text=parameters_vect, vect=tfidf_vect)\n",
    "# LogisticRegression\n",
    "best_logreg_countvect = grid_vect(logreg, parameters_logreg, X_train, X_test, parameters_text=parameters_vect, vect=tfidf_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_logreg = best_logreg_countvect.predict(X_test)\n",
    "prediction_mnb=best_mnb_countvect.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_score=100*(f1_score(y_test,prediction_mnb,average='weighted'))\n",
    "logreg_score=100*(f1_score(y_test,prediction_logreg,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy score of Multinomial naive bayes algorithm -----> 35.961999137488704\n",
      "\n",
      "Accuracy score of Log Reg algorithm -----> 36.065244509999296\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAccuracy score of Multinomial naive bayes algorithm -----> \" + str(mnb_score))\n",
    "print(\"\\nAccuracy score of Log Reg algorithm -----> \" + str(logreg_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['features', 'clf']\n",
      "parameters:\n",
      "{'clf__C': (0.25, 0.5, 1.0),\n",
      " 'clf__dual': (True, False),\n",
      " 'clf__loss': ('hinge', 'squared_hinge'),\n",
      " 'clf__penalty': ('l1', 'l2')}\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  35 tasks      | elapsed:   20.8s\n",
      "[Parallel(n_jobs=-1)]: Done 120 out of 120 | elapsed:   34.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 37.584s\n",
      "\n",
      "Best CV score: 0.525\n",
      "Best parameters set:\n",
      "\tclf__C: 0.25\n",
      "\tclf__dual: True\n",
      "\tclf__loss: 'hinge'\n",
      "\tclf__penalty: 'l2'\n",
      "Test score with best_estimator_: 0.524\n",
      "\n",
      "\n",
      "Classification Report Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.00      0.00      0.00       160\n",
      "           0       0.52      1.00      0.69       339\n",
      "           1       0.00      0.00      0.00       148\n",
      "\n",
      "    accuracy                           0.52       647\n",
      "   macro avg       0.17      0.33      0.23       647\n",
      "weighted avg       0.27      0.52      0.36       647\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svc = LinearSVC(dual=False)\n",
    "best_linear_svc=grid_vect(svc, parameters_svc, X_train, X_test, parameters_text=parameters_vect, vect=tfidf_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_score=100*(f1_score(y_test,prediction_mnb,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy score of SVC algorithm -----> 35.961999137488704\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAccuracy score of SVC algorithm -----> \" + str(svc_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
