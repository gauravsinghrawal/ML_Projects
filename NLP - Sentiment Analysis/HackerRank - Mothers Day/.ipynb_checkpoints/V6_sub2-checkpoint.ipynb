{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\gaurav.singh.rawal\\AppData\\Roaming\\nltk_data.\n",
      "[nltk_data]     ..\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "from time import time\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import emoji\n",
    "from pprint import pprint\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.set(font_scale=1.3)\n",
    "from sklearn import model_selection\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer,TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split,RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import classification_report,f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, Lasso\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "import xgboost as xgb\n",
    "import gensim\n",
    "from contractions import CONTRACTION_MAP\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(37)\n",
    "color = sns.color_palette()\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv(r\"C:\\Users\\gaurav.singh.rawal\\Documents\\Gaurav\\Data Science\\Machine Learning-Predictive Analytics\\ML-Projects\\GIT - ML - Projects\\ML_Projects\\NLP - Sentiment Analysis\\HackerRank - Mothers Day\\train.csv\")\n",
    "test_data=pd.read_csv(r\"C:\\Users\\gaurav.singh.rawal\\Documents\\Gaurav\\Data Science\\Machine Learning-Predictive Analytics\\ML-Projects\\GIT - ML - Projects\\ML_Projects\\NLP - Sentiment Analysis\\HackerRank - Mothers Day\\test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>original_text</th>\n",
       "      <th>lang</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>original_author</th>\n",
       "      <th>sentiment_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.245025e+18</td>\n",
       "      <td>Happy #MothersDay to all you amazing mothers out there! I know it's hard not being able to see your mothers today but it's on all of us to do what we can to protect the most vulnerable members of our society. #BeatCoronaVirus pic.twitter.com/va4nFjFQ5B</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>BeenXXPired</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.245759e+18</td>\n",
       "      <td>Happy Mothers Day Mum - I'm sorry I can't be there to bring you Mothers day flowers &amp; a cwtch - honestly at this point I'd walk on hot coals to be able to. But I'll be there with bells on as soon as I can be. Love you lots xxx (p.s we need more photos!) https:// photos.app.goo.gl/M3vXBLrsCzD4TE bY7 …</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>FestiveFeeling</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.246087e+18</td>\n",
       "      <td>Happy mothers day To all This doing a mothers days work. Today been quiet but Had time to reflect. Dog walk, finish a jigsaw do the garden, learn few more guitar chords, drunk some strawberry gin and tonic and watch Lee evens on DVD. My favourite place to visit. #isolate pic.twitter.com/GZ0xVvF6f9</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>KrisAllenSak</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.244803e+18</td>\n",
       "      <td>Happy mothers day to this beautiful woman...royalty soothes you mummy jeremy and emerald and more #PrayForRoksie #UltimateLoveNG pic.twitter.com/oeetI22Pvv</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>Queenuchee</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.244876e+18</td>\n",
       "      <td>Remembering the 3 most amazing ladies who made me who I am! My late grandmother iris, mum carol and great grandmother Ethel. Missed but never forgotten! Happy mothers day to all those great mums out there! Love sent to all xxxx pic.twitter.com/xZZZdEybjE</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>brittan17446794</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  \\\n",
       "0  1.245025e+18   \n",
       "1  1.245759e+18   \n",
       "2  1.246087e+18   \n",
       "3  1.244803e+18   \n",
       "4  1.244876e+18   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                   original_text  \\\n",
       "0  Happy #MothersDay to all you amazing mothers out there! I know it's hard not being able to see your mothers today but it's on all of us to do what we can to protect the most vulnerable members of our society. #BeatCoronaVirus pic.twitter.com/va4nFjFQ5B                                                    \n",
       "1  Happy Mothers Day Mum - I'm sorry I can't be there to bring you Mothers day flowers & a cwtch - honestly at this point I'd walk on hot coals to be able to. But I'll be there with bells on as soon as I can be. Love you lots xxx (p.s we need more photos!) https:// photos.app.goo.gl/M3vXBLrsCzD4TE bY7 …   \n",
       "2  Happy mothers day To all This doing a mothers days work. Today been quiet but Had time to reflect. Dog walk, finish a jigsaw do the garden, learn few more guitar chords, drunk some strawberry gin and tonic and watch Lee evens on DVD. My favourite place to visit. #isolate pic.twitter.com/GZ0xVvF6f9      \n",
       "3  Happy mothers day to this beautiful woman...royalty soothes you mummy jeremy and emerald and more #PrayForRoksie #UltimateLoveNG pic.twitter.com/oeetI22Pvv                                                                                                                                                     \n",
       "4  Remembering the 3 most amazing ladies who made me who I am! My late grandmother iris, mum carol and great grandmother Ethel. Missed but never forgotten! Happy mothers day to all those great mums out there! Love sent to all xxxx pic.twitter.com/xZZZdEybjE                                                  \n",
       "\n",
       "  lang retweet_count  original_author  sentiment_class  \n",
       "0  en   0             BeenXXPired      0                \n",
       "1  en   1             FestiveFeeling   0                \n",
       "2  en   0             KrisAllenSak    -1                \n",
       "3  en   0             Queenuchee       0                \n",
       "4  en   0             brittan17446794 -1                "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_63d36aca_a829_11ea_be29_d43b045a62ecrow0_col0 {\n",
       "            background-color:  #9e9ac8;\n",
       "            color:  #000000;\n",
       "        }    #T_63d36aca_a829_11ea_be29_d43b045a62ecrow0_col1 {\n",
       "            background-color:  #3f007d;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_63d36aca_a829_11ea_be29_d43b045a62ecrow1_col0 {\n",
       "            background-color:  #fcfbfd;\n",
       "            color:  #000000;\n",
       "        }    #T_63d36aca_a829_11ea_be29_d43b045a62ecrow1_col1 {\n",
       "            background-color:  #fcfbfd;\n",
       "            color:  #000000;\n",
       "        }    #T_63d36aca_a829_11ea_be29_d43b045a62ecrow2_col0 {\n",
       "            background-color:  #3f007d;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_63d36aca_a829_11ea_be29_d43b045a62ecrow2_col1 {\n",
       "            background-color:  #fcfbfd;\n",
       "            color:  #000000;\n",
       "        }</style><table id=\"T_63d36aca_a829_11ea_be29_d43b045a62ec\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >sentiment_class</th>        <th class=\"col_heading level0 col1\" >original_text</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_63d36aca_a829_11ea_be29_d43b045a62eclevel0_row0\" class=\"row_heading level0 row0\" >1</th>\n",
       "                        <td id=\"T_63d36aca_a829_11ea_be29_d43b045a62ecrow0_col0\" class=\"data row0 col0\" >0</td>\n",
       "                        <td id=\"T_63d36aca_a829_11ea_be29_d43b045a62ecrow0_col1\" class=\"data row0 col1\" >1701</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_63d36aca_a829_11ea_be29_d43b045a62eclevel0_row1\" class=\"row_heading level0 row1\" >0</th>\n",
       "                        <td id=\"T_63d36aca_a829_11ea_be29_d43b045a62ecrow1_col0\" class=\"data row1 col0\" >-1</td>\n",
       "                        <td id=\"T_63d36aca_a829_11ea_be29_d43b045a62ecrow1_col1\" class=\"data row1 col1\" >769</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_63d36aca_a829_11ea_be29_d43b045a62eclevel0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_63d36aca_a829_11ea_be29_d43b045a62ecrow2_col0\" class=\"data row2 col0\" >1</td>\n",
       "                        <td id=\"T_63d36aca_a829_11ea_be29_d43b045a62ecrow2_col1\" class=\"data row2 col1\" >765</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1cc0560a0c8>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = train_data.groupby('sentiment_class').count()['original_text'].reset_index().sort_values(by='original_text',ascending=False)\n",
    "temp.style.background_gradient(cmap='Purples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1cc06776748>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x='sentiment_class',data=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3235 entries, 0 to 3234\n",
      "Data columns (total 6 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   id               3235 non-null   float64\n",
      " 1   original_text    3235 non-null   object \n",
      " 2   lang             3231 non-null   object \n",
      " 3   retweet_count    3231 non-null   object \n",
      " 4   original_author  3235 non-null   object \n",
      " 5   sentiment_class  3235 non-null   int64  \n",
      "dtypes: float64(1), int64(1), object(4)\n",
      "memory usage: 151.8+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['retweet_count']=train_data['retweet_count'].astype(str)\n",
    "test_data['retweet_count']=test_data['retweet_count'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"retweet_count\"]= train_data[\"retweet_count\"].str.replace(r\"[a-zA-Z$@#&*!''-/]\",'')\n",
    "train_data[\"retweet_count\"]= train_data[\"retweet_count\"].replace(r'^\\s*$', np.nan, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"retweet_count\"]= test_data[\"retweet_count\"].str.replace(r\"[a-zA-Z$@#&*!''-/]\",'')\n",
    "test_data[\"retweet_count\"]= test_data[\"retweet_count\"].replace(r'^\\s*$', np.nan, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['retweet_count']=pd.to_numeric(train_data['retweet_count'])\n",
    "test_data['retweet_count']=pd.to_numeric(test_data['retweet_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.loc[train_data[\"retweet_count\"]<0,'retweet_count']=0\n",
    "train_data.loc[train_data[\"retweet_count\"].isna(),'retweet_count']=0\n",
    "test_data.loc[test_data[\"retweet_count\"]<0,'retweet_count']=0\n",
    "test_data.loc[test_data[\"retweet_count\"].isna(),'retweet_count']=0\n",
    "train_data.loc[train_data[\"retweet_count\"]>100,'retweet_count']=100\n",
    "test_data.loc[test_data[\"retweet_count\"]>100,'retweet_count']=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"retweet_count\"]=train_data[\"retweet_count\"].astype(int)\n",
    "test_data[\"retweet_count\"]=test_data[\"retweet_count\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['original_text']=train_data['original_text'].apply(expand_contractions)\n",
    "test_data['original_text']=test_data['original_text'].apply(expand_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>original_text</th>\n",
       "      <th>lang</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>original_author</th>\n",
       "      <th>sentiment_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.245025e+18</td>\n",
       "      <td>Happy #MothersDay to all you amazing mothers out there! I know it is hard not being able to see your mothers today but it is on all of us to do what we can to protect the most vulnerable members of our society. #BeatCoronaVirus pic.twitter.com/va4nFjFQ5B</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>BeenXXPired</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.245759e+18</td>\n",
       "      <td>Happy Mothers Day Mum - I am sorry I cannot be there to bring you Mothers day flowers &amp; a cwtch - honestly at this point I would walk on hot coals to be able to. But I will be there with bells on as soon as I can be. Love you lots xxx (p.s we need more photos!) https:// photos.app.goo.gl/M3vXBLrsCzD4TE bY7 …</td>\n",
       "      <td>en</td>\n",
       "      <td>1</td>\n",
       "      <td>FestiveFeeling</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.246087e+18</td>\n",
       "      <td>Happy mothers day To all This doing a mothers days work. Today been quiet but Had time to reflect. Dog walk, finish a jigsaw do the garden, learn few more guitar chords, drunk some strawberry gin and tonic and watch Lee evens on DVD. My favourite place to visit. #isolate pic.twitter.com/GZ0xVvF6f9</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>KrisAllenSak</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.244803e+18</td>\n",
       "      <td>Happy mothers day to this beautiful woman...royalty soothes you mummy jeremy and emerald and more #PrayForRoksie #UltimateLoveNG pic.twitter.com/oeetI22Pvv</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>Queenuchee</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.244876e+18</td>\n",
       "      <td>Remembering the 3 most amazing ladies who made me who I am! My late grandmother iris, mum carol and great grandmother Ethel. Missed but never forgotten! Happy mothers day to all those great mums out there! Love sent to all xxxx pic.twitter.com/xZZZdEybjE</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>brittan17446794</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  \\\n",
       "0  1.245025e+18   \n",
       "1  1.245759e+18   \n",
       "2  1.246087e+18   \n",
       "3  1.244803e+18   \n",
       "4  1.244876e+18   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                           original_text  \\\n",
       "0  Happy #MothersDay to all you amazing mothers out there! I know it is hard not being able to see your mothers today but it is on all of us to do what we can to protect the most vulnerable members of our society. #BeatCoronaVirus pic.twitter.com/va4nFjFQ5B                                                          \n",
       "1  Happy Mothers Day Mum - I am sorry I cannot be there to bring you Mothers day flowers & a cwtch - honestly at this point I would walk on hot coals to be able to. But I will be there with bells on as soon as I can be. Love you lots xxx (p.s we need more photos!) https:// photos.app.goo.gl/M3vXBLrsCzD4TE bY7 …   \n",
       "2  Happy mothers day To all This doing a mothers days work. Today been quiet but Had time to reflect. Dog walk, finish a jigsaw do the garden, learn few more guitar chords, drunk some strawberry gin and tonic and watch Lee evens on DVD. My favourite place to visit. #isolate pic.twitter.com/GZ0xVvF6f9              \n",
       "3  Happy mothers day to this beautiful woman...royalty soothes you mummy jeremy and emerald and more #PrayForRoksie #UltimateLoveNG pic.twitter.com/oeetI22Pvv                                                                                                                                                             \n",
       "4  Remembering the 3 most amazing ladies who made me who I am! My late grandmother iris, mum carol and great grandmother Ethel. Missed but never forgotten! Happy mothers day to all those great mums out there! Love sent to all xxxx pic.twitter.com/xZZZdEybjE                                                          \n",
       "\n",
       "  lang  retweet_count  original_author  sentiment_class  \n",
       "0  en   0              BeenXXPired      0                \n",
       "1  en   1              FestiveFeeling   0                \n",
       "2  en   0              KrisAllenSak    -1                \n",
       "3  en   0              Queenuchee       0                \n",
       "4  en   0              brittan17446794 -1                "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of words in the original_text ##\n",
    "train_data[\"num_words\"] = train_data[\"original_text\"].apply(lambda x: len(str(x).split()))\n",
    "test_data[\"num_words\"] = test_data[\"original_text\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "## Number of unique words in the original_text ##\n",
    "train_data[\"num_unique_words\"] = train_data[\"original_text\"].apply(lambda x: len(set(str(x).split())))\n",
    "test_data[\"num_unique_words\"] = test_data[\"original_text\"].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "## Number of characters in the original_text ##\n",
    "train_data[\"num_chars\"] = train_data[\"original_text\"].apply(lambda x: len(str(x)))\n",
    "test_data[\"num_chars\"] = test_data[\"original_text\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "## Number of stopwords in the original_text ##\n",
    "train_data[\"num_stopwords\"] = train_data[\"original_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "test_data[\"num_stopwords\"] = test_data[\"original_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "\n",
    "## Number of punctuations in the original_text ##\n",
    "train_data[\"num_punctuations\"] =train_data['original_text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "test_data[\"num_punctuations\"] =test_data['original_text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "\n",
    "## Number of title case words in the original_text ##\n",
    "train_data[\"num_words_upper\"] = train_data[\"original_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "test_data[\"num_words_upper\"] = test_data[\"original_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "\n",
    "## Number of title case words in the original_text ##\n",
    "train_data[\"num_words_title\"] = train_data[\"original_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "test_data[\"num_words_title\"] = test_data[\"original_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "\n",
    "## Average length of the words in the original_text ##\n",
    "train_data[\"mean_word_len\"] = train_data[\"original_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "test_data[\"mean_word_len\"] = test_data[\"original_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"cols_to_drop = ['id','original_text','sentiment_class','lang','original_author']\\ntrain_y = train_data.sentiment_class\\ntrain_X = train_data.drop(cols_to_drop, axis=1)\\ntest_X = test_data.drop(['id','original_text','lang','original_author'], axis=1)\""
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''cols_to_drop = ['id','original_text','sentiment_class','lang','original_author']\n",
    "train_y = train_data.sentiment_class\n",
    "train_X = train_data.drop(cols_to_drop, axis=1)\n",
    "test_X = test_data.drop(['id','original_text','lang','original_author'], axis=1)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def runXGB(train_X, train_y, test_X, test_y=None, test_X2=None, seed_val=0, child=1, colsample=0.3):\\n    param = {}\\n    param[\\'objective\\'] = \\'multi:softprob\\'\\n    param[\\'eta\\'] = 0.1\\n    param[\\'max_depth\\'] = 3\\n    param[\\'num_class\\'] = 3\\n    param[\\'eval_metric\\'] = \"mlogloss\"\\n    param[\\'min_child_weight\\'] = child\\n    param[\\'subsample\\'] = 0.8\\n    param[\\'colsample_bytree\\'] = colsample\\n    param[\\'seed\\'] = seed_val\\n    num_rounds = 2000\\n\\n    plst = list(param.items())\\n    xgtrain = xgb.DMatrix(train_X, label=train_y)\\n\\n    if test_y is not None:\\n        xgtest = xgb.DMatrix(test_X, label=test_y)\\n        watchlist = [ (xgtrain,\\'train\\'), (xgtest, \\'test\\') ]\\n        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=50, verbose_eval=20)\\n    else:\\n        xgtest = xgb.DMatrix(test_X)\\n        model = xgb.train(plst, xgtrain, num_rounds)\\n\\n    pred_test_y = model.predict(xgtest, ntree_limit = model.best_ntree_limit)\\n    if test_X2 is not None:\\n        xgtest2 = xgb.DMatrix(test_X2)\\n        pred_test_y2 = model.predict(xgtest2, ntree_limit = model.best_ntree_limit)\\n    return pred_test_y, pred_test_y2, model'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def runXGB(train_X, train_y, test_X, test_y=None, test_X2=None, seed_val=0, child=1, colsample=0.3):\n",
    "    param = {}\n",
    "    param['objective'] = 'multi:softprob'\n",
    "    param['eta'] = 0.1\n",
    "    param['max_depth'] = 3\n",
    "    param['num_class'] = 3\n",
    "    param['eval_metric'] = \"mlogloss\"\n",
    "    param['min_child_weight'] = child\n",
    "    param['subsample'] = 0.8\n",
    "    param['colsample_bytree'] = colsample\n",
    "    param['seed'] = seed_val\n",
    "    num_rounds = 2000\n",
    "\n",
    "    plst = list(param.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=50, verbose_eval=20)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(xgtest, ntree_limit = model.best_ntree_limit)\n",
    "    if test_X2 is not None:\n",
    "        xgtest2 = xgb.DMatrix(test_X2)\n",
    "        pred_test_y2 = model.predict(xgtest2, ntree_limit = model.best_ntree_limit)\n",
    "    return pred_test_y, pred_test_y2, model'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\\ncv_scores = []\\npred_full_test = 0\\npred_train = np.zeros([train_data.shape[0], 3])\\nfor dev_index, val_index in kf.split(train_X):\\n    dev_X, val_X = train_X.loc[dev_index], train_X.loc[val_index]\\n    dev_y, val_y = train_y[dev_index], train_y[val_index]\\n    pred_val_y, pred_test_y, model = runXGB(dev_X, dev_y, val_X, val_y, test_X, seed_val=0)\\n    pred_full_test = pred_full_test + pred_test_y\\n    pred_train[val_index,:] = pred_val_y\\n    cv_scores.append(100*(f1_score(val_y,pred_val_y,average=\\'weighted\\')))\\n    break\\nprint(\"cv scores : \", cv_scores)'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "cv_scores = []\n",
    "pred_full_test = 0\n",
    "pred_train = np.zeros([train_data.shape[0], 3])\n",
    "for dev_index, val_index in kf.split(train_X):\n",
    "    dev_X, val_X = train_X.loc[dev_index], train_X.loc[val_index]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "    pred_val_y, pred_test_y, model = runXGB(dev_X, dev_y, val_X, val_y, test_X, seed_val=0)\n",
    "    pred_full_test = pred_full_test + pred_test_y\n",
    "    pred_train[val_index,:] = pred_val_y\n",
    "    cv_scores.append(100*(f1_score(val_y,pred_val_y,average='weighted')))\n",
    "    break\n",
    "print(\"cv scores : \", cv_scores)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanText(BaseEstimator, TransformerMixin):\n",
    "    def remove_mentions(self, input_text):\n",
    "        return re.sub(r'@\\w+', '', input_text)\n",
    "    \n",
    "    def remove_urls(self, input_text):\n",
    "        return re.sub(r'http.?://[^\\s]+[\\s]?', '', input_text)\n",
    "    \n",
    "    def emoji_oneword(self, input_text):\n",
    "        # By compressing the underscore, the emoji is kept as one word\n",
    "        return input_text.replace('_','')\n",
    "    \n",
    "    def remove_punctuation(self, input_text):\n",
    "        # Make translation table\n",
    "        punct = string.punctuation\n",
    "        trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space\n",
    "        return input_text.translate(trantab)\n",
    "    def remove_digits(self, input_text):\n",
    "        return re.sub('\\d+', '', input_text)\n",
    "    \n",
    "    def to_lower(self, input_text):\n",
    "        return input_text.lower()\n",
    "    \n",
    "    def remove_stopwords(self, input_text):\n",
    "        stopwords_list = stopwords.words('english')\n",
    "        # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "        whitelist = [\"n't\", \"not\", \"no\",\"never\"]\n",
    "        words = input_text.split() \n",
    "        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
    "        return \" \".join(clean_words) \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    def transform(self, X, **transform_params):\n",
    "        clean_X = X.apply(self.remove_mentions).apply(self.remove_urls).apply(self.emoji_oneword).apply(self.remove_punctuation).apply(self.remove_digits).apply(self.to_lower).apply(self.remove_stopwords)\n",
    "        return clean_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = CleanText()\n",
    "train_data['original_text'] = ct.fit_transform(train_data.original_text)\n",
    "test_data['original_text']=ct.fit_transform(test_data.original_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['original_text'] = train_data['original_text'].astype(str)\n",
    "test_data['original_text'] = test_data['original_text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "def clean_text(raw_text):\n",
    "    return \" \".join(w for w in nltk.wordpunct_tokenize(raw_text) \\\n",
    "     if w.lower() in words or not w.isalpha())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['original_text'] = train_data['original_text'].apply(clean_text)\n",
    "test_data['original_text'] = test_data['original_text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from langdetect import detect\\ndef clean_lang(df):\\n    #df.loc[((detect(df[\\'original_text\\'])==\\'en\\') and df[\\'lang\\']!=\\'en\\'),\\'lang\\'] = \\'en\\'\\n    for row_df in df:\\n        if ((detect(row_df[1])==\\'en\\') and row_df[2]!=\\'en\\'):\\n            row_df[2]=\"\"+\\'en\\'\\n            \\n    return df '"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from langdetect import detect\n",
    "def clean_lang(df):\n",
    "    #df.loc[((detect(df['original_text'])=='en') and df['lang']!='en'),'lang'] = 'en'\n",
    "    for row_df in df:\n",
    "        if ((detect(row_df[1])=='en') and row_df[2]!='en'):\n",
    "            row_df[2]=\"\"+'en'\n",
    "            \n",
    "    return df '''   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train_data[train_data['lang']=='en']\n",
    "test_data=test_data[test_data['lang']=='en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "def stem_text(input_text):\n",
    "    words = input_text.split()\n",
    "    return \" \".join([porter.stem(word) for word in words])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['original_text'] = train_data['original_text'].apply(stem_text)\n",
    "test_data['original_text'] = test_data['original_text'].apply(stem_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(input_text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(input_text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['original_text'] = train_data['original_text'].apply(lemmatize_text).astype(str)\n",
    "test_data['original_text'] = test_data['original_text'].apply(lemmatize_text).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text_df):\n",
    "    text_df[\"clean_text\"]= text_df[\"original_text\"].str.replace(\"mum\", \"mother\", case = False)\n",
    "    text_df[\"clean_text\"]= text_df[\"clean_text\"].str.replace(\"mom\", \"mother\", case = False)\n",
    "    text_df[\"clean_text\"]= text_df[\"clean_text\"].str.replace(\"mothersday\",\"mother day\") \n",
    "    text_df[\"clean_text\"]= text_df[\"clean_text\"].str.replace(\"httpswww\",\"\") \n",
    "    text_df[\"clean_text\"]= text_df[\"clean_text\"].str.replace(\"http\",\"\") \n",
    "    text_df[\"clean_text\"]= text_df[\"clean_text\"].str.replace(\"u\",\"\") \n",
    "    return text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=clean_text(train_data)\n",
    "test_df=clean_text(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1304, 14)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnExtractor(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "    def transform(self, X, **transform_params):\n",
    "        return X[self.cols]\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2994, 15)\n",
      "(1304, 14)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n",
      "            ...\n",
      "            1375, 1376, 1377, 1379, 1380, 1381, 1382, 1384, 1385, 1386],\n",
      "           dtype='int64', length=1304)\n"
     ]
    }
   ],
   "source": [
    "print(test_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fit transform the tfidf vectorizer ###\n",
    "tfidf = TfidfVectorizer(norm=None,token_pattern=r\"(?u)\\b\\w+\\b\", stop_words=None, sublinear_tf=True, max_df=0.5, analyzer='word', \n",
    "           ngram_range=(1,2))\n",
    "tfidf_train = tfidf_vec.fit(train_df['clean_text'])\n",
    "train_tfidf = pd.DataFrame(tfidf_train.transform(train_df['clean_text']))\n",
    "train_df = pd.concat([train_df,train_tfidf], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3219, 16)\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tfidf = pd.DataFrame(tfidf_train.transform(test_df['clean_text']))\n",
    "test_df = pd.concat([test_df,test_tfidf], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=1304, step=1)\n",
      "Int64Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n",
      "            ...\n",
      "            1375, 1376, 1377, 1379, 1380, 1381, 1382, 1384, 1385, 1386],\n",
      "           dtype='int64', length=1381)\n"
     ]
    }
   ],
   "source": [
    "print(test_tfidf.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,\n",
      "            ...\n",
      "            1375, 1376, 1377, 1379, 1380, 1381, 1382, 1384, 1385, 1386],\n",
      "           dtype='int64', length=1381)\n"
     ]
    }
   ],
   "source": [
    "print(test_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_id=df_test.id\n",
    "df_test=df_test.drop(['id','original_text','lang','original_author','clean_text'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_train.sentiment_class\n",
    "X = df_train.drop(['id','original_text','sentiment_class','lang','original_author','clean_text'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_scores={}\n",
    "best_models={}\n",
    "mnb = MultinomialNB(alpha=0.1)\n",
    "bnb = BernoulliNB()\n",
    "linsvc = LinearSVC()\n",
    "logreg = LogisticRegression()\n",
    "sgd=SGDClassifier()\n",
    "svc=SVC()\n",
    "gbm=GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid settings for the vectorizers (Count and TFIDF)\n",
    "parameters_vect = {\n",
    "    'features__pipe__vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'features__pipe__vect__ngram_range': ((1, 1), (1, 2),(1,3),(2,2)),\n",
    "    'features__pipe__vect__min_df': (1,2,3,4,5)\n",
    "}\n",
    "# Parameter grid settings for MultinomialNB\n",
    "'''parameters_mnb = {\n",
    "    'alpha': (0.01,0.1,0.25,0.5,0.75),\n",
    "    'fit_prior' : (True,False)\n",
    "}'''\n",
    "# Parameter grid settings for LogisticRegression\n",
    "parameters_logreg = {\n",
    "    'clf__C': (0.25, 0.5, 1.0),\n",
    "    'clf__penalty': ('l1', 'l2'),\n",
    "    'clf__fit_intercept': (True,False)\n",
    "}\n",
    "parameters_sgd = {\n",
    "    'clf__alpha': (0.00001, 0.000001),\n",
    "    'clf__penalty': ('l1', 'l2','elasticnet'),\n",
    "    'clf__max_iter': (10, 25, 50, 75, 100)\n",
    "}\n",
    "parameters_svc = {'clf__kernel': ['linear', 'rbf'],\n",
    "                    'clf__gamma':[0.1,1,10],\n",
    "                    'clf__C':[0.1,1,0.001,0.0001]\n",
    "                 }\n",
    "parameters_gbm = {'clf__loss':['deviance','exponential'],\n",
    "         'clf__learning_rate': [0.05,0.1],\n",
    "         'clf__max_depth':[5,10,20],\n",
    "         'clf__n_estimators': [10,25,50]\n",
    "        }\n",
    "parameters_xgb = {'clf__learning_rate':[0.01,0.1],\n",
    "         'clf__n_estimators':[50,100,500],\n",
    "         'clf__max_depth':[5,10,15,20],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "def grid_vect(model, X_train, X_test, y_train, y_test, parameters_clf=None):\n",
    "    #pipeline = Pipeline([('features', features),('clf', clf)])\n",
    "    \n",
    "    # Join the parameters dictionaries together\n",
    "    if parameters_clf is not None:\n",
    "        parameters = dict()\n",
    "        parameters.update(parameters_clf)\n",
    "        # Make sure you have scikit-learn version 0.19 or higher to use multiple scoring metrics\n",
    "        grid_search_model = RandomizedSearchCV(model, parameters, n_jobs=-1, scoring='f1_weighted',verbose=1, cv=5,random_state=42)\n",
    "        print(\"Performing grid search...\")\n",
    "        grid_search_model.fit(X_train, y_train)\n",
    "        print(\"Best CV score: %0.3f\" % grid_search_model.best_score_)\n",
    "        print(\"Best parameters set:\")\n",
    "        best_parameters = grid_search_model.best_estimator_.get_params()\n",
    "        for param_name in sorted(parameters.keys()):\n",
    "            print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "        \n",
    "        print(\"Test score with best_estimator_: %0.3f\" % grid_search_model.best_estimator_.score(X_test, y_test))\n",
    "        return grid_search_model\n",
    "    else:\n",
    "        scores = cross_val_score(model, X_train, y_train, cv=5,scoring='precision_macro')\n",
    "        print(\"Test score with cross validation are: \")\n",
    "        print(scores)\n",
    "        print(\"Test score with MNB: %0.3f\" % (100*(model.fit(X_train, y_train).score(X_test, y_test))))\n",
    "        model.fit(X_train,y_train)\n",
    "        return model\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>num_words</th>\n",
       "      <th>num_unique_words</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_stopwords</th>\n",
       "      <th>num_punctuations</th>\n",
       "      <th>num_words_upper</th>\n",
       "      <th>num_words_title</th>\n",
       "      <th>mean_word_len</th>\n",
       "      <th>0</th>\n",
       "      <th>...</th>\n",
       "      <th>24045</th>\n",
       "      <th>24046</th>\n",
       "      <th>24047</th>\n",
       "      <th>24048</th>\n",
       "      <th>24049</th>\n",
       "      <th>24050</th>\n",
       "      <th>24051</th>\n",
       "      <th>24052</th>\n",
       "      <th>24053</th>\n",
       "      <th>24054</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3147</th>\n",
       "      <td>3</td>\n",
       "      <td>57</td>\n",
       "      <td>53</td>\n",
       "      <td>275</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>3.842105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2428</th>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>37</td>\n",
       "      <td>280</td>\n",
       "      <td>19</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>5.244444</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>40</td>\n",
       "      <td>293</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5.681818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>32</td>\n",
       "      <td>43</td>\n",
       "      <td>41</td>\n",
       "      <td>274</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>5.395349</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>171</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>8.052632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24064 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      retweet_count  num_words  num_unique_words  num_chars  num_stopwords  \\\n",
       "3147  3              57         53                275        25              \n",
       "2428  0              45         37                280        19              \n",
       "274   0              44         40                293        18              \n",
       "489   32             43         41                274        17              \n",
       "838   0              19         18                171        4               \n",
       "\n",
       "      num_punctuations  num_words_upper  num_words_title  mean_word_len    0  \\\n",
       "3147  4                 2                7                3.842105       0.0   \n",
       "2428  23                1                9                5.244444       0.0   \n",
       "274   15                0                6                5.681818       0.0   \n",
       "489   18                2                8                5.395349       0.0   \n",
       "838   9                 0                7                8.052632       0.0   \n",
       "\n",
       "      ...  24045  24046  24047  24048  24049  24050  24051  24052  24053  \\\n",
       "3147  ...  0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0     \n",
       "2428  ...  0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0     \n",
       "274   ...  0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0     \n",
       "489   ...  0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0     \n",
       "838   ...  0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0     \n",
       "\n",
       "      24054  \n",
       "3147  0.0    \n",
       "2428  0.0    \n",
       "274   0.0    \n",
       "489   0.0    \n",
       "838   0.0    \n",
       "\n",
       "[5 rows x 24064 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score with cross validation are: \n",
      "[0.39442026 0.36307185 0.36887035 0.3213841  0.34555627]\n",
      "Test score with MNB: 31.052\n",
      "\n",
      "Accuracy score of Multinomial naive bayes algorithm -----> 31.766590952249917\n"
     ]
    }
   ],
   "source": [
    "best_mnb_countvect = grid_vect(mnb,X_train,X_test,y_train,y_test,None)\n",
    "prediction_mnb=best_mnb_countvect.predict(X_test)\n",
    "mnb_score=100*(f1_score(y_test,prediction_mnb,average='weighted'))\n",
    "train_pred1=best_mnb_countvect.predict(X_train)\n",
    "test_pred1=best_mnb_countvect.predict(X_test)\n",
    "print(\"\\nAccuracy score of Multinomial naive bayes algorithm -----> \" + str(mnb_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_bnb_countvect = grid_vect(bnb,X_train, X_test,model_features,parameters_mnb,\n",
    "                               parameters_countvect=parameters_vect,parameters_tfidf=parameters_tfidf)\n",
    "prediction_bnb=best_bnb_countvect.predict(X_test)\n",
    "bnb_score=100*(f1_score(y_test,prediction_bnb,average='weighted'))\n",
    "best_scores['mnb_countvect']=best_bnb_countvect.best_score_\n",
    "best_models['mnb_countvect']=best_bnb_countvect\n",
    "train_pred1=best_bnb_countvect.predict(X_train)\n",
    "test_pred1=best_bnb_countvect.predict(X_test)\n",
    "print(\"\\nAccuracy score of Multinomial naive bayes algorithm -----> \" + str(bnb_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_mnb_model=mnb.set_params(**best_mnb_countvect.best_params_)\n",
    "best_mnb_countvect.fit(X,y)\n",
    "test_predictions=best_mnb_countvect.predict(df_test)\n",
    "prediction_df = pd.DataFrame(columns=['sentiment_class'],data=test_predictions)\n",
    "prediction_df = pd.concat([test_df_id,prediction_df],axis=1)\n",
    "prediction_df.to_csv(r'C:\\Users\\gaurav.singh.rawal\\Documents\\Gaurav\\Data Science\\Machine Learning-Predictive Analytics\\ML-Projects\\GIT - ML - Projects\\ML_Projects\\NLP - Sentiment Analysis\\HackerRank - Mothers Day\\predictions.csv',index=False,encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegression\n",
    "best_losso_countvect = grid_vect(linsvc,X_train, X_test,model_features,parameters_linsvc,\n",
    "                                 parameters_countvect=parameters_vect,parameters_tfidf=parameters_tfidf)\n",
    "prediction_losso = best_losso_countvect.predict(X_test)\n",
    "losso_score=100*(f1_score(y_test,prediction_losso,average='weighted'))\n",
    "best_scores['losso_countvect']=best_losso_countvect.best_score_\n",
    "best_models['losso_countvect']=best_losso_countvect\n",
    "train_pred2=best_losso_countvect.predict(X_train)\n",
    "test_pred2=best_losso_countvect.predict(X_test)\n",
    "print(\"\\nAccuracy score of Lasso algorithm -----> \" + str(losso_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD\n",
    "best_sgd_countvect = grid_vect(sgd, parameters_sgd, X_train, X_test,model_features, parameters_text=parameters_vect)\n",
    "prediction_sgd = best_sgd_countvect.predict(X_test)\n",
    "sgd_score=100*(f1_score(y_test,prediction_sgd,average='weighted'))\n",
    "best_scores['sgd_countvect']=best_sgd_countvect.best_score_\n",
    "best_models['sgd_countvect']=best_sgd_countvect\n",
    "train_pred3=best_sgd_countvect.predict(X_train)\n",
    "test_pred3=best_sgd_countvect.predict(X_test)\n",
    "print(\"\\nAccuracy score of SGD algorithm -----> \" + str(sgd_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC\n",
    "best_svc_countvect = grid_vect(svc, parameters_svc, X_train, X_test,model_features, parameters_text=parameters_vect)\n",
    "prediction_svc = best_svc_countvect.predict(X_test)\n",
    "svc_score=100*(f1_score(y_test,prediction_svc,average='weighted'))\n",
    "best_scores['svc_countvect']=best_svc_countvect.best_score_\n",
    "best_models['svc_countvect']=best_svc_countvect\n",
    "train_pred4=best_svc_countvect.predict(X_train)\n",
    "test_pred4=best_svc_countvect.predict(X_test)\n",
    "print(\"\\nAccuracy score of SVC algorithm -----> \" + str(svc_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBC\n",
    "best_gbm_countvect = grid_vect(gbm, parameters_gbm, X_train, X_test,model_features, parameters_text=parameters_vect)\n",
    "prediction_gbm = best_gbm_countvect.predict(X_test)\n",
    "gbm_score=100*(f1_score(y_test,prediction_gbm,average='weighted'))\n",
    "best_scores['gbm_countvect']=best_gbm_countvect.best_score_\n",
    "best_models['gbm_countvect']=best_gbm_countvect\n",
    "train_pred5=best_gbm_countvect.predict(X_train)\n",
    "test_pred5=best_gbm_countvect.predict(X_test)\n",
    "print(\"\\nAccuracy score of gbc algorithm -----> \" + str(gbm_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "countvect_classifier= VotingClassifier(estimators=[('mnb', best_models['mnb_countvect']), ('lr', best_models['logreg_countvect']),\n",
    "                         ('sgd', best_models['sgd_countvect']), ('svc', best_models['svc_countvect']),\n",
    "                         ('gbm', best_models['gbm_countvect'])],voting='hard')\n",
    "countvect_classifier.fit(X_train,y_train)\n",
    "countvect_classifier_predictions=countvect_classifier.predict(X_test)\n",
    "countvect_classifier_predictions_score=100*(f1_score(y_test,countvect_classifier_predictions,average='weighted'))\n",
    "print(\"\\nAccuracy score of Voting Classifier algorithm -----> \" + str(countvect_classifier_predictions_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([pd.DataFrame(train_pred1),pd.DataFrame(train_pred2),\n",
    "                      pd.DataFrame(train_pred3),pd.DataFrame(train_pred4),pd.DataFrame(train_pred5)], axis=1)\n",
    "df_test = pd.concat([pd.DataFrame(test_pred1),pd.DataFrame(test_pred2),pd.DataFrame(test_pred3)\n",
    "                     ,pd.DataFrame(test_pred4),pd.DataFrame(test_pred5)], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns=['mnb','lr','sgd','svc','gbm']\n",
    "df_test.columns=['mnb','lr','sgd','svc','gbm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "def hyperparameter_tuner(model,X_train,y_train,hp_list):\n",
    "    \n",
    "    hp_perf=[]\n",
    "    hp_model=RandomizedSearchCV(model,param_distributions=hp_list,n_iter=10,n_jobs=-1, scoring='f1_weighted',verbose=1, cv=5)\n",
    "    hp_model.fit(X_train,y_train)\n",
    "    best_hp_model=hp_model.best_estimator_\n",
    "    best_param=hp_model.best_params_\n",
    "    best_score=hp_model.best_score_  \n",
    "   \n",
    "    return best_hp_model,best_param,best_score\n",
    "\n",
    "xgb=XGBClassifier({\n",
    "    'objective': 'multi:softprob',\n",
    "    'eta': 0.1,\n",
    "    'max_depth': 3,\n",
    "    'silent' :1,\n",
    "    'num_class' : 3,\n",
    "    'eval_metric' : \"mlogloss\",\n",
    "    'min_child_weight': 1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.3,\n",
    "    'seed':17,\n",
    "    'num_rounds':2000,\n",
    "})\n",
    "xgb.fit(df_train,y_train)\n",
    "\n",
    "#best_model,best_params,best_score=hyperparameter_tuner(xgb,df_train,y_train,parameters_xgb)\n",
    "\n",
    "prediction_xgb = xgb.predict(df_test)\n",
    "xgb_score=100*(f1_score(y_test,prediction_xgb,average='weighted'))\n",
    "print(\"\\nAccuracy score of XGB stacking algorithm -----> \" + str(xgb_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = BaggingClassifier(RandomForestClassifier(max_depth=25,min_samples_leaf=2,n_estimators=25))\n",
    "model.fit(df_train,y_train)\n",
    "bagging_pred = model.predict(df_test)\n",
    "bagging_score=100*(f1_score(y_test,bagging_pred,average='weighted'))\n",
    "print(\"\\nAccuracy score of Random Forest Bagging algorithm -----> \" + str(bagging_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
